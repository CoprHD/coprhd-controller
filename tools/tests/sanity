#!/bin/bash
#
# Copyright (c) 2015 EMC Corporation
# All Rights Reserved
#

# ==============================================================
# Uncomment the below line to get tracing output for this script
#set -x

#Configuration file to be used
BASEDIR=$(dirname $0)
CONFIG_FILE=${1}

Usage()
{
    echo 'Usage: sanity <configuration_file> <bourne_ip>|<[bourne_ipv6]> {all | snapvx | isilon | vplex [normal|quick|xio|vnx] [noingest|ingest [all|exported|unexported]] | vplexexport | vplexsnap | custvolname | vnxblock | ingestblock | vnxblock_flex_varray | netapp | netappc | recoverpoint | xtremio [quick|hlu|quick hlu] | srdf |
 vnxfile | datadomainfile | vmaxblock | hds | vmaxblock_flex_varray | combined_block | blocksnapshot | blockmirror | full_copy | blockconsistencygroup | init | quick | sbsdk |
 errorhandling | vnxe | security | syssvc | audit | monitor | vdc | catalog | recovery | backuprestore backupserver-uri username password | dr | ipsec | ceph} [local|ldap] [<timeslot>] | sanzoning | application | unityblock [all|block|vplex|rp|hlu] | unityfile | arrayaffinity | portgroup'
    echo 'E.g.:  sanity conf/sanity.conf 137.69.169.21 all'
    echo 'E.g.:  sanity ~/.sanity/sanity.conf 137.69.169.21 isilon'
    echo 'Note: IPv6 needs to be square bracket surrounded. E.g.:  sanity [2620:0:170:2842::180] security'
    echo 'If the env variable DISCOVER_SAN=1 is set: discover and use SAN networking.'
    echo 'E.g.:  sanity conf/sanity.conf 137.69.169.21 audit ldap 2006-03-23T12[:23]'
    exit 2
}

toLower()
{
    echo "$(echo ${1} | tr '[:upper:]' '[:lower:]')"
}

toUpper()
{
    echo "$(echo ${1} | tr '[:lower:]' '[:upper:]')"
}

if [ -f $CONFIG_FILE ]; then
    echo "Using Configuration file $CONFIG_FILE for sanity"
    source ${1}
else
    echo "Configuration file $CONFIG_FILE not found"
    Usage
fi



[ $# -ge 3 ] || Usage
date

init_env_var()
{
    [[ -n "${TEST_APPLIANCE}" ]] && return
    remote_ip="${1}"
    [[ "${remote_ip}" == "localhost" ]] && export TEST_APPLIANCE=no && return 
    local_ip=`ifconfig ${ethdev} 2>/dev/null|awk '/inet addr:/ {print $2}' | sed 's/addr://'`
    [[ "${local_ip}" == "${remote_ip}" ]] && export TEST_APPLIANCE=no && return
    local_fqdn=$(nslookup ${local_ip} | sed -n  's/.*arpa.*name = \(.*\)/\1/p')
    [[ "${local_fqdn%.*}" == "${remote_ip}" ]] && export TEST_APPLIANCE=no && return    
}
init_env_var "${2}"

# Use the environment variable DISCOVER_SAN to determine if 
# SAN Networking should be discovered, thereby allowing zoning to be
# done for the exports.
DISCOVER_SAN=${DISCOVER_SAN:-0}
echo "DISCOVER_SAN: " $DISCOVER_SAN

# By default, we're not running quick sanity
QUICK=${QUICK:-0}

# The token file name will have a suffix which is this shell's PID
# It will allow to run the sanity in parallel
export BOURNE_TOKEN_FILE=${BOURNE_TOKEN_FILE:-"/tmp/token$$.txt"}
BOURNE_SAVED_TOKEN_FILE="/tmp/token_saved.txt"

PATH=$BASEDIR:/bin:/usr/bin:/usr/local/bin
OBJECTORFILENAME="/tmp/testipc.txt"
BOURNE_IPS=${2:-$BOURNE_IPADDR}
HOSTNAME=$BOURNE_IPADDR
IFS=',' read -ra BOURNE_IP_ARRAY <<< "$BOURNE_IPS"
BOURNE_IP=${BOURNE_IP_ARRAY[0]}
IP_INDEX=0

# By default, we're not running quick sanity
QUICK=0

SS=${3}
if [ ${SS} = "vplex" ]; then
    VPLEX_QUICK_PARAM=${4:-'normal'}
    if [ $VPLEX_QUICK_PARAM = 'quick' ]; then
        QUICK=1
    fi

    # The VPLEX quick parameter can also choose xio vs. vnx
    if [ "$VPLEX_QUICK_PARAM" = "xio" ]; then
        VPLEX_USE_XIO=1
    fi
    if [ "$VPLEX_QUICK_PARAM" = "vnx" -o "$VPLEX_QUICK_PARAM" = "quick" ]; then
        VPLEX_USE_XIO=0
    fi

    VPLEX_INGEST_PARAM=${5:-'noingest'}
    if [ "$VPLEX_INGEST_PARAM" != 'noingest' ]; then
        # all, exported, or unexported
        VPLEX_INGEST_PARAM=${5}
        shift
    fi
    echo "VPLEX_QUICK_PARAM is $VPLEX_QUICK_PARAM"
    echo "VPLEX_USE_XIO is $VPLEX_USE_XIO"
    echo "VPLEX_INGEST_PARAM is ingest $VPLEX_INGEST_PARAM"
    AUTH=${6:-ldap}
    EXTRA_PARAM=${7}
elif [ ${SS} = "recoverpoint" ]; then
    RP_QUICK_PARAM=${4:-'quick'}
    AUTH=${5:-ldap}
    EXTRA_PARAM=${6}
elif [ ${SS} = "sbsdk" ]; then
    SBSDK_QUICK_PARAM='quick'
    AUTH=${5:-ldap}
    EXTRA_PARAM=${6}
elif [ ${SS} = "xtremio" ]; then
    XIO_QUICK_PARAM=${4:-'quick'}
    if [ "$XIO_QUICK_PARAM" != "quick" ] && [ "$XIO_QUICK_PARAM" = "hlu" ]; then
        XIO_HLU_PARAM=1
    elif [ ${XIO_QUICK_PARAM} = "quick" ]; then
        if [ ${5} = "hlu" ]; then
            XIO_HLU_PARAM=1
            shift
        fi
    fi
    AUTH=${5:-ldap}
    EXTRA_PARAM=${6}
elif [ ${SS} = "snapvx" ]; then
    SNAPVX_QUICK_PARAM=${4:-'noquick'}
    AUTH=${5:-ldap}
    EXTRA_PARAM=${6}

    if [ ${SNAPVX_QUICK_PARAM} = "quick" ]; then
        QUICK=1
    fi
elif [ ${SS} = "srdf" ]; then
    SRDF_QUICK_PARAM=${4:-'noquick'}
    AUTH=${5:-ldap}
    EXTRA_PARAM=${6}

    if [ ${SRDF_QUICK_PARAM} = "quick" ]; then
        QUICK=1
    fi
elif [ ${SS} = "vdc" ]; then
    VDC_DEFAULT_ENDPOINT=vdc_dummy_endpoint
    VDC_ENDPOINT_B=${4:-$VDC_DEFAULT_ENDPOINT}
    VDC_ENDPOINT_C=${5:-$VDC_DEFAULT_ENDPOINT}
    if [ -n "${BOURNE_IPV6_MODE}" ]; then
	AUTH=ipv6
    else
	AUTH=ldap
    fi
    echo AUTH=$AUTH
elif [ ${SS} = "dr" ]; then
    DR_SITE_B_IP=${4}
    DR_SITE_C_IP=${5}
    if [ -n "${BOURNE_IPV6_MODE}" ]; then
	AUTH=ipv6
    else
	AUTH=ldap
    fi
    echo AUTH=$AUTH
elif [ ${SS} = "backuprestore" ]; then
    if [[ $# -lt 6 ]]; then
	echo "Invalid backuprestore parameters."
	echo "usage: backuprestore backup-server-uri username password domain"
	exit 1
    fi

    BACKUP_SERVER_URL=$4
    BACKUP_SERVER_USERNAME=$5
    BACKUP_SERVER_PASSWORD=$6
    BACKUP_SERVER_DOMAIN=$7

    if [[ $BACKUP_SERVER_URL == smb* ]]; then
    BACKUP_SERVER_TYPE="CIFS"
    elif [[ $BACKUP_SERVER_URL == ftp* ]] || [[ $BACKUP_SERVER_URL == ftps* ]];then
    BACKUP_SERVER_TYPE="FTP"
    fi

    if [ -n "${BOURNE_IPV6_MODE}" ]; then
	AUTH=ipv6
    else
	AUTH=ldap
    fi
    echo AUTH=$AUTH
elif [ ${SS} = "unityfile" ]; then
    # Unity tests cannot be run with sanity ldap configuration. So use local authentication.
    AUTH=local
    echo AUTH=$AUTH
elif [ ${SS} = "unityblock" ]; then
    UNITY_OPTION=${4:-'all'}
    AUTH=${5:-ldap}
    EXTRA_PARAM=${6}

    echo "option: " $UNITY_OPTION
else
    # By default, we're not running quick sanity
    if [ ${SS} = "quick" ]; then
	QUICK=1
	RP_INGESTTESTS=0
    else
        QUICK=0
    fi
    echo "QUICK: " $QUICK

    AUTH=${4:-ldap}
    EXTRA_PARAM=${5}
fi

[ -z "${AUTH}" ] && AUTH=${4:-ldap}

DO_SETUP_ONLY=0
for a in $* 
do
    if [ "$a"x = "-setup_onlyx" ]; then
        DO_SETUP_ONLY=1
    fi
done

case $SS in
    all|snapvx|isilon|vplex|vplexexport|vplexsnap|netapp|netappc|recoverpoint|srdf|vnxfile|datadomainfile|vnxblock|vnxblock_flex_varray|hds|xtremio|vmaxblock|ingestblock|vmaxblock_flex_varray|combined_block|syssvc|blocksnapshot|blockmirror|full_copy|blockconsistencygroup|security|monitor|audit|ui|init|quick|errorhandling|vdc|vnxe|recovery|backuprestore|sanzoning|dr|ipsec|application|sbsdk|ceph|unityblock|unityfile|custvolname|arrayaffinity|portgroup)
    ;;

    # These tests are handled by sanity integration tests written with the code
    catalog)
    echo "Running external sanity: ./gradlew -q :sanity:catalog -DSANITY_IP=${BOURNE_IP}"
    (cd $(dirname $0)/../../..; ./gradlew -q :sanity:catalog -DSANITY_IP=${BOURNE_IP})
    exit 0
    ;;

    *)
    Usage
esac

# Webstorage parameters taken from the environment if set.
WS_SETUP_FILESHARE_COUNT=${WS_SETUP_FILESHARE_COUNT:-1}
WS_SETUP_ISILON_FS_SIZE=${WS_SETUP_ISILON_FS_SIZE:-104857600000}
WS_SETUP_VNX_FS_SIZE=${WS_SETUP_VNX_FS_SIZE:-104857600000}
WS_SETUP_NETAPP_FS_SIZE=${WS_SETUP_NETAPP_FS_SIZE:-53687091200}
WS_SETUP_NFS_FS_SIZE=${WS_SETP_NFS_FS_SIZE:-10240000}
WS_SETUP_MODE=0
WS_SETUP_COS=""
WS_SETUP_BUCKETS=""
WS_SETUP_GEO_REPGROUP=${WS_SETUP_GEO_REPGROUP:-""}
if [ ${WS_SETUP+x} ] ; then
    WS_SETUP_MODE=1
    echo "WS_SETUP mode is enabled, output file=${WS_SETUP}"
fi

case $AUTH in
    local|ldap)
    ;;
    ipv6)
        export BOURNE_IPV6_MODE="YES"
	ISI_IP="2620:0:170:2842::6199"
	ISI_SN="6805ca0e0736b7cf7b52af25f8f67c8e6d35"
	VNXF_IP="2620:0:170:2860::60"
	VNXDM_IP="2620:0:170:2860::60"
	VNXF_SN="APM00112400154"
	NETAPPF_IP="2620:0:170:2842:2a0:98ff:fe3d:bab4"
	NETAPPDM_IP="2620:0:170:2842:2a0:98ff:fe3d:bab4"
	NETAPPF_SN="700001322607"
	VMAX_SMIS_IP="2620:0:170:2842:250:56ff:fe91:56af"
	VNX_SMIS_IP="2620:0:170:2842:250:56ff:fe91:56af"
	VNXB_SN="APM00112900837"
	VMAX_SN="000195701430"
	VPLEX_VMAX_SMIS_IP="2620:0:170:2842:250:56ff:fe91:56af"
	VPLEX_VNX1_SMIS_IP="2620:0:170:2842:250:56ff:fe91:56af"
	VPLEX_VMAX_NATIVEGUID="000195701430"
	VPLEX_VNX1_NATIVEGUID="APM00112900837"
	VPLEX_IP="2620:0:170:2842::208"
	VPLEX_GUID="VPLEX+FNM00130900344:FNM00131100242"
	HDS_PROVIDER="lglap095.lss.emc.com"
	HDS_PROVIDER_IP="2620:0:170:2858::95";;
    *)
    Usage
esac

ethdev=`/sbin/ifconfig | grep Ethernet | head -1 | awk '{print $1}'`
if [ "$ethdev" = "" ] ; then
   ethdev=eth0
fi
# COP-16713: Sometimes a linux ethernet interface is more than 9 characters
#            However, ifconfig will only show the first 9.  If the name is 
#            long, run "ip link show", which will give us the full name.
if [ ${#ethdev} -gt 8 ] ; then
    ethdev=`ip link show | grep ${ethdev} | awk '{print $2}' | awk -F: '{print $1}'`
fi

macaddr=`/sbin/ifconfig ${ethdev} | /usr/bin/awk '/HWaddr/ { print $5 }'`
if [ "$macaddr" = "" ] ; then
    macaddr=`/sbin/ifconfig en0 | /usr/bin/awk '/ether/ { print $2 }'`
fi

# Generate a wwn segment for initiators that is unique to this run.
#
# 00:50:56:9F
hostwwnseg1=`echo ${macaddr} | cut -c1-11`

# 00:50:56:9F:34
hostwwnseg2=`echo ${RANDOM} | cut -c1-2`

# 00:50:56:9F:34:81
hostwwnseg3=`echo ${RANDOM} | cut -c1-2`

wwnsegment="${hostwwnseg1}:${hostwwnseg2}:${hostwwnseg3}"

seed=`date "+%H%M%S%N"`
seed2b=`printf "%02X" $$ | cut -b1-2`
hostseed="${hostwwnseg2}${hostwwnseg3}"
hostbase=host${hostseed}
export BOURNE_API_SYNC_TIMEOUT=700

#
# Zone configuration
#
NH=nh
NH2=nh2
NH3=nh3
NH4=nh4
REMOTE_NH=remotenh
FC_ZONE_A=fctz_a
FC_ZONE_B=fctz_b
IP_ZONE=iptz
REM_IP_ZONE=remiptz
IP_ZONE2=iptz2
IP_ZONE3=iptz3

FCTZ_A=$NH/$FC_ZONE_A
FCTZ_B=$NH/$FC_ZONE_B

#
# Vdc Configuration
#
VDC_ENDPOINT_B_NAME=vdc_name_B_$$
VDC_ENDPOINT_B_SECRETKEY=
VDC_ENDPOINT_B_CERTCHAIN=
VDC_ENDPOINT_B_ID=
VDC_TEST_PROJECT=vdc_project_$$
VDC_TEST_DISCONN_RECONN_PROJECT=disrecon_vdc_project_$$
VDC_TEST_REMOVE_PROJECT=testremove_vdc_project_$$

#
# Disaster Recovery Configuration
#
DR_SITE_B_NAME=dr_site_B_$$
DR_SITE_B_DESCRIPTION="Site B description"
DR_SITE_C_NAME=dr_site_C_$$
DR_SITE_C_DESCRIPTION="Site C description"
DR_SITE_A_NAME="Active"
#
# Recovery Configuration
#
RECOVERY_CORRUPTED_NODE_IP=

#
# bourne tenant & project configuration
#
SHORTENED_HOST=`hostname`
: ${TENANT=$SHORTENED_HOST}
: ${PROJECT=sanity}
: ${SRDF_PROJECT=dontuseme}
: ${UNITY_PROJECT=unity}

SVCUSER=svcuser

#Local ldap server config.
LOCAL_LDAP_AUTHN_NAME='Local_Ldap_Provider'
LOCAL_LDAP_AUTHN_MODE='ldap'
LOCAL_LDAP_AUTHN_URLS='ldap://'${LOCAL_LDAP_SERVER_IP}
LOCAL_LDAP_AUTHN_DOMAINS=VIPRSANITY.COM
LOCAL_LDAP_AUTHN_SEARCH_BASE='ou=ViPR,dc=viprsanity,dc=com'
LOCAL_LDAP_AUTHN_MANAGER_DN='cn=manager,dc=viprsanity,dc=com'
LOCAL_LDAP_AUTHN_MANAGER_PWD=secret
LOCAL_LDAP_AUTHN_SEARCH_FILTER='uid=%U'
LOCAL_LDAP_AUTHN_AUTHN_GROUP_ATTR=CN
LOCAL_LDAP_AUTHN_PROVIDER_NEWNAME='Local_Ldap_Provider_Updated_Name'
LOCAL_LDAP_AUTHN_WHITELIST='ldapViPR*'
LOCAL_LDAP_AUTHN_SEARCH_SCOPE=SUBTREE
LOCAL_LDAP_AUTHN_GROUP_OBJECT_CLASSES='groupOfNames,groupOfUniqueNames,posixGroup,organizationalRole'
LOCAL_LDAP_AUTHN_GROUP_MEMBER_ATTRIBUTES='member,uniqueMember,memberUid,roleOccupant'

#Local ldap server user and group config.
LOCAL_LDAP_VIPR_USER_GROUP='  ldapViPRGroup_RootTenantOuter    '
LOCAL_LDAP_TENANT_ATTRIBUTE_KEY=title
LOCAL_LDAP_TENANT_ATTRIBUTE_ROOT_TENANT_VALUE=RootTenantUser
LOCAL_LDAP_TENANT_ATTRIBUTE_ROOT_SUBTENANT1_VALUE=SubTenant1User
LOCAL_LDAP_TENANT_ATTRIBUTE_ROOT_SUBTENANT2_VALUE=SubTenant2User
LOCAL_LDAP_SUPERUSER_USERNAME=ldapViPRUser1@${LOCAL_LDAP_AUTHN_DOMAINS}
LOCAL_LDAP_SUPERUSER_PASSWORD=secret
LOCAL_LDAP_GROUPUSER_USERNAME=ldapViPRUser2@${LOCAL_LDAP_AUTHN_DOMAINS}
LOCAL_LDAP_GROUPUSER_PASSWORD=secret
LOCAL_LDAP_TENANTADMIN_USERNAME=ldapViPRUser3@${LOCAL_LDAP_AUTHN_DOMAINS}
LOCAL_LDAP_TENANTADMIN_PASSWORD=secret
LOCAL_LDAP_TENANT_USERNAME=ldapViPRUser4@${LOCAL_LDAP_AUTHN_DOMAINS}
LOCAL_LDAP_TENANT_PASSWORD=secret
LOCAL_LDAP_TENANT_PROJECT_ADMINS_GROUP=ldapViPRGroup_RootTenantProjectAdmins@${LOCAL_LDAP_AUTHN_DOMAINS}
LOCAL_LDAP_PROJECT_ADMIN_USERNAME=ldapViPRUser5@${LOCAL_LDAP_AUTHN_DOMAINS}
LOCAL_LDAP_PROJECT_ADMIN_PASSWORD=secret
LOCAL_LDAP_MAXGROUPSUSER_USERNAME=ldapViPRUser5@${LOCAL_LDAP_AUTHN_DOMAINS}
LOCAL_LDAP_MAXGROUPSUSER_PASSWORD=secret
LOCAL_LDAP_USER_USERNAME_1=ldapViPRUser6@${LOCAL_LDAP_AUTHN_DOMAINS}
LOCAL_LDAP_USER_PASSWORD_1=secret
LOCAL_LDAP_USER_USERNAME_2=ldapViPRUser7@${LOCAL_LDAP_AUTHN_DOMAINS}
LOCAL_LDAP_USER_PASSWORD_2=secret


#Secure ldap server config.
LOCAL_SECURE_LDAP_AUTHN_NAME='Local_Secure_Ldap_Provider'
LOCAL_SECURE_LDAP_AUTHN_URLS='ldaps://'${LOCAL_LDAP_SERVER_IP}
LOCAL_SECURE_LDAP_AUTHN_DOMAINS=SECURE.VIPRSANITY.COM
LOCAL_SECURE_LDAP_AUTHN_SEARCH_BASE='dc=secure,dc=viprsanity,dc=com'
LOCAL_SECURE_LDAP_AUTHN_MANAGER_DN='uid=secureManagerDN,dc=secure,dc=viprsanity,dc=com'
LOCAL_SECURE_LDAP_AUTHN_MANAGER_PWD=secret
LOCAL_SECURE_LDAP_AUTHN_WHITELIST='secureViPRGroup*'

#Secure ldap server user and group config.
LOCAL_SECURE_LDAP_TENANT_ATTRIBUTE_KEY=title
LOCAL_SECURE_LDAP_TENANT_ATTRIBUTE_VALUE=SecureViPRUser
LOCAL_SECURE_LDAP_USER_USERNAME=secureLdapViPRUser1@${LOCAL_SECURE_LDAP_AUTHN_DOMAINS}
LOCAL_SECURE_LDAP_USER_PASSWORD=secret
LOCAL_SECURE_LDAP_USER_USERNAME_WITH_SPACES='       secureLdapViPRUser1@'${LOCAL_SECURE_LDAP_AUTHN_DOMAINS}'       '


#
# cos configuration
#
COS_VNXFILE=cosvnxf
COS_VNXE=cosvnxe
COS_UNITY=cosunity
COS_UNITY_CIFS=cosunitycifs
COS_ISIFILE=cosisi
REM_COS_ISIFILE=remotecosisi
COS_NETAPP=cosnetappf
COS_NETAPPC=cosnetappcf
COS_DDFILE=cosdatadomainf
COS_VNXBLOCK=cosvnxb
COS_VNXBLOCK_FC=cosvnxb_fc
COS_VNXBLOCK_ISCSI=cosvnxb_iscsi
COS_VNXBLOCK_THIN=cosvnxb_thin
COS_VNXBLOCK_THICK=cosvnxb_thick
COS_VMAXBLOCK=cosvmaxb
COS_VMAXBLOCK_FC=cosvmaxb_fc
COS_VMAXBLOCK_ISCSI=cosvmaxb_iscsi
COS_VMAXBLOCK_THIN=cosvmaxb_thin
COS_VMAXBLOCK_THICK=cosvmaxb_thick
COS_VMAXBLOCK_SRDF_SOURCE=vpool_srdf_source
COS_VMAXBLOCK_SRDF_TARGET=vpool_srdf_target
COS_VMAXBLOCK_V3_SRDF_SOURCE=vpool_v3_srdf_source
COS_VMAXBLOCK_V3_SRDF_TARGET=vpool_v3_srdf_target
COS_VMAXBLOCK_V3_COMP_ENABLED=cosvmaxb_v3_comp_enabled
COS_VMAXBLOCK_V3_COMP_NOTENABLED=cosvmaxb_v3_comp_disabled
COS_RP=cos_rp
COS_RP_BASE=cos_rp_base
COS_HDS=coshds
COS_VNXEBLOCK_CG=cosvnxe_cg
COS_VNXEBLOCK_FC=cosvnxe_fc
COS_VNXEBLOCK_ISCSI=cosvnxe_iscsi
COS_ECS=ecs_vpool
COS_UNITYBLOCK_CG=cosunity_cg
COS_ARRAYAFFINITY=cos_arrayaffinity
COS_ARRAYAFFINITY_VMAX=cos_arrayaffinity_vmax
COS_ARRAYAFFINITY_VNX=cos_arrayaffinity_vnx
HOST_ARRAYAFFINITY=$hostbase
HOST_ARRAYAFFINITY_FQDN=${HOST_ARRAYAFFINITY}.lss.emc.com
HOST_ARRAYAFFINITY_URI=""

#
# Isilon configuration
#
ISI_DEV=isilon_device
ISI_SMARTCONNECT_IP=${ISI_IP}
ISI_NATIVEGUID=ISILON+$ISI_SN
REM_ISI_DEV=remote_isilon_device
REM_ISI_SMARTCONNECT_IP=${REM_ISI_IP}
REM_ISI_NATIVEGUID=ISILON+$REM_ISI_SN
ISI_SMBFILESHARE1=smbfileshare1$(date +%Y%m%d%H%M%S) 
ISI_SMBFILESHARE2=smbfileshare2$(date +%Y%m%d%H%M%S)
ISI_SMBSNAPSHARE1=smbsnapshare1$(date +%Y%m%d%H%M%S)
ISI_SMBSNAPSHARE2=smbsnapshare2$(date +%Y%m%d%H%M%S) 

#
# ECS configuration
#
ECS_DEV=ecs_device
ECS_NATIVEGUID=ECS+$ECS_SN
ECS_SOFT_QUOTA=1024
ECS_HARD_QUOTA=2048
ECS_BUCKET=bucket-${RANDOM}

#
# VNX file device configuration
#
VNXF_DEV=vnxf_device
VNXF_NATIVEGUID=CELERRA+$VNXF_SN
VNXF_SMBFILESHARE1=smbfileshare1$(date +%Y%m%d%H%M%S) 

#
# VMAX3 configuration
#
VMAX3_SMIS_DEV=VMAX3_SMIS_DEV
COS_VMAX3BLOCK_FC=COS_VMAX3BLOCK_FC

#
# XtremIO config
#

XTREMIO=xtremio4
XTREMIOVOL=volXtremIO
XTREMIO_3X_NATIVEGUID=XTREMIO+$XTREMIO_3X_SN
XTREMIO_4X_NATIVEGUID=XTREMIO+$XTREMIO_4X_SN
XTREMIO_3X_COS_FC=xtremioTest3
XTREMIO_4X_COS_FC=xtremioTest4
XTREMIO_INGEST_HOST=ingesthost.lss.emc.com
XTREMIO_INGEST_HOST_LABEL=ingesthost

# XtremIO 2nd setup
XTREMIO2=xtremio2
XTREMIO2_NATIVEGUID=XTREMIO+$XTREMIO2_SN

#
# Netapp file device configuration
#
NETAPPF_DEV=netapp_device
NETAPPF_NATIVEGUID=NETAPP+$NETAPPF_SN
NETAPPF_SMBFILESHARE1=smbfileshare1$(date +%Y%m%d%H%M%S) 
NETAPPF_SMBFILESHARE2=smbfileshare2$(date +%Y%m%d%H%M%S)
NETAPPF_SMBSNAPSHARE1=smbsnapshare1$(date +%Y%m%d%H%M%S)
NETAPPF_SMBSNAPSHARE2=smbsnapshare2$(date +%Y%m%d%H%M%S)  

#
# NetApp Cluster Mode device configuration
#
NETAPPCF_DEV=netappc_device
NETAPPCF_NATIVEGUID=NETAPPC+$NETAPPCF_SN
NETAPPCF_SMBFILESHARE1=smbfileshare1$(date +%Y%m%d%H%M%S)
NETAPPCF_SMBFILESHARE2=smbfileshare2$(date +%Y%m%d%H%M%S)
NETAPPCF_SMBSNAPSHARE1=smbsnapshare1$(date +%Y%m%d%H%M%S)
NETAPPCF_SMBSNAPSHARE2=smbsnapshare2$(date +%Y%m%d%H%M%S)

#
# VNXE  device configuration
#
VNXE_DEV=vnxe_device
VNXE_NATIVEGUID=VNXE+$VNXE_SN
VNXE_SMBFILESHARE1=cifs1$(date +%Y%m%d%H%M%S) 


#
# Ceph cluster configuration
#
CEPH_PROVIDER=ceph_test_cluster
CEPH_CLIENT_HOST_NAME=ceph_client
CEPH_COS=ceph_vpool
CEPH_VOLNAME=EXPCephTest${RANDOM}
CEPH_EXPORT_GROUP_NAME=EXPCephGroup${RANDOM}

# UNITY  device configuration
#
UNITY_DEV=${UNITY_DEV:-unity_device}
UNITY_NATIVEGUID=${UNITY_NATIVEGUID:-UNITY+$UNITY_SN}
UNITY_SMBFILESHARE1=sanity_cifs1$(date +%Y%m%d%H%M%S) 

#
# DataDomain file device configuration
#
DATADOMAINF_DEV=datadomainf_device
DATADOMAINF_PROVIDER=datadomain_provider
DATADOMAINF_PROVIDER_INTERFACE=ddmc
DATADOMAINF_NATIVEGUID=$DATADOMAINF_ID
DATADOMAINF_SMBFILESHARE1=smbfileshare1$(date +%Y%m%d%H%M%S)
DATADOMAINF_SMBFILESHARE2=smbfileshare2$(date +%Y%m%d%H%M%S)

#
# SRDF Configuration
#
SRDF_VOLUME=srdfSanity-${HOSTNAME}-${RANDOM}
SRDF_V3_VMAXA_SID=${SRDF_V3_VMAXA_NATIVEGUID:10:24}
SRDF_V3_VMAXB_SID=${SRDF_V3_VMAXB_NATIVEGUID:10:24}
SRDF_SSH_ERROR=0
SRDF_SSH_RESULT=""
SRDF_USED_RDFGS=()
SRDF_RDF_NUMBER=1
SRDF_RDF_CREATE_RETRY=1 # number of times for retry in case of group name/number collision

ssleep() {
    secho "Sleeping $1 seconds"
    sleep $1
}

set_validation_check() {
    run syssvc $CONFIG_FILE $BOURNE_IPADDR set_prop validation_check $1
}

srdf_ssh(){
    SRDF_SSH_ERROR=0
    SRDF_SSH_RESULT=""
    SRDF_SSH_RESULT=$(sshpass -p $2 ssh -q -o StrictHostKeyChecking=no -o UserKnownHostsFile=no root@$1 $3) || SRDF_SSH_ERROR=$?
}

srdf_generate_rdfg_name() {
    # RDF group name maximum 10 characters
    echo $(date +"S"%m%d$RANDOM | cut -c1-10)
}

SRDF_SLEEPING_SECONDS=180
srdf_sleep() {
    ssleep ${SRDF_SLEEPING_SECONDS}
}

setup_yaml() {
    storage_type=$1
    tools_file="${BASEDIR}/export-tests/tools.yml"
    if [ -f "$tools_file" ]; then
	echo "Stale $tools_file found. Deleting it."
	rm $tools_file
    fi

    echo "Creating ${tools_file}"
    touch $tools_file

    if [ "$storage_type" = "unity" ]; then
        printf 'array:\n  %s:\n  - ip: %s:%s\n    username: %s\n    password: %s' "$storage_type" "$UNITY_IP" "$UNITY_PORT" "$UNITY_USER" "$UNITY_PW" >> $tools_file
        return
    fi

    storage_sn=$2
    storage_password=$3
    if [ "${storage_password}" = "" ]; then
	echo "storage_password is not set. Cannot make a valid tools.yml file without a storage_password"
	exit;
    fi

    storage_id=`storagedevice list | grep COMPLETE | grep ${storage_sn} | awk '{print $5}'`
    storage_version=`storagedevice show ${storage_id} | grep firmware_version | awk '{print $2}' | cut -d '"' -f2`
    storage_ip=`storagedevice show ${storage_id} | grep smis_provider_ip | awk '{print $2}' | cut -d '"' -f2`
    storage_port=`storagedevice show ${storage_id} | grep port_number | awk '{print $2}' | cut -d ',' -f1`
    storage_user=`storagedevice show ${storage_id} | grep user_name | awk '{print $2}' | cut -d '"' -f2`

    # create the yml file to be used for array tooling
    printf 'array:\n  %s:\n  - ip: %s:%s\n    id: %s\n    username: %s\n    password: %s\n    version: %s' "$storage_type" "$storage_ip" "$storage_port" "$storage_sn" "$storage_user" "$storage_password" "$storage_version" >> $tools_file
}

#
# RecoverPoint configuration
#
RP_SYSTEM_TYPE=rp
RP_REMOVE_BAD_CHARS_HOSTNAME=${HOSTNAME//[^a-zA-Z0-9]/}
RP_MODIFIED_HOSTNAME=${RP_REMOVE_BAD_CHARS_HOSTNAME:0:8}
RP_SANITY_RANDOM=$((RANDOM % 999))

RP_VOLUME=rp-${RP_MODIFIED_HOSTNAME}-${RP_SANITY_RANDOM}-vol
RP_CONSISTENCY_GROUP=rp-${RP_MODIFIED_HOSTNAME}-${RP_SANITY_RANDOM}-cg

RP_VPLEX_VOLUME=rpvp-${RP_MODIFIED_HOSTNAME}-${RP_SANITY_RANDOM}-vol
RP_VPLEX_CONSISTENCY_GROUP=rpvp-${RP_MODIFIED_HOSTNAME}-${RP_SANITY_RANDOM}-cg

RP_METROPOINT_VOLUME=mp-${RP_MODIFIED_HOSTNAME}-${RP_SANITY_RANDOM}-vol
RP_METROPOINT_CONSISTENCY_GROUP=mp-${RP_MODIFIED_HOSTNAME}-${RP_SANITY_RANDOM}-cg

RP_XIO_VOLUME=rpx-${RP_MODIFIED_HOSTNAME}-${RP_SANITY_RANDOM}-vol
RP_XIO_CONSISTENCY_GROUP=rpx-${RP_MODIFIED_HOSTNAME}-${RP_SANITY_RANDOM}-cg

RP_UNITY_VOLUME=rpu-${RP_MODIFIED_HOSTNAME}-${RP_SANITY_RANDOM}-vol
RP_UNITY_CONSISTENCY_GROUP=rpu-${RP_MODIFIED_HOSTNAME}-${RP_SANITY_RANDOM}-cg

RP_EXPORT_GROUP=rp-${RP_MODIFIED_HOSTNAME}-${RP_SANITY_RANDOM}
RP_EXPORT_GROUP_HOST=host${hostseed}.sanity.com

#
# Configuration for Simulator
#
RP_PROVIDER_SIMULATOR_IP=$SIMULATOR_IP
SIMULATOR_CISCO_MDS_IP=$SIMULATOR_IP
RP_SIMULATOR_IP=$SIMULATOR_IP
RP_SIMULATOR=rp-sim
PROVIDER_SIMULATOR=provider-sim
FABRIC_SIMULATOR=fabric-sim
SIMULATOR_VSAN_11=VSAN_11
SIMULATOR_VSAN_12=VSAN_12
VPLEX_PROVIDER_SIMULATOR=$PROVIDER_SIMULATOR
VPLEX_PROVIDER_SIMULATOR_IP=$SIMULATOR_IP
VPLEX_SIMULATOR=vplex-sim

#
# Full copy configuration
#
FULL_COPY_VOLUME=full-copy-test-${HOSTNAME}-${RANDOM}

#
# MIRROR (VMAX) block device configuration
#
COS_MIRROR=cosmirror
COS_MIRROR_WITH_OPTIONAL=cosmirror_with_optional
COS_MIRROR_WITH_2_MIRRORS=cosmirror_max_2
COS_MIRROR_BEFORE_CHANGE=cosmirror_before_change
COS_MIRROR_AFTER_CHANGE=cosmirror_after_change
COS_MIRROR_VNX=cosmirror_vnx
COS_VMAX_CG_MIRROR=cosvmax_cg_mirror

#
# Export group configuration
#
VNX_VOLUME=VnxSanityVol-${HOSTNAME}-${RANDOM}
VNX_META_VOLUME=VnxSanityMeta-${HOSTNAME}-${RANDOM}
VNXEXPORT_GROUP=VnxExp${RANDOM}
VNXEXPORT_GROUP_HOST=host.vnx.export${seed}
VMAX_VOLUME=VmaxSanity-${HOSTNAME}-${RANDOM}
VMAX_META_VOLUME=VmaxSanityMeta-${HOSTNAME}-${RANDOM}
VMAXEXPORT_GROUP=VmaxExp${RANDOM}
VMAXEXPORT_GROUP_HOST=host.vnx.export${seed}
VMAX_VNXEXPORT_GROUP=VmaxVnxExp-${RANDOM}
VMAX_VNXEXPORT_GROUP_HOST=vmax.vnx.export${seed}
XTREMIOEXPORT_GROUP_HOST=host.xtremio.export${seed}
XTREMIOEXPORT_GROUP=XtremIOExp${RANDOM}
BLOCKEXPORT_GROUP=BlockSanityExportGroup-${HOSTNAME}-${RANDOM}
MIRROR_VOLUME=VmaxMirror-${HOSTNAME}-${RANDOM}
MIRROR_VOLUME_VNX=VnxMirror-${HOSTNAME}-${RANDOM}
CONSISTENCY_GROUP_SRDF=${RANDOM}
CONSISTENCY_GROUP=consistency-group-`date +%s | cut -c5-10`
CONSISTENCY_GROUP_SNAPSHOT=group-snapshot-`date +%s | cut -c5-10`
VNX_COS_GROUP=vnx-cos-group-`date +%s | cut -c5-10`
VMAX_COS_GROUP=vmax-cos-group-`date +%s | cut -c5-10`

#
# fileshare tests configuration
#
# Min 1GB FS
FS_SIZE=1073741824
FS_SIZEMB=1024MB
# Expand size 1.5GB FS
FS_EXPAND_SIZE=1610612736
FSEXP_RO_EPS="www.ferrari.com www.porsche.com"
FSEXP_RW_EPS="www.lexus.com www.infiniti.com www.acura.com"
FSEXP_SHARED_VARRAY_RW_EPS="client1.emc.com client2.emc.com"
FSEXP_ROOT_EPS="www.honda.com"
FSEXP1="www.ford.com"
FSEXP2="www.gmc.com"
FSEXP3="www.pontiac.com"
FSEXP4="www.kia.com"
FSEXP_DEFAULT_EPS="$FSEXP1 $FSEXP2 $FSEXP3"
SNAPEXP_DEFAULT_EPS="www.emc.com www.abc.com www.amazon.com"
FS_VNXE_SIZE=2000000000
FS_VNXE_EXPAND_SIZE=3000000000
FS_UNITY_SIZE=4000000000
FS_UNITY_EXPAND_SIZE=5000000000


# Alternate configuration file for alternate hardware/variables
ALTERNATE_CONFIG_FILE=myhardware.conf
if [ -f ${ALTERNATE_CONFIG_FILE} ]
then
   source ${ALTERNATE_CONFIG_FILE}
fi

pwwn()
{
    WWN=${WWN:-0}
    idx=$1
    echo 3${WWN}:${wwnsegment}:${idx}
}

nwwn()
{
    WWN=${WWN:-0}
    idx=$1
    echo 4${WWN}:${wwnsegment}:${idx}
}

wwnIdx() {
    i=$1
    j=$2
    k=$(($i - 1))
    k=$((2 * $k))
    k=$(($j + $k))
    echo $k
}

#
# block tests configuration
#
BLK_SIZE=1073741824
BLK_SIZE_EXPAND=2147483648 # 2GB
BLK_SIZE_EXPAND_2=2462056448 # 2GB + 300MB
BLK_SIZE_EXPAND_3=2566914048 # 2GB + 400MB
BLK_LUN1=1
BLK_LUN2=2
BLK_LUN3=3
BLK_CLIENT_FC=`pwwn ${seed2b}`
BLK_CLIENT_FC_NODE=`nwwn ${seed2b}`
HOSTNAME=`hostname`
BLK_CLIENT_iSCSI=iqn.2010-01.com.emc.snafu-${HOSTNAME}_${seed}
BLK_HOSTID=sanity-host-${HOSTNAME}-${seed}

export BOURNE_IPADDR="$BOURNE_IP"

source sanity_utils
source subs/zones

# search configuration
TAG=$(tr -dc A-Za-z0-9_ < /dev/urandom | head -c 4)
SEARCH_PREFIX=$(echo $TAG|head -c 2)

# Place to put command output in case of failure
CMD_OUTPUT=/tmp/output.txt
rm -f ${CMD_OUTPUT}

# General echo output
secho()
{
    echo "*** " `date` " $*"
}

# General header echo output
hecho()
{
    echo "********************************************"
    echo $1
    echo "********************************************"
}    

#
# run commands and check for exit status
# Do not add to the undo list; mostly because the "create" version
# of the command does not match the "delete" in arguments, like 
# some blocksnapshot create or multi-volume create commands
#
run_noundo()
{
    cmd=$*
    echo === $cmd
    rm -f ${CMD_OUTPUT}
    if [ "${HIDE_OUTPUT}" = "" -o "${HIDE_OUTPUT}" = "1" ]; then
	$cmd &> ${CMD_OUTPUT}
    else
	$cmd 2>&1
    fi
}

#
# run commands and check for exit status
#
run()
{
    run_noundo $*
    set_undo $*
}

balance()
{
    cmd=$*
    echo $cmd
    $cmd --ip=${BOURNE_IP_ARRAY[$IP_INDEX]}
    if [ $((IP_INDEX+1)) -ge ${#BOURNE_IP_ARRAY[@]} ]; then 
        IP_INDEX=0
    else
        IP_INDEX=$((IP_INDEX+1))
    fi       
}

#
# Creates a neighborhood
# Sets up one IP transport zone and one FC transport zone
#
zone_setup()
{
    # do this only once
    neighborhood show $NH  &> /dev/null && return $?

    run neighborhood create $NH
    if [ "$EXTRA_PARAM" = "search" ] ; then
        neighborhood search $(echo $NH | head -c 2)
        run neighborhood tag $NH $TAG
        neighborhood search $SEARCH_PREFIX --tag true
    fi

    run neighborhood create $NH2

    run transportzone create $IP_ZONE $NH --type IP
    if [ "$EXTRA_PARAM" = "search" ] ; then
        transportzone search $(echo $IP_ZONE | head -c 2)
        run transportzone tag $NH/$IP_ZONE $TAG
        transportzone search $SEARCH_PREFIX --tag true
    fi

    # Set the object transport zone
    # Cleanup just in case... Ignore errors because grep will exit with non-zero if
    # it doesn't match anything (which is the case if the object transport zone
    # isn't set).
    trap - ERR

    run transportzone add $NH/$IP_ZONE $FSEXP1
    run transportzone add $NH/$IP_ZONE $FSEXP2
    run transportzone add $NH/$IP_ZONE $FSEXP3

    #
    # set up zone for cisco switch simulator
    #
    if [ $QUICK -eq 1 -o "${VPLEX_QUICK_PARAM}" = "quick" -o "${SBSDK_QUICK_PARAM}" = "quick" ]; then
        cisco_mds_quick_setup_once
    elif [ $DISCOVER_SAN -eq 1 ]; then
        brocade_setup_once
    else
        run transportzone create $FC_ZONE_A $NH --type FC
        run transportzone create $FC_ZONE_B $NH2 --type FC
        run transportzone add    $NH/$FC_ZONE_A $BLK_CLIENT_FC
    fi

    for i in A1 A2 A3 A4 A5 A6 A7 A8 C1 C2 C3 C4 C5 C6 C7 C8
    do
        wwn=`pwwn $i`
	run transportzone add $FCTZ_A $wwn
    done

    for i in B1 B2 B3 B4 B5 B6 B7 B8 D1 D2 D3 D4 D5 D6 D7 D8
    do
        wwn=`pwwn $i`
        run transportzone add $FCTZ_B $wwn
    done
    trap '_failure $LINENO' ERR
	
}

brocade_setup_once() 
{
    # Do once
    nsys=`networksystem list | wc -l`
    [ "$nsys" -gt 0 ] && return;

    #Discover the Brocade SAN switch.
    secho "Discovering brocade ..."
    run networksystem create $BROCADE_NETWORK brocade --smisip $BROCADE_IP --smisport 5989 --smisuser $BROCADE_USER --smispw $BROCADE_PW --smisssl true
    sleep 30
    export NETWORK_SYSTEM=$BROCADE_NETWORK

    run transportzone assign "FABRIC_losam082-fabric" $NH
    run transportzone assign "FABRIC_VPlex_LGL6221_FID_40" $NH
    run transportzone assign "FABRIC_vplex154nbr2" $NH2
    VPLEX_TZ1="$NH/FABRIC_losam082-fabric"
    VPLEX_TZ2="$NH2/FABRIC_vplex154nbr2"
    FC_ZONE_A="FABRIC_losam082-fabric"
    FC_ZONE_B="FABRIC_vplex154nbr2"
    FCTZ_A=$NH/$FC_ZONE_A
    FCTZ_B=$NH2/$FC_ZONE_B
}

cisco_mds_quick_setup_once()
{
    # Do once - Discover Cisco MDS simulator switch
    secho "Discover Cisco MDS simulator switch"
    run networksystem create CiscoMdsSimulator  mds --devip $SIMULATOR_CISCO_MDS --devport 22 --username $SIMULATOR_CISCO_MDS_USER --password $SIMULATOR_CISCO_MDS_PW
    export NETWORK_SYSTEM="CiscoMdsSimulator"

    run syssvc $CONFIG_FILE $BOURNE_IPADDR set_prop controller_mds_clone_zoneset true
    run syssvc $CONFIG_FILE $BOURNE_IPADDR set_prop controller_mds_allow_zoneset_commit true
    
    FC_ZONE_A=VSAN_11
    FC_ZONE_B=VSAN_12
    FCTZ_A=$NH/$FC_ZONE_A
    FCTZ_B=$NH2/$FC_ZONE_B

    run transportzone assign VSAN_11 $NH
    run transportzone assign VSAN_12 $NH2

    # add ports to VSAN_11
    run transportzone add $FCTZ_A 51:00:50:56:9F:01:3B:A1
    run transportzone add $FCTZ_A 51:00:50:56:9F:01:3B:A2
    run transportzone add $FCTZ_A 51:00:50:56:9F:01:3B:A3
    run transportzone add $FCTZ_A 51:00:50:56:9F:01:3B:A4
}


#
# Setup clusters, hosts, etc.
# Setup two clusters with 2 hosts each. Each host has initiators
# in transport zone A and B
#
host_setup()
{
    # do this only once
    cluster show $TENANT/sanityCluster1 &> /dev/null && return $?

    secho "Setup hosts and clusters for $TENANT started"
    proj=$PROJECT
    tenant=$TENANT

    for i in 1 2
    do
        cluster=sanityCluster$i
        run cluster create $cluster $tenant --project $proj
        j=1
        while [ $j -lt 3 ]
        do
            host=$hostbase$tenant$i$j
            k=`wwnIdx $i $j`
            nwwn1=`nwwn A$k`
            nwwn2=`nwwn B$k`
            nwwn3=`nwwn C$k`
            nwwn4=`nwwn D$k`
            pwwn1=`pwwn A$k`
            pwwn2=`pwwn B$k`
            pwwn3=`pwwn C$k`
            pwwn4=`pwwn D$k`
            run hosts create $host $tenant Windows ${host}.lss.emc.com --port 8111 --username user --password 'password' --osversion 1.0 --cluster ${tenant}/${cluster}
            run initiator create $host FC ${pwwn1} --node ${nwwn1}
            run initiator create $host FC ${pwwn2} --node ${nwwn2}
            run initiator create $host FC ${pwwn3} --node ${nwwn3}
            run initiator create $host FC ${pwwn4} --node ${nwwn4}
            j=$(( $j + 1 ))
        done
    done

    secho "Setup hosts and clusters for $TENANT ended"
}


#
# create a tenant and project for running the sanity tests
#
tenant_setup()
{
    run security add_authn_provider $LOCAL_LDAP_AUTHN_MODE $LOCAL_LDAP_AUTHN_URLS $LOCAL_LDAP_AUTHN_MANAGER_DN $LOCAL_LDAP_AUTHN_MANAGER_PWD $LOCAL_LDAP_AUTHN_SEARCH_BASE $LOCAL_LDAP_AUTHN_SEARCH_FILTER $LOCAL_LDAP_AUTHN_AUTHN_GROUP_ATTR "$LOCAL_LDAP_AUTHN_NAME" $LOCAL_LDAP_AUTHN_DOMAINS "$LOCAL_LDAP_AUTHN_WHITELIST" $LOCAL_LDAP_AUTHN_SEARCH_SCOPE --group_object_classes "$LOCAL_LDAP_AUTHN_GROUP_OBJECT_CLASSES" --group_member_attributes "$LOCAL_LDAP_AUTHN_GROUP_MEMBER_ATTRIBUTES"
    run security get_authn_provider "$LOCAL_LDAP_AUTHN_NAME"
    if [ "$EXTRA_PARAM" = "search" ] ; then
        run security search_authn_provider $(echo $LOCAL_LDAP_AUTHN_NAME | head -c 2)
        run security tag_authn_provider "$LOCAL_LDAP_AUTHN_NAME" $TAG
        run security search_authn_provider $SEARCH_PREFIX --tag true
    fi

    if [ "$BOURNE_SECURITY_DISABLED" != '1' -a "$AUTH" != 'local' ] ; then
        run tenant add_group $LOCAL_LDAP_AUTHN_DOMAINS "$LOCAL_LDAP_VIPR_USER_GROUP"
        run security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD
        run security login $SYSADMIN $SYSADMIN_PASSWORD
        run tenant add_attribute $LOCAL_LDAP_AUTHN_DOMAINS $LOCAL_LDAP_TENANT_ATTRIBUTE_KEY $LOCAL_LDAP_TENANT_ATTRIBUTE_ROOT_TENANT_VALUE
        run security add_tenant_role subject_id $LOCAL_LDAP_SUPERUSER_USERNAME TENANT_ADMIN
        run security add_zone_role subject_id $LOCAL_LDAP_SUPERUSER_USERNAME SYSTEM_ADMIN
        run security add_zone_role subject_id $LOCAL_LDAP_SUPERUSER_USERNAME SYSTEM_MONITOR
        run security add_zone_role subject_id $LOCAL_LDAP_SUPERUSER_USERNAME SECURITY_ADMIN 
        run security add_zone_role subject_id $LOCAL_LDAP_SUPERUSER_USERNAME SYSTEM_AUDITOR
        run security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
        run security verify_user_roles "SYSTEM_ADMIN,SYSTEM_MONITOR,SECURITY_ADMIN,SYSTEM_AUDITOR,TENANT_ADMIN"
    fi
}

#
# create a tenant and project for running the sanity tests
#
project_setup()
{
    tenant show $TENANT &> /dev/null && return $?
    run tenant create $TENANT $LOCAL_LDAP_AUTHN_DOMAINS 'OU' ${seed}
    secho "Tenant $TENANT created."
    if [ "$EXTRA_PARAM" = "search" ] ; then
        tenant search $(echo $TENANT | head -c 2)
        tenant tag "$TENANT" $TAG
        tenant search $SEARCH_PREFIX --scope $TENANT --tag true
    fi

    project show $PROJECT &> /dev/null && return $?
    run project create $PROJECT --tenant $TENANT 

    if [ "$EXTRA_PARAM" = "search" ] ; then
        project search $(echo $PROJECT | head -c 2)
        project tag "$PROJECT" $TAG
        project search $SEARCH_PREFIX --scope $TENANT --tag true
    fi
}

#
# setup 3 types of cos
# -- file cos for isilon tests
# -- file cos for vnx tests
# -- block cos for vnx FC & iSCSI test
#
isilon_cos_setup()
{
    echo "setting up isilon COS"
	run cos create file $COS_ISIFILE 				\
	--description 'Virtual-Pool-Isilon' true \
                         --protocols NFS CIFS --max_snapshots 10    \
                         --provisionType 'Thin' \
		         --neighborhoods $NH
    ROOT_TENANT=`tenant root|tail -1`
    run cos allow $COS_ISIFILE file $ROOT_TENANT
    if [ "$EXTRA_PARAM" = "search" ] ; then
        cos search $(echo $COS_ISIFILE | head -c 2) --resource_type file_vpool
        cos tag "$COS_ISIFILE" "file" $TAG
        cos search $SEARCH_PREFIX --tag true --resource_type file_vpool
    fi
	
}

ecs_cos_setup()
{
    echo "setting up ECS cos-vpool"
    run cos create object $COS_ECS --description 'cos_vpool-for-ECS' true --protocols S3 --provisionType 'Thick' --neighborhoods $NH
}

vnxfile_cos_setup()
{
    echo "setting up VNX COS"
    run cos create file $COS_VNXFILE 				\
	--description 'Virtual-Pool-for-VNX-file' true \
                         --protocols NFS CIFS --max_snapshots 10    \
                         --provisionType 'Thin' \
		         --neighborhoods $NH
    ROOT_TENANT=`tenant root|tail -1`
    run cos allow $COS_VNXFILE file $ROOT_TENANT
}

netapp_cos_setup()
{
    run cos create file $COS_NETAPP 				\
	--description 'Virtual-Pool-for-NETAPP' false 	\
                         --protocols NFS CIFS --max_snapshots 10 --provisionType 'Thin' \
			 --neighborhoods $NH
    ROOT_TENANT=`tenant root|tail -1`
    run cos allow $COS_NETAPP file $ROOT_TENANT
}

netappc_cos_setup()
{
    run cos create file $COS_NETAPPC                                \
	--description 'Virtual-Pool-for-NETAPPC' false   \
                         --protocols NFS CIFS --max_snapshots 10 --provisionType 'Thin' \
                         --neighborhoods $NH
    ROOT_TENANT=`tenant root|tail -1`
    run cos allow $COS_NETAPPC file $ROOT_TENANT
}

vnxe_cos_setup()
{
    secho "setting up VNXe Virtual Pool"
    run cos create file $COS_VNXE 				\
	--description 'Virtual-Pool-for-VNXe-file' true \
                         --protocols NFS CIFS --max_snapshots 10    \
                         --provisionType 'Thin' \
		         --neighborhoods $NH
    ROOT_TENANT=`tenant root|tail -1`
    run cos allow $COS_VNXE file $ROOT_TENANT
    
    run cos create block $COS_VNXEBLOCK_CG                            \
	--description 'Virtual-Pool-for-VNXe-block-cg' true         \
                         --protocols iSCSI                   \
                         --numpaths 2 \
                         --max_snapshots 10 \
                         --system_type vnxe \
                         --provisionType 'Thin' \
                         --neighborhoods $NH \
                         --multiVolumeConsistency 
                         
    run cos create block $COS_VNXEBLOCK_FC 				\
	--description 'Virtual-Pool-for-VNX-block-FC' true 	\
                         --protocols FC 			\
                         --numpaths 2 \
                         --max_snapshots 10 \
	                	 --system_type vnxe \
                         --provisionType 'Thin' \
                         --neighborhoods $NH

    run cos create block $COS_VNXEBLOCK_ISCSI 			\
	--description 'Virtual-Pool-for-VNXe-block-iSCSI' true \
                         --protocols iSCSI			\
                         --numpaths 2 \
                         --max_snapshots 10 \
	                  --system_type vnxe \
                         --provisionType 'Thin' \
                         --expandable true \
                         --neighborhoods $NH
                                 
}

datadomainfile_cos_setup()
{
    run cos create file $COS_DDFILE                \
	--description 'Virtual-Pool-for-DATADOMAIN-file' true   \
             --protocols NFS CIFS --max_snapshots 10 \
             --provisionType 'Thin' \
             --long_term_retention 'true' \
             --neighborhoods $NH
    ROOT_TENANT=`tenant root|tail -1`
    run cos allow $COS_DDFILE file $ROOT_TENANT
}

vnxblock_cos_setup()
{
    secho "VNX Block Virtual Pool setup"
    if [ $QUICK -eq 0 ]; then
      run cos create block $COS_VNXBLOCK 				\
	  --description 'Virtual-Pool-for-VNX-block' true 	\
                         --protocols FC iSCSI			\
                         --numpaths 2 \
                         --max_snapshots 10 \
	                 --system_type vnxblock \
                         --provisionType 'Thin' \
                         --neighborhoods $NH
    else 
      run cos create block $COS_VNXBLOCK                            \
	  --description 'Virtual-Pool-for-VNX-block' true         \
                         --protocols FC                    \
                         --numpaths 2 \
                         --max_snapshots 10 \
                         --system_type vnxblock \
                         --provisionType 'Thin' \
                         --neighborhoods $NH
    fi
    
    if [ "$EXTRA_PARAM" = "search" ] ; then
        cos search $(echo $COS_VNXBLOCK | head -c 2) --resource_type block_vpool
        cos tag "$COS_VNXBLOCK" "block" $TAG
        cos search $SEARCH_PREFIX --tag true --resource_type block_vpool
    fi

    run cos create block $COS_VNXBLOCK_FC 				\
	--description 'Virtual-Pool-for-VNX-block-FC' true 	\
                         --protocols FC 			\
                         --numpaths 2 \
                         --max_snapshots 10 \
	                 --system_type vnxblock \
                         --provisionType 'Thin' \
                         --neighborhoods $NH

    run cos create block $COS_VNXBLOCK_ISCSI 			\
	--description 'Virtual-Pool-for-VNX-block-iSCSI' true \
                         --protocols iSCSI			\
                         --numpaths 2 \
                         --max_snapshots 10 \
	                 --system_type vnxblock \
                         --provisionType 'Thin' \
			 --neighborhoods $NH


    run cos create block $COS_VNXBLOCK_THIN 				\
	--description 'VNX-thin-storage' true      \
                             --protocols FC iSCSI	    \
                             --numpaths 2 \
                             --max_snapshots 10 \
	                 --system_type vnxblock \
                             --provisionType 'Thin' \
                             --expandable true \
                         --neighborhoods $NH

    run cos create block $COS_VNXBLOCK_THICK 				\
	--description 'VNX-thick-storage' true      \
                             --protocols FC iSCSI	    \
                             --numpaths 2 \
                             --max_snapshots 10 \
	                 --system_type vnxblock \
                             --provisionType 'Thick' \
                             --expandable true \
                         --neighborhoods $NH
                                 
}

xtremio_cos_setup()
{
    run cos create block $XTREMIO_3X_COS_FC                          \
	--description 'Virtual-Pool-for-XtremIO-block-FC' false      \
                         --protocols FC                         \
                         --numpaths 1 \
                         --max_snapshots 10 \
                         --expandable true \
                         --system_type xtremio \
                         --provisionType 'Thin' \
                         --neighborhoods $NH
    run cos allow $XTREMIO_3X_COS_FC block $TENANT

    run cos create block $XTREMIO_4X_COS_FC                          \
	--description 'Virtual-Pool-for-XtremIO-block-FC' false      \
                         --protocols FC                         \
                         --numpaths 1 \
                         --expandable true \
                         --max_snapshots 10 \
                         --system_type xtremio \
                         --provisionType 'Thin' \
                         --neighborhoods $NH   \
                         --multiVolumeConsistency
    run cos allow $XTREMIO_4X_COS_FC block $TENANT  
}

xtremio_port_setup()
{
    secho "XtremIO port setup"
    run storageport update $XTREMIO_3X_NATIVEGUID FC --addvarrays $NH
    run storageport update $XTREMIO_4X_NATIVEGUID FC --addvarrays $NH
}

xtremio_storage_setup() {
    secho "Starting XtremIO 3.x storageprovider create"
    run storageprovider create xtremio3x $XTREMIO_3X_IP 443 $XTREMIO_3X_USER $XTREMIO_3X_PASSWD xtremio

    secho "Starting XtremIO 4.x storageprovider create"
    run storageprovider create xtremio4x $XTREMIO_4X_IP 443 $XTREMIO_4X_USER $XTREMIO_4X_PASSWD xtremio

    secho 'XtremIO discover storage systems'
    storagedevice discover_all
    storagedevice list
}

xtremio_network_setup()
{
    secho "XtremIO network setup"
    networksystem show $BROCADE_NETWORK &> /dev/null && return $?
    run networksystem create $BROCADE_NETWORK brocade --smisip $BROCADE_IP --smisport 5989 --smisuser $BROCADE_USER --smispw $BROCADE_PW --smisssl true
    run transportzone assign ${SRDF_VMAXA_VSAN} $NH
    run transportzone assign ${XTREMIO_4X_VSAN} $NH
}

xtremio_varray_setup()
{
    secho 'XtremIO varray setup for $TENANT'
    run neighborhood allow $NH $TENANT
}

xtremio_ingest_host_setup() {
    PWWN1=10:00:00:00:C9:42:6D:50
    WWNN1=20:00:00:00:C9:42:6D:50
    PWWN2=10:00:00:00:C9:42:6D:51
    WWNN2=20:00:00:00:C9:42:6D:51

    run hosts create ${XTREMIO_INGEST_HOST} $TENANT Windows ${XTREMIO_INGEST_HOST} --port 8111 --username user --password 'password' --osversion 1.0 

    run initiator create ${XTREMIO_INGEST_HOST} FC ${PWWN1} --node ${WWNN1}
    run initiator create ${XTREMIO_INGEST_HOST} FC ${PWWN2} --node ${WWNN2}
    if [ "$XIO_QUICK_PARAM" = "quick" ]; then
        run transportzone add VSAN_11 ${PWWN1}
        run transportzone add VSAN_11 ${PWWN2}
    else
        run transportzone add ${XTREMIO_4X_VSAN} ${PWWN1}
        run transportzone add ${XTREMIO_4X_VSAN} ${PWWN2}
    fi

}

xtremio_ingest_test()
{
    secho "XtremIO ingest tests"
    INGEST_3X_NATIVEGUID=$XIO_3X_SIM_NATIVEGUID
    INGEST_4X_NATIVEGUID=$XIO_4X_SIM_NATIVEGUID

    if [ "$XIO_QUICK_PARAM" != "quick" ]; then
        INGEST_3X_NATIVEGUID=$XTREMIO_3X_NATIVEGUID
        INGEST_4X_NATIVEGUID=$XTREMIO_4X_NATIVEGUID
    fi

    # Labels
    export_name=$XTREMIOEXPORT_GROUP
    XIO_RANDOM=${RANDOM}
    XIO_INGEST_VOL1_CG=${XIO_INGEST_VOL1_CGBASE}${XIO_RANDOM}
    XIO_INGEST_VOL1=${XIO_INGEST_VOL1BASE}1${XIO_RANDOM}
    XIO_INGEST_VOL2=${XIO_INGEST_VOL1BASE}2${XIO_RANDOM}
    XIO_INGEST_SNAP1=xioingestsnap1${XIO_RANDOM}
    XIO_INGEST_SNAP2=xioingestsnap2${XIO_RANDOM}

    # Create the consistency group + volume
    run blockconsistencygroup create ${PROJECT} ${XIO_INGEST_VOL1_CG}
    run volume create ${XIO_INGEST_VOL1} ${PROJECT} ${NH} ${XTREMIO_4X_COS_FC} 1GB --consistencyGroup ${XIO_INGEST_VOL1_CG}

    run volume create ${XIO_INGEST_VOL2} ${PROJECT} ${NH} ${XTREMIO_3X_COS_FC} 1GB


    # Create XIO snapshots
    secho "Create snapshots"
    run_noundo blocksnapshot create ${PROJECT}/${XIO_INGEST_VOL1} ${XIO_INGEST_SNAP1}
    XIO_INGEST_SNAP1_ARRAY_LABEL=`/opt/storageos/bin/dbutils list BlockSnapshot | grep deviceLabel | awk '{print $3}'`
    run_noundo blocksnapshot delete ${PROJECT}/${XIO_INGEST_VOL1}/${XIO_INGEST_SNAP1} --vipronly

    run_noundo blocksnapshot create ${PROJECT}/${XIO_INGEST_VOL2} ${XIO_INGEST_SNAP2}
    XIO_INGEST_SNAP2_ARRAY_LABEL=`/opt/storageos/bin/dbutils list BlockSnapshot | grep deviceLabel | awk '{print $3}'`
    run_noundo blocksnapshot delete ${PROJECT}/${XIO_INGEST_VOL2}/${XIO_INGEST_SNAP2} --vipronly

    # Delete the volume and CG
    run volume delete ${PROJECT}/${XIO_INGEST_VOL1} --vipronly
    run blockconsistencygroup delete ${XIO_INGEST_VOL1_CG} --vipronly

    run volume delete ${PROJECT}/${XIO_INGEST_VOL2} --vipronly

    secho "XIO ingest discovery"

    # Discover unmanaged volumes on the array
    run storagedevice discover_namespace $INGEST_3X_NATIVEGUID 'UNMANAGED_VOLUMES'
    run storagedevice discover_namespace $INGEST_4X_NATIVEGUID 'UNMANAGED_VOLUMES'

    ##Make sure we fail when we ingest the volumes because snaps are not ingested yet
    fail unmanagedvolume ingest_unexport ${NH} ${XTREMIO_4X_COS_FC} $PROJECT --volspec "${XIO_INGEST_VOL1}"
    fail unmanagedvolume ingest_unexport ${NH} ${XTREMIO_3X_COS_FC} $PROJECT --volspec "${XIO_INGEST_VOL2}"

    run unmanagedvolume ingest_unexport ${NH} ${XTREMIO_4X_COS_FC} $PROJECT --volspec "${XIO_INGEST_SNAP1_ARRAY_LABEL}"
    run unmanagedvolume ingest_unexport ${NH} ${XTREMIO_3X_COS_FC} $PROJECT --volspec "${XIO_INGEST_SNAP2_ARRAY_LABEL}"

    # Verify the volumes, CG and snaps got created; fail script if this fails.
    run volume show ${PROJECT}/${XIO_INGEST_VOL1}
    run blockconsistencygroup show "${XIO_INGEST_VOL1_CG}"
    run blocksnapshot show ${PROJECT}/${XIO_INGEST_VOL1}/${XIO_INGEST_SNAP1_ARRAY_LABEL}

    run volume show ${PROJECT}/${XIO_INGEST_VOL2}
    run blocksnapshot show ${PROJECT}/${XIO_INGEST_VOL2}/${XIO_INGEST_SNAP2_ARRAY_LABEL}

    ## Run block tests for the volumes and snaps
    xtremio_export_test ${export_name}1 ${XIO_INGEST_VOL1} ${XIO_INGEST_VOL2} ${XIO_INGEST_SNAP1_ARRAY_LABEL} ${XIO_INGEST_SNAP2_ARRAY_LABEL}
    if [ "$XIO_QUICK_PARAM" != "quick" ]; then
        blockconsistencygroup_restore_snapshot $XIO_INGEST_VOL1_CG $XIO_INGEST_SNAP1_ARRAY_LABEL
    fi

    # Delete the volumes, snaps and CG again
    run_noundo blocksnapshot delete ${PROJECT}/${XIO_INGEST_VOL1}/${XIO_INGEST_SNAP1_ARRAY_LABEL} --vipronly
    run_noundo blocksnapshot delete ${PROJECT}/${XIO_INGEST_VOL2}/${XIO_INGEST_SNAP2_ARRAY_LABEL} --vipronly

    run_noundo volume delete ${PROJECT}/${XIO_INGEST_VOL1} --vipronly
    run_noundo blockconsistencygroup delete ${XIO_INGEST_VOL1_CG} --vipronly

    run_noundo volume delete ${PROJECT}/${XIO_INGEST_VOL2} --vipronly

    # Discover unmanaged volumes on the array again
    run storagedevice discover_namespace $INGEST_3X_NATIVEGUID 'UNMANAGED_VOLUMES'
    run storagedevice discover_namespace $INGEST_4X_NATIVEGUID 'UNMANAGED_VOLUMES'
    
    ##Re-ingest
    ##Make sure we fail when we ingest the snaps because volumes are not ingested yet

    fail unmanagedvolume ingest_unexport ${NH} ${XTREMIO_4X_COS_FC} $PROJECT --volspec "${XIO_INGEST_SNAP1_ARRAY_LABEL}"
    fail unmanagedvolume ingest_unexport ${NH} ${XTREMIO_3X_COS_FC} $PROJECT --volspec "${XIO_INGEST_SNAP2_ARRAY_LABEL}"

    run unmanagedvolume ingest_unexport ${NH} ${XTREMIO_4X_COS_FC} $PROJECT --volspec "${XIO_INGEST_VOL1}"
    run unmanagedvolume ingest_unexport ${NH} ${XTREMIO_3X_COS_FC} $PROJECT --volspec "${XIO_INGEST_VOL2}"

    # Verify the volumes, CG and snaps got created; fail script if this fails.
    run volume show ${PROJECT}/${XIO_INGEST_VOL1}
    run blockconsistencygroup show "${XIO_INGEST_VOL1_CG}"
    run blocksnapshot show ${PROJECT}/${XIO_INGEST_VOL1}/${XIO_INGEST_SNAP1_ARRAY_LABEL}

    run volume show ${PROJECT}/${XIO_INGEST_VOL2}
    run blocksnapshot show ${PROJECT}/${XIO_INGEST_VOL2}/${XIO_INGEST_SNAP2_ARRAY_LABEL}

    ## Run block tests for the volumes and snaps
    xtremio_export_test ${export_name}1 ${XIO_INGEST_VOL1} ${XIO_INGEST_VOL2} ${XIO_INGEST_SNAP1_ARRAY_LABEL} ${XIO_INGEST_SNAP2_ARRAY_LABEL}
    if [ "$XIO_QUICK_PARAM" != "quick" ]; then
        blockconsistencygroup_restore_snapshot $XIO_INGEST_VOL1_CG $XIO_INGEST_SNAP1_ARRAY_LABEL
    fi

    # Real removal of volume/CG
    run_noundo blocksnapshot delete ${PROJECT}/${XIO_INGEST_VOL1}/${XIO_INGEST_SNAP1_ARRAY_LABEL}
    run_noundo volume delete ${PROJECT}/"${XIO_INGEST_VOL1}" --wait
    run_noundo blockconsistencygroup delete ${XIO_INGEST_VOL1_CG}

    run_noundo blocksnapshot delete ${PROJECT}/${XIO_INGEST_VOL2}/${XIO_INGEST_SNAP2_ARRAY_LABEL}
    run_noundo volume delete ${PROJECT}/"${XIO_INGEST_VOL2}" --wait

}

xtremio_ingest_export_test()
{
    secho "XtremIO ingest export tests"

    INGEST_3X_NATIVEGUID=$XIO_3X_SIM_NATIVEGUID
    INGEST_4X_NATIVEGUID=$XIO_4X_SIM_NATIVEGUID

    if [ "$XIO_QUICK_PARAM" != "quick" ]; then
        INGEST_3X_NATIVEGUID=$XTREMIO_3X_NATIVEGUID
        INGEST_4X_NATIVEGUID=$XTREMIO_4X_NATIVEGUID
    fi

    # Labels
    XIO_INGEST_VOL1_CG=${XIO_INGEST_VOL1_CGBASE}${XIO_RANDOM}exp
    XIO_INGEST_VOL1=${XIO_INGEST_VOL1BASE}1${XIO_RANDOM}exp
    XIO_INGEST_VOL2=${XIO_INGEST_VOL1BASE}2${XIO_RANDOM}exp
    XIO_INGEST_SNAP1=xioingestsnap1${XIO_RANDOM}exp
    XIO_INGEST_SNAP2=xioingestsnap2${XIO_RANDOM}exp

    xtremio_ingest_host_setup

    # Create the consistency group + volume
    run blockconsistencygroup create ${PROJECT} ${XIO_INGEST_VOL1_CG}
    run volume create ${XIO_INGEST_VOL1} ${PROJECT} ${NH} ${XTREMIO_4X_COS_FC} 1GB --consistencyGroup ${XIO_INGEST_VOL1_CG}

    run volume create ${XIO_INGEST_VOL2} ${PROJECT} ${NH} ${XTREMIO_3X_COS_FC} 1GB

    # Export the volume to a host

    run export_group create ${PROJECT} EG-xio-ingest ${NH} --type Host --volspec "${PROJECT}/${XIO_INGEST_VOL1},${PROJECT}/${XIO_INGEST_VOL2}" --hosts "${XTREMIO_INGEST_HOST}"

    # Create XIO snapshots
    secho "Create snapshots"
    run blocksnapshot create ${PROJECT}/${XIO_INGEST_VOL1} ${XIO_INGEST_SNAP1}
    # Capture the deviceLabel of the snapshot which will be the label after unmanaged volume discovery
    XIO_INGEST_SNAP1_ARRAY_LABEL=`/opt/storageos/bin/dbutils list BlockSnapshot | grep deviceLabel | awk '{print $3}'`
    run export_group update ${PROJECT}/EG-xio-ingest --addVolspec "${PROJECT}/${XIO_INGEST_VOL1}/${XIO_INGEST_SNAP1}"
    run blocksnapshot delete ${PROJECT}/${XIO_INGEST_VOL1}/${XIO_INGEST_SNAP1} --vipronly

    run blocksnapshot create ${PROJECT}/${XIO_INGEST_VOL2} ${XIO_INGEST_SNAP2}
    XIO_INGEST_SNAP2_ARRAY_LABEL=`/opt/storageos/bin/dbutils list BlockSnapshot | grep deviceLabel | awk '{print $3}'`
    run export_group update ${PROJECT}/EG-xio-ingest --addVolspec "${PROJECT}/${XIO_INGEST_VOL2}/${XIO_INGEST_SNAP2}"
    run blocksnapshot delete ${PROJECT}/${XIO_INGEST_VOL2}/${XIO_INGEST_SNAP2} --vipronly

    # Delete the volume and CG
    run volume delete ${PROJECT}/${XIO_INGEST_VOL1} --vipronly
    run volume delete ${PROJECT}/${XIO_INGEST_VOL2} --vipronly

    run export_group delete ${PROJECT}/EG-xio-ingest

    run blockconsistencygroup delete ${XIO_INGEST_VOL1_CG} --vipronly


    secho "XIO ingest discovery"

    # Discover unmanaged volumes on the array
    run storagedevice discover_namespace $INGEST_3X_NATIVEGUID 'UNMANAGED_VOLUMES'
    run storagedevice discover_namespace $INGEST_4X_NATIVEGUID 'UNMANAGED_VOLUMES'

    ##Make sure we fail when we ingest the volumes because snaps are not ingested yet
    fail unmanagedvolume ingest_export ${NH} ${XTREMIO_4X_COS_FC} $PROJECT --volspec "${XIO_INGEST_VOL1}" --host ${XTREMIO_INGEST_HOST}
    fail unmanagedvolume ingest_export ${NH} ${XTREMIO_3X_COS_FC} $PROJECT --volspec "${XIO_INGEST_VOL2}" --host ${XTREMIO_INGEST_HOST}


    run unmanagedvolume ingest_export ${NH} ${XTREMIO_4X_COS_FC} $PROJECT --volspec "${XIO_INGEST_SNAP1_ARRAY_LABEL}" --host ${XTREMIO_INGEST_HOST}

    run unmanagedvolume ingest_export ${NH} ${XTREMIO_3X_COS_FC} $PROJECT --volspec "${XIO_INGEST_SNAP2_ARRAY_LABEL}" --host ${XTREMIO_INGEST_HOST}

    # Verify the volumes, EG, CG and snaps got created; fail script if this fails.
    run volume show ${PROJECT}/${XIO_INGEST_VOL1}
    run blockconsistencygroup show "${XIO_INGEST_VOL1_CG}"
    run blocksnapshot show ${PROJECT}/${XIO_INGEST_VOL1}/${XIO_INGEST_SNAP1_ARRAY_LABEL}

    run volume show ${PROJECT}/${XIO_INGEST_VOL2}
    run blocksnapshot show ${PROJECT}/${XIO_INGEST_VOL2}/${XIO_INGEST_SNAP2_ARRAY_LABEL}
    run export_group show  ${PROJECT}/${XTREMIO_INGEST_HOST}

    ## Run block tests for the volumes and snaps
    xtremio_export_test ${export_name}1 ${XIO_INGEST_VOL1} ${XIO_INGEST_VOL2} ${XIO_INGEST_SNAP1_ARRAY_LABEL} ${XIO_INGEST_SNAP2_ARRAY_LABEL}
    if [ "$XIO_QUICK_PARAM" != "quick" ]; then
        blockconsistencygroup_restore_snapshot $XIO_INGEST_VOL1_CG $XIO_INGEST_SNAP1_ARRAY_LABEL
    fi

    # Delete the volumes, snaps and CG again
    run_noundo blocksnapshot delete ${PROJECT}/${XIO_INGEST_VOL1}/${XIO_INGEST_SNAP1_ARRAY_LABEL} --vipronly
    run_noundo blocksnapshot delete ${PROJECT}/${XIO_INGEST_VOL2}/${XIO_INGEST_SNAP2_ARRAY_LABEL} --vipronly

    run_noundo volume delete ${PROJECT}/${XIO_INGEST_VOL1} --vipronly
    run_noundo blockconsistencygroup delete ${XIO_INGEST_VOL1_CG} --vipronly

    run_noundo volume delete ${PROJECT}/${XIO_INGEST_VOL2} --vipronly

    run_noundo export_group delete ${PROJECT}/${XTREMIO_INGEST_HOST}

    # Discover unmanaged volumes on the array again
    run storagedevice discover_namespace $INGEST_3X_NATIVEGUID 'UNMANAGED_VOLUMES'
    run storagedevice discover_namespace $INGEST_4X_NATIVEGUID 'UNMANAGED_VOLUMES'
    
    ##Re-ingest
    ##Make sure we fail when we ingest the snaps because volumes are not ingested yet

    fail unmanagedvolume ingest_export ${NH} ${XTREMIO_4X_COS_FC} $PROJECT --volspec "${XIO_INGEST_SNAP1_ARRAY_LABEL}" --host ${XTREMIO_INGEST_HOST}

    fail unmanagedvolume ingest_export ${NH} ${XTREMIO_3X_COS_FC} $PROJECT --volspec "${XIO_INGEST_SNAP2_ARRAY_LABEL}" --host ${XTREMIO_INGEST_HOST}

    run unmanagedvolume ingest_export ${NH} ${XTREMIO_4X_COS_FC} $PROJECT --volspec "${XIO_INGEST_VOL1}" --host ${XTREMIO_INGEST_HOST}

    run unmanagedvolume ingest_export ${NH} ${XTREMIO_3X_COS_FC} $PROJECT --volspec "${XIO_INGEST_VOL2}" --host ${XTREMIO_INGEST_HOST}


    # Verify the volumes, CG and snaps got created; fail script if this fails.
    run volume show ${PROJECT}/${XIO_INGEST_VOL1}
    run blockconsistencygroup show "${XIO_INGEST_VOL1_CG}"
    run blocksnapshot show ${PROJECT}/${XIO_INGEST_VOL1}/${XIO_INGEST_SNAP1_ARRAY_LABEL}

    run volume show ${PROJECT}/${XIO_INGEST_VOL2}
    run blocksnapshot show ${PROJECT}/${XIO_INGEST_VOL2}/${XIO_INGEST_SNAP2_ARRAY_LABEL}
    run export_group show  ${PROJECT}/${XTREMIO_INGEST_HOST}

    ## Run block tests for the volumes and snaps
    xtremio_export_test ${export_name}1 ${XIO_INGEST_VOL1} ${XIO_INGEST_VOL2} ${XIO_INGEST_SNAP1_ARRAY_LABEL} ${XIO_INGEST_SNAP2_ARRAY_LABEL}
    
    if [ "$XIO_QUICK_PARAM" != "quick" ]; then
        blockconsistencygroup_restore_snapshot $XIO_INGEST_VOL1_CG $XIO_INGEST_SNAP1_ARRAY_LABEL
    fi

    # Real removal of volume/CG
    run_noundo export_group update ${PROJECT}/${XTREMIO_INGEST_HOST} --remVols "${PROJECT}/${XIO_INGEST_VOL1}/${XIO_INGEST_SNAP1_ARRAY_LABEL},${PROJECT}/${XIO_INGEST_VOL2}/${XIO_INGEST_SNAP2_ARRAY_LABEL}"
    run_noundo export_group update ${PROJECT}/${XTREMIO_INGEST_HOST} --remVols "${PROJECT}/${XIO_INGEST_VOL1},${PROJECT}/${XIO_INGEST_VOL2}"
    run_noundo blocksnapshot delete ${PROJECT}/${XIO_INGEST_VOL1}/${XIO_INGEST_SNAP1_ARRAY_LABEL}
    run_noundo volume delete ${PROJECT}/"${XIO_INGEST_VOL1}" --wait

    run_noundo blocksnapshot delete ${PROJECT}/${XIO_INGEST_VOL2}/${XIO_INGEST_SNAP2_ARRAY_LABEL}
    run_noundo volume delete ${PROJECT}/"${XIO_INGEST_VOL2}" --wait
    run_noundo export_group delete ${PROJECT}/${XTREMIO_INGEST_HOST}
}


xtremio_simulator_setup()
{
    project_setup
    xtremio_varray_setup   
    
    run syssvc $CONFIG_FILE $BOURNE_IPADDR set_prop controller_discovery_refresh_interval 5
    #run syssvc $CONFIG_FILE $BOURNE_IPADDR set_prop artificial_failure "none"

    run storageprovider create xtremio3x $XIO_SIMULATOR_IP $XIO_3X_SIMULATOR_PORT root root xtremio
    run storageprovider create xtremio4x $XIO_SIMULATOR_IP $XIO_4X_SIMULATOR_PORT root root xtremio
    run networksystem create $FABRIC_SIMULATOR  mds --devip $SIMULATOR_CISCO_MDS --devport 22 --username $RP_FABRIC_SIM_USER --password $RP_FABRIC_SIM_PW
    
    storagedevice discover_all
    storagedevice list
    run transportzone assign VSAN_11 ${NH}

    # Make sure all of the pools are updated and matched
    run storagedevice discover_all

    run storagepool update $XIO_3X_SIM_NATIVEGUID --nhadd $NH --type block
    run storagepool update $XIO_4X_SIM_NATIVEGUID --nhadd $NH --type block
    
    run storageport update $XIO_3X_SIM_NATIVEGUID FC --addvarrays $NH
    run storageport update $XIO_4X_SIM_NATIVEGUID FC --addvarrays $NH

    #Add the host ports to the network which contians the storage ports
    for i in A1 A2 A3 A4 A5 A6 A7 A8 C1 C2 C3 C4 C5 C6 C7 C8
    do
        wwn=`pwwn $i`
    
        secho "Adding $wwn to zone VSAN_11..."
	run transportzone add VSAN_11 $wwn
    done


    for i in B1 B2 B3 B4 B5 B6 B7 B8 D1 D2 D3 D4 D5 D6 D7 D8
    do
        wwn=`pwwn $i`
        secho "Adding $wwn to zone VSAN_11..."
        run transportzone add VSAN_11 $wwn
    done
    xtremio_cos_setup
    run cos update block $XTREMIO_3X_COS_FC --storage $XIO_4X_SIM_NATIVEGUID
    run cos update block $XTREMIO_4X_COS_FC --storage $XIO_4X_SIM_NATIVEGUID
}

xtremio_common_setup()
{
    project_setup
    xtremio_varray_setup
    xtremio_network_setup
    xtremio_storage_setup
    xtremio_port_setup
    
    
    #Add the host ports to the network which contians the storage ports
    for i in A1 A2 A3 A4 A5 A6 A7 A8 C1 C2 C3 C4 C5 C6 C7 C8
    do
        wwn=`pwwn $i`
    
        secho "Adding $wwn to zone $XTREMIO_4X_VSAN..."
	run transportzone add $XTREMIO_4X_VSAN $wwn
    done


    for i in B1 B2 B3 B4 B5 B6 B7 B8 D1 D2 D3 D4 D5 D6 D7 D8
    do
        wwn=`pwwn $i`
        secho "Adding $wwn to zone $XTREMIO_4X_VSAN..."
        run transportzone add $XTREMIO_4X_VSAN $wwn
    done

    xtremio_cos_setup
    run cos update block $XTREMIO_3X_COS_FC --storage $XTREMIO_4X_NATIVEGUID
    run cos update block $XTREMIO_4X_COS_FC --storage $XTREMIO_4X_NATIVEGUID
}

xtremio_setup()
{
    if [ "$XIO_QUICK_PARAM" = "quick" ]; then
        secho "XtremIO Simulator setup"
        xtremio_simulator_setup
    else
        secho "XtremIO setup"
        xtremio_common_setup
    fi
}

xtremio_tests()
{
   echo 'xtremio tests invoked'

   # HLU specific tests
   if [ "$XIO_HLU_PARAM" = "1" ]; then
       secho "Creating tools.yml"

       storage_sn=$XTREMIO_4X_SN
       storage_password=$XTREMIO_4X_PASSWD
       if [ "$XIO_QUICK_PARAM" = "quick" ]; then
           storage_sn=$XIO_4X_SIM_SN
       fi

       setup_yaml xtremio $storage_sn $storage_password

       xtremio_export_test_hlu
       return
   fi

   if [ "$XIO_BLOCK_TESTS" = "1" ]; then
       xtremio_block_tests
       xtremio_blockconsistency_tests
   fi

   # Ingest specific tests
   if [ "$XIO_INGESTTESTS" = "1" ]; then
        secho "XIO ingest volumes"
        xtremio_ingest_test

        secho "XIO ingest exported volumes"
        xtremio_ingest_export_test
        secho "XIO ingest tests complete"
   fi
}

xtremio_blockconsistency_tests()
{
    # Create Consistency Group
    blockconsistencygroup_create_test
    blockconsistencygroup_show_test
    xtremio_blockconsistencygroup_add_volume_test $CONSISTENCY_GROUP
    xtremio_blockconsistencygroup_snapshot_tests xio-$CONSISTENCY_GROUP $XTREMIO_4X_COS_FC xio-$CONSISTENCY_GROUP_SNAPSHOT
    blockconsistencygroup_bulk_test
    ### Delete volume from consistency group
    echo "Deleting volume from consistency group"
    run volume delete $PROJECT/volume-${CONSISTENCY_GROUP}
}

xtremio_blockconsistencygroup_add_volume_test()
{
    cg_name=$1

    ### Create Volume
    echo "Adding volume to consistency group"
    run volume create  volume-${cg_name} $PROJECT $NH $XTREMIO_4X_COS_FC 1280000000 --consistencyGroup $cg_name

    ### Check that volume is inside the group
    echo "Checking volume is part of consistency group"
    run blockconsistencygroup check_volume  $PROJECT volume-${cg_name} $cg_name --expected

    ### Check that consistencygroup cannot be deleted at this point
    echo "Checking consistency group cannot be deleted with active volumes"
    run blockconsistencygroup delete_with_volumes $cg_name
}

xtremio_blockconsistencygroup_snapshot_tests()
{    
    cg_name=$1
    cg_cos=$2
    cg_snapshot=$3
    
    echo "Running Consistency Group Snapshot tests"
    echo "Consistency Group: $cg_name"
    echo "CoS: $cg_cos"
    echo "CG Snapshot: $cg_snapshot" 
    
    blockconsistencygroup_setup_snapshot $cg_name $cg_cos
    blockconsistencygroup_create_snapshot $cg_name $cg_snapshot
    volume expand $PROJECT/volume-${cg_name} $BLK_SIZE_EXPAND
    blockconsistencygroup_expand_snapshot $PROJECT/volume-${cg_name}/$cg_snapshot $BLK_SIZE_EXPAND
    
    if [ "$XIO_QUICK_PARAM" != "quick" ]; then
        blockconsistencygroup_restore_snapshot $cg_name $cg_snapshot
    fi
    blockconsistencygroup_show_snapshot $cg_name $cg_snapshot
    blockconsistencygroup_list_snapshot $cg_name $cg_snapshot
    blockconsistencygroup_deactivate_snapshot $cg_name $cg_snapshot

    echo "Deleting volume in snapshot consistency group"
    run volume delete $PROJECT/volume-${cg_name}   
}

VERIFY_EXPORT_COUNT=0
VERIFY_EXPORT_FAIL_COUNT=0
verify_export() {
    # Parameters: Initiator group Name, Number of Initiators, Number of Luns, HLU
    # If checking if the Initiator group does not exist, then parameter $2 should be "gone"
    IG_name=$1
    masking_view_name=${IG_name}.lss.emc.com
    run ${BASEDIR}/\export-tests/xiohelper.sh verify_export $masking_view_name $2 $3 $4
    if [ $? -ne "0" ]; then
       echo There was a failure
       VERIFY_EXPORT_FAIL_COUNT=`expr $VERIFY_EXPORT_FAIL_COUNT + 1`
    fi
    VERIFY_EXPORT_COUNT=`expr $VERIFY_EXPORT_COUNT + 1`
}

xtremio_block_tests()
{
    export_name=$XTREMIOEXPORT_GROUP
    export_host=$XTREMIOEXPORT_GROUP_HOST
    cos1=$XTREMIO_3X_COS_FC
    volume=${XTREMIOVOL}
    cos2=$XTREMIO_4X_COS_FC
    snap1_label=snap1-${HOSTNAME}-${RANDOM}
    snap1_label=${snap1_label//.}
    snap2_label=snap2-${HOSTNAME}-${RANDOM}
    snap2_label=${snap2_label//.}
            
    run volume create ${volume}1 $PROJECT $NH $cos2 $BLK_SIZE --thinVolume true
    run volume create ${volume}2 $PROJECT $NH $cos2 $BLK_SIZE --thinVolume true
    run blocksnapshot create $PROJECT/${volume}1 ${snap1_label}
    run blocksnapshot create $PROJECT/${volume}2 ${snap2_label}
    
    run volume expand $PROJECT/${volume}2 $BLK_SIZE_EXPAND
    
    run blocksnapshot expand $PROJECT/${volume}2/${snap2_label} $BLK_SIZE_EXPAND
    
    xtremio_export_test ${export_name}1 ${volume}1 ${volume}2 ${snap1_label} ${snap2_label}

    run blocksnapshot restore $snap2
    run volume bulkget

    run blocksnapshot delete $PROJECT/${volume}1/${snap1_label}
    run blocksnapshot delete $PROJECT/${volume}2/${snap2_label}
    run volume delete $PROJECT/${volume}1 --wait
    run volume delete $PROJECT/${volume}2 --wait
}

xtremio_export_test()
{
    vol1=$PROJECT/$2
    vol2=$PROJECT/$3
    snap1=${vol1}/$4
    snap2=${vol2}/$5
    proj=$PROJECT
    tenant=$TENANT
    c=1
    h=1
    expname=$1
    hostname=$hostbase$tenant$c$h
    echo "Export details: $vol1 $vol2 $proj $tenant $hostname $expname $snap1 $snap2"

    exp=$proj/$expname
    k=`wwnIdx $c $h`
    pwwn1=`pwwn A$k`
    pwwn2=`pwwn B$k`
    pwwn3=`pwwn C$k`
    pwwn4=`pwwn D$k`

    if [ "$BOURNE_SECURITY_DISABLED" != '1' ] ; then
        run bulkapi blocksnapshots $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
    fi

    run export_group create $proj $expname $NH --volspec "$vol1,$snap1,$snap2" --inits "$hostname/$pwwn1"
    run export_group show $exp
    run export_group update $exp --addVolspec "$vol2" --remVols $vol1
    run export_group show $exp
    run export_group update $exp --remInits "$hostname/$pwwn1"
    run export_group show $exp
    run export_group update $exp --remVols $vol2,$snap1,$snap2
    run export_group show $exp

    if [ "$BOURNE_SECURITY_DISABLED" != '1' ] ; then
        run bulkapi exportgroups $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
    fi

    run export_group delete $exp
}

xtremio_export_test_hlu() {
    echo "XtremIO export consistent HLU tests START"
    cos1=$XTREMIO_3X_COS_FC
    volume=${XTREMIOVOL}
    cos2=$XTREMIO_4X_COS_FC
    snap1_label=snap1-${HOSTNAME}-${RANDOM}
    snap2_label=snap2-${HOSTNAME}-${RANDOM}
    snap3_label=snap3-${HOSTNAME}-${RANDOM}

    run volume create ${volume}1 $PROJECT $NH $cos2 $BLK_SIZE --thinVolume true
    run volume create ${volume}2 $PROJECT $NH $cos2 $BLK_SIZE --thinVolume true
    run volume create ${volume}3 $PROJECT $NH $cos2 $BLK_SIZE --thinVolume true
    run volume create ${volume}4 $PROJECT $NH $cos2 $BLK_SIZE --thinVolume true
    run volume create ${volume}5 $PROJECT $NH $cos2 $BLK_SIZE --thinVolume true
    run blocksnapshot create $PROJECT/${volume}1 ${snap1_label}
    run blocksnapshot create $PROJECT/${volume}2 ${snap2_label}
    run blocksnapshot create $PROJECT/${volume}3 ${snap3_label}

    xtremio_export_test_hlu1
    xtremio_export_test_hlu2
    xtremio_export_test_hlu_snapshot_1 ${snap1_label} ${snap2_label} ${snap3_label}
    xtremio_export_test_hlu_snapshot_2 ${snap1_label} ${snap2_label} ${snap3_label}

    run blocksnapshot delete $PROJECT/${volume}1/${snap1_label}
    run blocksnapshot delete $PROJECT/${volume}2/${snap2_label}
    run blocksnapshot delete $PROJECT/${volume}3/${snap3_label}
    run volume delete $PROJECT/${volume}1 --wait
    run volume delete $PROJECT/${volume}2 --wait
    run volume delete $PROJECT/${volume}3 --wait
    run volume delete $PROJECT/${volume}4 --wait
    run volume delete $PROJECT/${volume}5 --wait

    echo There were $VERIFY_EXPORT_COUNT export verifications
    echo There were $VERIFY_EXPORT_FAIL_COUNT export verification failures
    echo "XtremIO export consistent HLU tests END"
}

#Consistent Cluster HLU
# CLUSTER with 2 hosts
# Step-1: Export a volume to the cluster. Result: All hosts in the cluster sees the volume with same HLU (least unused number)
# Step-2: Export a volume to HOST1 (exclusive). Result: HLU assigned should be unused among cluster view
# Step-3: Export a volume to the cluster. Result: All hosts in the cluster sees the volume with same HLU (least unused number)
# Step-4: Export a volume to HOST2 (exclusive). Result: HLU assigned should be unused among cluster view
# Step-5: Delete the private volume from HOST1 exported in step-2
# Step-6: Remove HOST2 from cluster
# Step-7: Export a new volume to the cluster. Result: All hosts in the cluster sees the volume with same HLU.
# Step-8: Remove one shared volume from cluster exported in Step-1
# Step-9: Add HOST2 to CLUSTER. Result: All shared volumes of cluster to be exported to this new host with the HLU for those volumes same as that of cluster's view
xtremio_export_test_hlu1()
{
	HOST1=$hostbase${tenant}11
	HOST2=$hostbase${tenant}12
	expname=${XTREMIOEXPORT_GROUP}
	volname=${XTREMIOVOL}
	cluster=sanityCluster1
	echo "XTREMIO Consistent HLU Test 1"

	run export_group create $proj ${expname}1 $NH --type Cluster --volspec "${proj}/${volname}"1 --cluster ${tenant}/${cluster}
	verify_export ${HOST1} 4 1 1
	verify_export ${HOST2} 4 1 1
	
	run export_group create $proj ${expname}2 $NH --type Host --volspec "${proj}/${volname}"2 --hosts "${HOST1}"
	verify_export ${HOST1} 4 2 1,2
	verify_export ${HOST2} 4 1 1
	
	run export_group update $proj/${expname}1 --addVolspec "${proj}/${volname}"3
	verify_export ${HOST1} 4 3 1,2,3
	verify_export ${HOST2} 4 2 1,3
	
	run export_group create $proj ${expname}3 $NH --type Host --volspec "${proj}/${volname}"4 --hosts "${HOST2}"
	verify_export ${HOST1} 4 3 1,2,3
	verify_export ${HOST2} 4 3 1,2,3
	
	run export_group delete $proj/${expname}2
	verify_export ${HOST1} 4 2 1,3
    verify_export ${HOST2} 4 3 1,2,3
	
	run export_group update $proj/${expname}1 --remHosts "${HOST2}"
    verify_export ${HOST1} 4 2 1,3
    verify_export ${HOST2} 4 1 2

    run export_group update $proj/${expname}1 --remVols "${proj}/${volname}"1
    verify_export ${HOST1} 4 1 3
    verify_export ${HOST2} 4 1 2

    run export_group update $proj/${expname}1 --addVols "${proj}/${volname}"5
    verify_export ${HOST1} 4 2 1,3
    verify_export ${HOST2} 4 1 2 

	run export_group update $proj/${expname}1 --addHosts "${HOST2}"
    verify_export ${HOST1} 4 2 1,3
    verify_export ${HOST2} 4 3 1,2,3

    run export_group update $proj/${expname}1 --addVols "$proj/${volname}"1
    verify_export ${HOST1} 4 3 1,3,4
    verify_export ${HOST2} 4 4 1,2,3,4

    run export_group delete $proj/${expname}3
    verify_export ${HOST1} 4 3 1,3,4
    verify_export ${HOST2} 4 3 1,3,4

    run export_group delete $proj/${expname}1
}

#Consistent Cluster HLU
# CLUSTER with 2 hosts
# Step-1: Export a volume to HOST1 (exclusive). Result: HLU assigned should be unused among cluster view
# Step-2: Export a volume to the cluster. Result: All hosts in the cluster sees the volume with same HLU (least unused number)
# Step-3: Export a volume to the cluster. Result: All hosts in the cluster sees the volume with same HLU (least unused number)
# Step-4: Export a volume to HOST2 (exclusive). Result: HLU assigned should be unused among cluster view
# Step-5: Delete the private volume from HOST1 exported in step-2
# Step-6: Remove HOST2 from cluster
# Step-7: Export a new volume to the cluster. Result: All hosts in the cluster sees the volume with same HLU.
# Step-8: Remove one shared volume from cluster exported in Step-1
# Step-9: Add HOST2 to CLUSTER. Result: All shared volumes of cluster to be exported to this new host with the HLU for those volumes same as that of cluster's view
xtremio_export_test_hlu2()
{
	HOST1=$hostbase${tenant}21
	HOST2=$hostbase${tenant}22
	expname=${XTREMIOEXPORT_GROUP}
	volname=${XTREMIOVOL}
	cluster=sanityCluster2
	echo "XTREMIO Consistent HLU Test 2"
	
	run export_group create $proj ${expname}1 $NH --type Host --volspec "${proj}/${volname}"1 --hosts "${HOST1}"
	verify_export ${HOST1} 4 1 1
	
	run export_group create $proj ${expname}2 $NH --type Cluster --volspec "${proj}/${volname}"2 --cluster ${tenant}/${cluster}
	verify_export ${HOST1} 4 2 1,2
	verify_export ${HOST2} 4 1 2 
	
	run export_group update $proj/${expname}2 --addVols "${proj}/${volname}"3
	verify_export ${HOST1} 4 3 1,2,3
	verify_export ${HOST2} 4 2 2,3
	
	run export_group create $proj ${expname}3 $NH --type Host --volspec "${proj}/${volname}"4 --hosts "${HOST2}"
	verify_export ${HOST2} 4 3 1,2,3
	
	run export_group delete $proj/${expname}1
	verify_export ${HOST1} 4 2 2,3
	verify_export ${HOST2} 4 3 1,2,3
	
	run export_group update $proj/${expname}2 --remHosts "${HOST2}"
	verify_export ${HOST1} 4 2 2,3
	verify_export ${HOST2} 4 1 1
    
    run export_group update $proj/${expname}2 --remVols "${proj}/${volname}"2
    verify_export ${HOST1} 4 1 3

    run export_group update $proj/${expname}2 --addHosts "${HOST2}"
    verify_export ${HOST1} 4 1 3
    verify_export ${HOST2} 4 2 1,3

    run export_group update $proj/${expname}2 --addVols "$proj/${volname}"2
    verify_export ${HOST1} 4 2 2,3
    verify_export ${HOST2} 4 3 1,2,3

    run export_group delete $proj/${expname}3
    verify_export ${HOST1} 4 2 2,3
    verify_export ${HOST2} 4 2 2,3

    run export_group delete $proj/${expname}2
}

xtremio_export_test_hlu_snapshot_1() {
	volname=${XTREMIOVOL}
	snap1=$proj/${volname}1/$1
    snap2=$proj/${volname}2/$2
	snap3=$proj/${volname}3/$3
	HOST1=$hostbase${tenant}11
	HOST2=$hostbase${tenant}12
	cluster=sanityCluster1
	expname=${XTREMIOEXPORT_GROUP}Snap
	echo "XTREMIO Consistent HLU Snapshot Test 1"
	
	run export_group create $proj ${expname}1 $NH --type Cluster --volspec "${proj}/${volname}"1 --cluster ${tenant}/${cluster}
	verify_export ${HOST1} 4 1 1
	verify_export ${HOST2} 4 1 1
	
	run export_group create $proj ${expname}2 $NH --type Host --volspec "${proj}/${volname}"2 --hosts "${HOST1}"
	verify_export ${HOST1} 4 2 1,2
	verify_export ${HOST2} 4 1 1
	
	run export_group update $proj/${expname}1 --addVolspec "${snap1}"
	verify_export ${HOST1} 4 3 1,2,3
	verify_export ${HOST2} 4 2 1,3
	
	run export_group update $proj/${expname}2 --addVolspec "${snap2}"
	verify_export ${HOST1} 4 4 1,2,3,4
	verify_export ${HOST2} 4 2 1,3
	
	run export_group delete $proj/${expname}2
	verify_export ${HOST1} 4 2 1,3
    verify_export ${HOST2} 4 2 1,3
	
	run export_group update $proj/${expname}1 --remHosts "${HOST2}"
    verify_export ${HOST1} 4 2 1,3
    verify_export ${HOST2} gone

    run export_group update $proj/${expname}1 --remVols "${snap1}"
    verify_export ${HOST1} 4 1 1

    run export_group update $proj/${expname}1 --addVols "$proj/${volname}3"
    verify_export ${HOST1} 4 2 1,2
    
	run export_group update $proj/${expname}1 --addHosts "${HOST2}"
    verify_export ${HOST1} 4 2 1,2
	verify_export ${HOST2} 4 2 1,2
    
    run export_group update $proj/${expname}1 --addVols "${snap1}"
    verify_export ${HOST1} 4 3 1,2,3
    verify_export ${HOST2} 4 3 1,2,3

    run export_group delete $proj/${expname}1

}

xtremio_export_test_hlu_snapshot_2()
{
	volname=${XTREMIOVOL}
	cluster=sanityCluster2
	snap1=$proj/${volname}1/$1
    snap2=$proj/${volname}2/$2
	snap3=$proj/${volname}3/$3
	HOST1=$hostbase${tenant}21
	HOST2=$hostbase${tenant}22
	expname=${XTREMIOEXPORT_GROUP}
    echo "XTREMIO Consistent HLU Snapshot Test 2"
	
	run export_group create $proj ${expname}1 $NH --type Host --volspec "${proj}/${volname}"1 --hosts "${HOST1}"
	verify_export ${HOST1} 4 1 1
	
	run export_group create $proj ${expname}2 $NH --type Cluster --volspec "${proj}/${volname}"2 --cluster ${tenant}/${cluster}
	verify_export ${HOST1} 4 2 1,2
	verify_export ${HOST2} 4 1 2
	
	run export_group update $proj/${expname}1 --addVols "${snap1}"
	verify_export ${HOST1} 4 3 1,2,3
	verify_export ${HOST2} 4 1 2
	
	run export_group update $proj/${expname}2 --addVols "${snap2}"
	verify_export ${HOST1} 4 4 1,2,3,4
	verify_export ${HOST2} 4 2 2,4
	
	run export_group delete $proj/${expname}1
    verify_export ${HOST1} 4 2 2,4
    verify_export ${HOST2} 4 2 2,4
	
	run export_group update $proj/${expname}2 --remHosts "${HOST2}"
	verify_export ${HOST1} 4 2 2,4
	verify_export ${HOST2} gone
    
    run export_group update $proj/${expname}2 --addVols "${proj}/${volname}"3
    verify_export ${HOST1} 4 3 1,2,4
    verify_export ${HOST2} gone

    run export_group update $proj/${expname}2 --remVols "${proj}/${volname}"2
    verify_export ${HOST1} 4 2 1,4

	run export_group update $proj/${expname}2 --addHosts "${HOST2}"
    verify_export ${HOST1} 4 2 1,4
    verify_export ${HOST2} 4 2 1,4

    run export_group update $proj/${expname}2 --addVols "${snap3}"
	verify_export ${HOST1} 4 3 1,2,4
	verify_export ${HOST2} 4 3 1,2,4

    run export_group delete $proj/${expname}2
}
#
# SRDF
#
srdf_setup()
{
    echo 'SRDF setup'
    srdf_setup_once
    srdf_common_setup
}

srdf_setup_once()
{
    if [ "$SRDF_QUICK_PARAM" = "quick" ]; then
        secho "Test with simulator"

        SRDF_SLEEPING_SECONDS=1
        SRDF_PROJECT=SRDF$(shuf -i 1-29 -n 1)
        V3_SRDF_VARRAY=nh

	SRDF_V3_VMAXA_SMIS_IP=${VMAX3_SIMULATOR_SMIS_IP}
	SRDF_V3_VMAXA_SMIS_PORT=${VMAX3_SIMULATOR_SMIS_PORT}
	SRDF_V3_VMAXA_SMIS_SSL=${VMAX3_SIMULATOR_SMIS_SSL}
	SRDF_V3_VMAXA_NATIVEGUID=${VMAX3_SIMULATOR_NATIVE_GUID}
	SRDF_V3_VMAXB_NATIVEGUID=${VMAX3_SIMULATOR_R2_NATIVE_GUID}

    else
        srdf_network_setup

        secho "Executing SRDF group setup"
        SRDF_PROJECT=$(srdf_generate_rdfg_name)
        # create RDF group first, SRDF_PROJECT may be changed in the call
        srdf_create_rdfg
        srdf_sleep
        # COP-25856 RDF group is not available 3 minutes after created via symcli, add more time
        ssleep 300
    fi
	
	project_setup
	zone_setup

    project show $SRDF_PROJECT &> /dev/null && return $?
    run project create $SRDF_PROJECT --tenant $TENANT

    run blockconsistencygroup create $SRDF_PROJECT $CONSISTENCY_GROUP_SRDF
    run blockconsistencygroup create $SRDF_PROJECT ${CONSISTENCY_GROUP_SRDF}v3
	
	smisprovider show $SRDF_V3_VMAXA_SMIS_DEV &> /dev/null && return $?
    run smisprovider create $SRDF_V3_VMAXA_SMIS_DEV $SRDF_V3_VMAXA_SMIS_IP $SRDF_V3_VMAXA_SMIS_PORT $SMIS_USER "$SMIS_PASSWD" $SRDF_V3_VMAXA_SMIS_SSL

    if [ "$SRDF_QUICK_PARAM" != "quick" ]; then
        smisprovider show $SRDF_V3_VMAXB_SMIS_DEV &> /dev/null && return $?
        run smisprovider create $SRDF_V3_VMAXB_SMIS_DEV $SRDF_V3_VMAXB_SMIS_IP $SRDF_V3_VMAXB_SMIS_PORT $SMIS_USER "$SMIS_PASSWD" $SRDF_V3_VMAXB_SMIS_SSL
    fi

    
    # Adding additional array in order to test pool matching.
    # smisprovider show $VMAX_SMIS_DEV &> /dev/null && return $?
    # smisprovider create $VMAX_SMIS_DEV $VMAX_SMIS_IP $VMAX_SMIS_PORT $SMIS_USER "$SMIS_PASSWD" $VMAX_SMIS_SSL

    run storagedevice discover_all --ignore_error
    #discoveredsystem show $SRDF_VMAXA_NATIVEGUID &> /dev/null && return $?
    #discoveredsystem show $SRDF_VMAXB_NATIVEGUID &> /dev/null && return $?
    discoveredsystem show $SRDF_V3_VMAXB_NATIVEGUID &> /dev/null && return $?
    discoveredsystem show $SRDF_V3_VMAXA_NATIVEGUID &> /dev/null && return $?    

}

srdf_network_setup() {
    secho "SRDF network setup"

    # Do once
    nsys=`networksystem list | wc -l`
    [ "$nsys" -gt 0 ] && return;

    #Discover the Brocade SAN switch.
    #brocade_setup_once

    run networksystem create $BROCADE_NETWORK brocade --smisip $BROCADE_IP --smisport 5989 --smisuser $BROCADE_USER --smispw $BROCADE_PW --smisssl true
    secho "Sleeping 30 seconds..."
    sleep 30
    is_present=$(transportzone listall | (grep ${SRDF_VMAXA_VSAN} || echo ''))
    if [ "$is_present" == '' ]; then
        echo "Discovering network"
        run networksystem discover $BROCADE_NETWORK
        secho "Sleeping 30 seconds..."
        sleep 30
    fi

    run tenant create $TENANT $LOCAL_LDAP_AUTHN_DOMAINS 'OU' ${seed}
    run neighborhood create $NH
    run neighborhood allow $NH $TENANT
}

srdf_create_rdfg() {
    retry=${SRDF_RDF_CREATE_RETRY}
    while : ; do
        SRDF_USED_RDFGS=()

        srdf_get_rdfg_number
        echo "Creating RDF group ${SRDF_PROJECT} with group number ${SRDF_RDF_NUMBER}"
        srdf_run_symrdf addgrp -rdfg ${SRDF_RDF_NUMBER} -dir ${SRDF_V3_VMAXA_DIR} -remote_rdfg ${SRDF_RDF_NUMBER} -remote_sid ${SRDF_V3_VMAXB_SID} -remote_dir ${SRDF_V3_VMAXB_DIR}
        ret=${SRDF_SSH_ERROR}
        if [ $ret -eq 0 ]; then
            echo "Completed creating group"
            return
        elif [ $ret -eq 1 ]; then
            echo "Failed to create group"
            return 1
        elif [ $retry -gt 0 ]; then
            retry=$[$retry-1]
            if [ $ret -eq 2 ]; then
                srdf_sleep
            fi

            if [ $ret -eq 3 ]; then
                echo "Generate new RDF name"
                SRDF_PROJECT=$(srdf_generate_rdfg_name)
            fi

            echo "Retrying with new group name/number"
        else
            echo "Failed to create group"
            return 1
        fi
    done
}

srdf_get_rdfg_number() {
    srdf_find_rdfgs "${SRDF_V3_VMAXA_SMIS_IP}" "${SRDF_V3_VMAXA_SSH_PW}" "${SRDF_V3_VMAXA_SID}"
    srdf_find_rdfgs "${SRDF_V3_VMAXB_SMIS_IP}" "${SRDF_V3_VMAXB_SSH_PW}" "${SRDF_V3_VMAXB_SID}"

    declare -A rdfgs
    for rdfg in ${SRDF_USED_RDFGS}; do
        rdfgs[$rdfg]=1
    done

    candidates=()
    for candidate in {1..250}; do
        if [[ ! ${rdfgs[$candidate]} ]]; then
            candidates+=($candidate)
        fi
    done

    if [ ${#candidates[@]} -eq 0 ]; then
        echo "Cannot find available RGF group number"
        return 1
    fi

    SRDF_RDF_NUMBER=${candidates[$RANDOM % ${#candidates[@]}]}
}

srdf_find_rdfgs() {
    cmd="${SYMCLI_PATH}/symcfg list -sid ${3} -rdfg all | grep '^[ 0-9].*[0-9]$' | awk '{print \$1}' 2>&1"
    srdf_ssh "${1}" "${2}" "$cmd"

    if [ "${SRDF_SSH_RESULT}" != "" ]; then
        SRDF_USED_RDFGS+=(${SRDF_SSH_RESULT})
    else
        echo "Failed to get RDF groups"
        return 1
    fi
}

srdf_run_symrdf() {
    cmd="${SYMCLI_PATH}/symrdf $@ -label $SRDF_PROJECT -sid ${SRDF_V3_VMAXA_SID} -noprompt 2>&1"
    srdf_ssh "${SRDF_V3_VMAXA_SMIS_IP}" "${SRDF_V3_VMAXA_SSH_PW}" "$cmd"
    ret=${SRDF_SSH_ERROR}
    if [ $ret -ne 0 ]; then
        echo "Failed to call symrdf $1 - $SRDF_SSH_RESULT"

        is_present=$(echo ${SRDF_SSH_RESULT} | (grep "^The SYMAPI database file is already locked by another process" || echo ''))
        if [ "$is_present" != '' ]; then
            SRDF_SSH_ERROR=2
            return
        fi

        is_present=$(echo ${SRDF_SSH_RESULT} | (grep "^The specified dynamic RDF group label is in use" || echo ''))
        if [ "$is_present" != '' ]; then
           SRDF_SSH_ERROR=3
           return
        fi

        is_present=$(echo ${SRDF_SSH_RESULT} | (grep "^The .* RDF group number specified is already defined" || echo ''))
        if [ "$is_present" != '' ]; then
            SRDF_SSH_ERROR=4
            return
        fi

        SRDF_SSH_ERROR=1
        return
    fi

    is_present=$(echo ${SRDF_SSH_RESULT} | (grep "^Successfully" || echo ''))
    if [ "$is_present" == '' ]; then
        echo "Operation $1 failed for RDF group $SRDF_PROJECT - ${SRDF_SSH_RESULT}"
        SRDF_SSH_ERROR=1
    else
        echo "Operation $1 completed for RDF group ${SRDF_PROJECT}"
    fi
}

srdf_common_setup() {
    #zone_setup - for CDP use NH as the protection neighborhood as well
    echo 'Adding endpoints to transport zone'
    # TODO: Try to get rid of this; these should be in the network already.
    if [ "$SRDF_QUICK_PARAM" != "quick" ]; then
        for storageport in ${SRDF_VMAXA_STORAGEPORTS}
        do
          run transportzone add $NH/${SRDF_VMAXA_VSAN} ${storageport}
        done
        echo 'Done adding endpoints to network'
        run transportzone assign ${SRDF_VMAXB_VSAN} $NH
        for storageport in ${SRDF_VMAXB_STORAGEPORTS}
        do
          run transportzone add ${NH}/${SRDF_VMAXB_VSAN} ${storageport}
        done
        echo 'Done adding endpoints to network'
    fi

    srdf_v3_varray_setup
    # srdf_cos_setup
    srdf_v3_cos_setup
}

srdf_cos_setup()
{
    # Create the target first so it exists when we create the source vpool
    cos show $COS_VMAXBLOCK_SRDF_TARGET block &> /dev/null && return $?
    run cos create block $COS_VMAXBLOCK_SRDF_TARGET		          \
	--description 'Target-Virtual-Pool-for-SRDF-Protection' true \
			 --protocols FC 		          \
			 --numpaths 1				  \
			 --max_snapshots 10 			  \
			 --provisionType 'Thin'	          \
			 --neighborhoods $NH                      \
                         --multiVolumeConsistency                  \
                         --system_type vmax			

    run cos update block $COS_VMAXBLOCK_SRDF_TARGET --storage ${SRDF_VMAXB_NATIVEGUID}
    run cos update block $COS_VMAXBLOCK_SRDF_TARGET --storage ${SRDF_VMAXA_NATIVEGUID}
    run cos allow $COS_VMAXBLOCK_SRDF_TARGET block $TENANT

    cos show ${COS_VMAXBLOCK_SRDF_SOURCE}_sync block &> /dev/null && return $?
    run cos create block ${COS_VMAXBLOCK_SRDF_SOURCE}_sync                 \
	--description 'Source-Virtual-Pool-for-Synchronous-SRDF-Protection' true \
			 --protocols FC 		        \
			 --numpaths 1				\
			 --max_snapshots 10			\
	                 --provisionType 'Thin'	        \
                         --system_type vmax                     \
			 --neighborhoods $NH                    \
			 --srdf "${NH}:${COS_VMAXBLOCK_SRDF_TARGET}:SYNCHRONOUS"

    run cos update block ${COS_VMAXBLOCK_SRDF_SOURCE}_sync --storage ${SRDF_VMAXA_NATIVEGUID}
    run cos update block ${COS_VMAXBLOCK_SRDF_SOURCE}_sync --storage ${SRDF_VMAXB_NATIVEGUID}
    run cos allow ${COS_VMAXBLOCK_SRDF_SOURCE}_sync block $TENANT

    cos show ${COS_VMAXBLOCK_SRDF_SOURCE}_async block &> /dev/null && return $?
    run cos create block ${COS_VMAXBLOCK_SRDF_SOURCE}_async                 \
	--description 'Source-Virtual-Pool-for-Asynchronous-SRDF-Protection' true \
			 --protocols FC 		        \
			 --numpaths 1				\
			 --max_snapshots 10			\
	                 --provisionType 'Thin'	        \
                         --system_type vmax                     \
			 --neighborhoods $NH                    \
                         --multiVolumeConsistency               \
			 --srdf "${NH}:${COS_VMAXBLOCK_SRDF_TARGET}:ASYNCHRONOUS"

    run cos update block ${COS_VMAXBLOCK_SRDF_SOURCE}_async --storage ${SRDF_VMAXA_NATIVEGUID}
    run cos update block ${COS_VMAXBLOCK_SRDF_SOURCE}_async --storage ${SRDF_VMAXB_NATIVEGUID}
    run cos allow ${COS_VMAXBLOCK_SRDF_SOURCE}_async block $TENANT
}

srdf_v3_varray_setup()
{
    secho 'SRDF V3 varray setup'

    # Create Virtual Array
    run neighborhood create $V3_SRDF_VARRAY
    run neighborhood allow $V3_SRDF_VARRAY $TENANT
    
    if [ "$SRDF_V3_VMAXA_NATIVEGUID" != "NONE" ]; then
        storageport update $SRDF_V3_VMAXA_NATIVEGUID FC --addvarrays $V3_SRDF_VARRAY
    fi
    
    if [ "$SRDF_V3_VMAXB_NATIVEGUID" != "NONE" ]; then
        storageport update $SRDF_V3_VMAXB_NATIVEGUID FC --addvarrays $V3_SRDF_VARRAY
    fi
}

srdf_v3_cos_setup()
{
    secho "SRDF V3 Virtual Pool setup" 
    # Create the target first so it exists when we create the source vpool
    cos show $COS_VMAXBLOCK_V3_SRDF_TARGET block &> /dev/null && return $?
    # Workaround for COP-25718, switch to use matchedPools once it is fixed
    run cos create block $COS_VMAXBLOCK_V3_SRDF_TARGET		          \
	--description 'Target-Virtual-Pool-for-V3-SRDF-Protection' false \
			 --protocols FC 		          \
			 --numpaths 1				  \
			 --max_snapshots 10 			  \
			 --provisionType 'Thin'	          \
			 --neighborhoods $V3_SRDF_VARRAY                      \
                         --multiVolumeConsistency                  \
                         --system_type vmax			
    
    run cos update block $COS_VMAXBLOCK_V3_SRDF_TARGET --storage ${SRDF_V3_VMAXB_NATIVEGUID}
    #run cos update block $COS_VMAXBLOCK_V3_SRDF_TARGET --storage ${SRDF_V3_VMAXA_NATIVEGUID}
    run cos allow $COS_VMAXBLOCK_V3_SRDF_TARGET block $TENANT
    
    cos show ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_active block &> /dev/null && return $?
    run cos create block ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_active                 \
	--description 'Source-Virtual-Pool-for-Active-SRDF-Protection' false \
			 --protocols FC 		        \
			 --numpaths 1				\
			 --max_snapshots 10			\
	                 --provisionType 'Thin'	        \
                         --system_type vmax                     \
                         --multiVolumeConsistency		\
			 --neighborhoods $V3_SRDF_VARRAY                    \
			 --srdf "${V3_SRDF_VARRAY}:${COS_VMAXBLOCK_V3_SRDF_TARGET}:ACTIVE"

    run cos update block ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_active --storage ${SRDF_V3_VMAXA_NATIVEGUID}
    #run cos update block ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_active --storage ${SRDF_V3_VMAXB_NATIVEGUID}
    run cos allow ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_active block $TENANT
	
	## ASYNCHRONOUS
	
	cos show ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_async block &> /dev/null && return $?
    run cos create block ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_async                 \
	--description 'Source-Virtual-Pool-for-Async-SRDF-Protection' true \
			 --protocols FC 		        \
			 --numpaths 1				\
			 --max_snapshots 10			\
	                 --provisionType 'Thin'	        \
                         --system_type vmax                     \
                         --multiVolumeConsistency		\
			 --neighborhoods $V3_SRDF_VARRAY                    \
			 --srdf "${V3_SRDF_VARRAY}:${COS_VMAXBLOCK_V3_SRDF_TARGET}:ASYNCHRONOUS"

    run cos update block ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_async --storage ${SRDF_V3_VMAXA_NATIVEGUID}
    #run cos update block ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_async --storage ${SRDF_V3_VMAXB_NATIVEGUID}
    run cos allow ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_async block $TENANT 
	
	## SYNCHRONOUS
	
	cos show ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_sync block &> /dev/null && return $?
    run cos create block ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_sync                 \
	--description 'Source-Virtual-Pool-for-Sync-SRDF-Protection' true \
			 --protocols FC 		        \
			 --numpaths 1				\
			 --max_snapshots 10			\
	                 --provisionType 'Thin'	        \
                         --system_type vmax                     \
                         --multiVolumeConsistency		\
			 --neighborhoods $V3_SRDF_VARRAY                    \
			 --srdf "${V3_SRDF_VARRAY}:${COS_VMAXBLOCK_V3_SRDF_TARGET}:SYNCHRONOUS"

    run cos update block ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_sync --storage ${SRDF_V3_VMAXA_NATIVEGUID}
    #run cos update block ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_sync --storage ${SRDF_V3_VMAXB_NATIVEGUID}
    run cos allow ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_sync block $TENANT 
	
	
}

srdf_cos_matcher_test()
{
    # Match various pool criterias, ensure SRDF vpools produce the right pools
    echo "BEGIN: Virtual Pool matching test: right now these tests are visually verified!"
    run cos match block true --protocols FC 		        \
			 --numpaths 1				\
			 --max_snapshots 10			\
	                 --provisionType 'Thin'	        \
			 --neighborhoods $NH                    \
                         --multiVolumeConsistency               \
			 --srdf "${NH}:${COS_VMAXBLOCK_V3_SRDF_TARGET}:ASYNCHRONOUS"
    #expectN=`grep "name" /tmp/file.txt | wc -l`
    #if [ $expectN -eq 3 ]
    #then
    #   echo "Expected 3 storage pools, got $expectN instead"
    #   fail;
    #fi   
    echo "END: Virtual Pool matching test: right now these tests are visually verified!"
}

srdf_cos_change()
{
   # Create non-protected volume, then move it to a cos that has protection
   srdfvolumecc="${SRDF_VOLUME}-coschange"
   run volume create ${srdfvolumecc} $PROJECT $NH $COS_VMAXBLOCK_V3_SRDF_TARGET 1GB
   echo "Created volume, now sleeping..."
   sleep 60
   echo "Changing cos..."
   run volume change_cos $PROJECT/${srdfvolumecc} ${COS_VMAXBLOCK_V3_SRDF_TARGET}_sync
   echo "Changed cos"
   sleep 60
   echo "Deleting volume"
   run volume delete $PROJECT/${srdfvolumecc} --wait
}

srdf_v3_cos_change()
{
   # Create non-protected volume, then move it to a cos that has protection
   cg=cg${RANDOM}
   run blockconsistencygroup create $SRDF_PROJECT $cg

   srdfvolumecc="${SRDF_VOLUME}-coschange"
   run volume create ${srdfvolumecc} $SRDF_PROJECT $V3_SRDF_VARRAY $COS_VMAXBLOCK_V3_SRDF_TARGET 1GB --consistencyGroup $cg
   echo "Created volume, now sleeping..."
   srdf_sleep
   echo "Changing cos..."

   # Workaround for COP-25718, remove the cos update once it is fixed
   run cos update block $COS_VMAXBLOCK_V3_SRDF_TARGET --storage ${SRDF_V3_VMAXA_NATIVEGUID}
   run cos update block ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_active --storage ${SRDF_V3_VMAXB_NATIVEGUID}

   run volume change_cos $SRDF_PROJECT/${srdfvolumecc} ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_active
   echo "Changed cos"
   srdf_sleep
   echo "Deleting volume"
   run volume delete $SRDF_PROJECT/${srdfvolumecc} --wait
   run_noundo blockconsistencygroup delete $cg
}

srdf_cos_change_meta()
{
   # Create non-protected meta volume, then move it to a cos that has protection
   srdfvolumeccmeta="${SRDF_VOLUME}-coschangemeta"
   run volume create ${srdfvolumeccmeta} $SRDF_PROJECT $NH $COS_VMAXBLOCK_SRDF_TARGET 300GB
   echo "Created volume, now sleeping..."
   sleep 60
   echo "Changing cos..."
   run volume change_cos $PROJECT/${srdfvolumeccmeta} ${COS_VMAXBLOCK_SRDF_SOURCE}_sync
   echo "Changed cos"
   sleep 60
   echo "Deleting volume"
   run volume delete $SRDF_PROJECT/${srdfvolumeccmeta} --wait
}



srdf_basic_sync_test()
{
   srdfvolume="${SRDF_VOLUME}_sync"
   run volume create ${srdfvolume}1 $SRDF_PROJECT $V3_SRDF_VARRAY ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_sync 1GB
   srdf_sleep
   #run volume create ${srdfvolume}2 $PROJECT $NH ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_sync 1GB --consistencygroup $CONSISTENCY_GROUP_SRDF
   #run volume create ${srdfvolume}3 $PROJECT $NH ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_sync 1GB --count 2
   #run volume delete $PROJECT/${srdfvolume}2 --wait
   run volume delete $SRDF_PROJECT/${srdfvolume}1 --wait
   srdf_sleep
   echo "Rediscovering storage systems"
   run storagedevice discover_all --ignore_error
   run volume create ${srdfvolume}2 $SRDF_PROJECT $V3_SRDF_VARRAY ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_sync 1GB --consistencyGroup ${CONSISTENCY_GROUP_SRDF}
   srdf_sleep
   run volume delete $SRDF_PROJECT/${srdfvolume}2 --wait
   #run volume delete $PROJECT/${srdfvolume}3-1 --wait
   #run volume delete $PROJECT/${srdfvolume}3-2 --wait
}

srdf_basic_async_test()
{
   srdfvolume="${SRDF_VOLUME}_async"
   srdf_sleep
   echo "Rediscovering storage systems"
   run storagedevice discover_all --ignore_error
   run volume create ${srdfvolume}1 $SRDF_PROJECT $V3_SRDF_VARRAY ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_async 1GB --consistencyGroup ${CONSISTENCY_GROUP_SRDF}
   srdf_sleep
   echo "Rediscovering storage systems"
   run storagedevice discover_all --ignore_error
   run volume create ${srdfvolume}2 $SRDF_PROJECT $V3_SRDF_VARRAY ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_async 1GB --consistencyGroup ${CONSISTENCY_GROUP_SRDF}
   echo "Created 2 volumes... now going to sleep..."
   srdf_sleep
   echo "Deleting volumes..."
   #run volume create ${srdfvolume}3 $PROJECT $NH ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_async 1GB --count 2
   run volume delete $SRDF_PROJECT/${srdfvolume}1 --wait
   run volume delete $SRDF_PROJECT/${srdfvolume}2 --wait
   #run volume delete $PROJECT/${srdfvolume}3-1 --wait
   #run volume delete $PROJECT/${srdfvolume}3-2 --wait
}

srdf_basic_active_test()
{
   srdfvolume="${SRDF_VOLUME}_active"
   run volume create ${srdfvolume}1 $SRDF_PROJECT $V3_SRDF_VARRAY ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_active 1GB
   srdf_sleep
   # rediscover storage systems for RDF group mode getting updated
   echo "Rediscovering storage systems"
   run storagedevice discover_all --ignore_error

   run volume create ${srdfvolume}2 $SRDF_PROJECT $V3_SRDF_VARRAY ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_active 1GB
   srdf_sleep
   run volume delete $SRDF_PROJECT/${srdfvolume}1 --wait
   run volume delete $SRDF_PROJECT/${srdfvolume}2 --wait

   # rediscover storage systems for RDF group mode getting updated
   echo "Rediscovering storage systems"
   run storagedevice discover_all --ignore_error
   run volume create ${srdfvolume}3 $SRDF_PROJECT $V3_SRDF_VARRAY ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_active 1GB --consistencyGroup ${CONSISTENCY_GROUP_SRDF}v3
   srdf_sleep
   echo "Rediscovering storage systems"
   run storagedevice discover_all --ignore_error
   run volume create ${srdfvolume}4 $SRDF_PROJECT $V3_SRDF_VARRAY ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_active 1GB --consistencyGroup ${CONSISTENCY_GROUP_SRDF}v3
   srdf_sleep
   run volume delete $SRDF_PROJECT/${srdfvolume}3 --wait
   run volume delete $SRDF_PROJECT/${srdfvolume}4 --wait
}

srdf_tests()
{
   echo 'Run SRDF tests'
   echo 'About to run cos matcher test'
   srdf_cos_matcher_test
   
   echo 'About to run basic SRDF active test'
   sleep 60
   srdf_basic_active_test

   echo 'About to run basic SRDF async test'
   sleep 60
   srdf_basic_async_test
   echo 'About to run basic SRDF sync test'
   sleep 60
   srdf_basic_sync_test
   
   echo 'About to run SRDF cos change test'
   #srdf_cos_change
   # echo 'About to run SRDF cos change test for meta volume'
   # sleep 60
   # srdf_cos_change_meta

   echo 'About to run V3 SRDF cos change test'
   sleep 60
   srdf_v3_cos_change

   if [ "$SRDF_QUICK_PARAM" != "quick" ]; then
       echo 'About to remove RDF group using symcli'
       sleep 60
       srdf_run_symrdf removegrp
   fi

   echo 'Done Running SRDF tests'
}

vmax_cos_setup()
{
    if [ $QUICK -eq 0 ]; then
       run cos create block $COS_VMAXBLOCK				\
	   --description 'Virtual-Pool-for-VMAX-block-FC+iSCSI' true 	\
                         --protocols FC iSCSI 			\
                         --numpaths 1 \
                         --max_snapshots 10 \
                         --provisionType 'Thin' \
	                 --system_type vmax \
                         --expandable true \
                         --neighborhoods $NH
    else
       run cos create block $COS_VMAXBLOCK                          \
	   --description 'Virtual-Pool-for-VMAX-block-FC+iSCSI' true       \
                         --protocols FC                   \
                         --numpaths 2 \
                         --max_snapshots 10 \
                         --provisionType 'Thin' \
                         --system_type vmax \
                         --expandable true \
                         --neighborhoods $NH
    fi

    run cos create block $COS_VMAXBLOCK_FC				\
	--description 'Virtual-Pool-for-VMAX-block-FC' true 	\
                         --protocols FC 			\
                         --numpaths 2 \
                         --max_snapshots 10 \
	                 --system_type vmax \
                         --provisionType 'Thin' \
			 --neighborhoods $NH

    run cos create block $COS_VMAXBLOCK_ISCSI			\
	--description 'Virtual-Pool-for-VMAX-block-iSCSI' true \
                         --protocols iSCSI 			\
                         --numpaths 1 \
                         --max_snapshots 10 \
	                 --system_type vmax \
                         --provisionType 'Thin' \
			 --neighborhoods $NH

    run cos create block $COS_VMAXBLOCK_THIN 				\
	--description 'VMAX-thin-storage' true      \
                             --protocols FC iSCSI	    \
                             --numpaths 1 \
                             --max_snapshots 10 \
	                     --system_type vmax \
                             --provisionType 'Thin' \
                             --expandable true \
                             --neighborhoods $NH

    run cos create block $COS_VMAXBLOCK_THICK 				\
	--description 'VMAX-thick-storage' true      \
                             --protocols FC iSCSI	    \
                             --numpaths 1  \
                             --max_snapshots 10 \
	                     --system_type vmax \
                             --provisionType 'Thick' \
			 --neighborhoods $NH



    if [ $VMAX3_COMPRESSION -eq 1 ]; then

	#Create virtual pools on VMAX all flash array with compression enabled and disabled.
    	run cos create block $COS_VMAXBLOCK_V3_COMP_ENABLED 				\
		--description 'VMAX-AFA-Compression-enabled' true      \
                             --protocols FC     \
                             --numpaths 1  \
                             --max_snapshots 10 \
	                     --system_type vmax \
                             --provisionType 'Thin' \
			     --auto_tiering_policy_name "${VMAX3_AUTO_TIER_POLICY_NAME}" \
			     --compressionEnabled true \
			 --neighborhoods $NH

    	run cos create block $COS_VMAXBLOCK_V3_COMP_NOTENABLED 				\
		--description 'VMAX-AFA-Compression-NOT-enabled' true      \
                             --protocols FC     \
                             --numpaths 1  \
                             --max_snapshots 10 \
	                     --system_type vmax \
			     --auto_tiering_policy_name "${VMAX3_AUTO_TIER_POLICY_NAME}" \
                             --provisionType 'Thin' \
			     --compressionEnabled false \
			 --neighborhoods $NH
   fi 
}

mirrorblock_cos_setup()
{
    run cos create block $COS_MIRROR                       \
	--description 'VMAX-Mirror-block-FC' true   \
                            --protocols FC iSCSI       \
                            --numpaths 1               \
                            --max_snapshots 10         \
                            --max_mirrors 10           \
                            --provisionType 'Thin'     \
	                    --system_type vmax \
                            --expandable false \
			    --neighborhoods $NH

    run cos create block $COS_MIRROR_WITH_OPTIONAL         \
	--description 'VMAX-Mirror-block-FC-with-optional-CoS' true   \
                            --protocols FC iSCSI       \
                            --numpaths 1               \
                            --max_snapshots 10         \
                            --max_mirrors 1            \
                            --provisionType 'Thin'     \
                            --mirror_cos $COS_VMAXBLOCK_FC   \
	                    --system_type vmax \
                            --expandable false \
			    --neighborhoods $NH

    run cos create block $COS_MIRROR_WITH_2_MIRRORS        \
	--description 'VMAX-Mirror-block-FC-with-2-mirrors-maximum' true   \
                            --protocols FC iSCSI       \
                            --numpaths 1               \
                            --max_snapshots 10         \
                            --max_mirrors 2            \
                            --provisionType 'Thin'     \
	                    --system_type vmax \
                            --expandable false \
			    --neighborhoods $NH

    run cos create block $COS_MIRROR_BEFORE_CHANGE         \
	--description 'VMAX-block-FC-with-no-mirrors-explicitly' true   \
                            --protocols FC iSCSI       \
                            --numpaths 1               \
                            --max_snapshots 10         \
                            --max_mirrors 0            \
                            --provisionType 'Thin'     \
	                    --system_type vmax \
                            --expandable false \
			    --neighborhoods $NH

    run cos create block $COS_MIRROR_AFTER_CHANGE          \
	--description 'VMAX-block-FC-with-1-mirror-explicitly' true   \
                            --protocols FC iSCSI       \
                            --numpaths 1               \
                            --max_snapshots 10         \
                            --max_mirrors 1            \
                            --provisionType 'Thin'     \
	                    --system_type vmax \
                            --expandable false \
			    --neighborhoods $NH

    run cos create block $COS_MIRROR_VNX                   \
	--description 'VNX-block-mirror' true 	\
                            --protocols FC iSCSI       \
                            --numpaths 1               \
                            --max_snapshots 10         \
                            --max_mirrors 3            \
                            --provisionType 'Thin'     \
	                    --system_type vnxblock \
                            --expandable false \
			    --neighborhoods $NH

    run cos create block $COS_VMAX_CG_MIRROR \
	--description 'Virtual-Pool-for-VMAX-block-FC' true \
                            --protocols FC \
                            --numpaths 2 \
                            --max_snapshots 10 \
                            --max_mirrors 10 \
                            --system_type vmax \
                            --expandable false \
                            --provisionType 'Thin' \
                            --neighborhoods $NH \
                            --multiVolumeConsistency
}

consistencygroup_block_cos_setup()
{
    # Create CoS for VNX
    run cos create block $VNX_COS_GROUP                       \
	--description 'Consistency-Group-Block-VNX-CoS' true   \
                            --protocols FC iSCSI       \
							--system_type vnxblock     \
                            --numpaths 1               \
                            --max_snapshots 10         \
                            --provisionType 'Thin'     \
			    --neighborhoods $NH        \
                            --multiVolumeConsistency   

    run cos update block $VNX_COS_GROUP --storage $VNXB_NATIVEGUID
    run cos allow $VNX_COS_GROUP block $TENANT

    # Create CoS for VMAX
    run cos create block $VMAX_COS_GROUP                       \
	--description 'Consistency-Group-Block-VMAX-CoS' true   \
                            --protocols FC iSCSI       \
							--system_type vmax         \
                            --numpaths 1               \
                            --max_snapshots 10         \
                            --provisionType 'Thin'     \
			    --neighborhoods $NH                \
                            --multiVolumeConsistency

    run cos update block $VMAX_COS_GROUP --storage $VMAX_NATIVEGUID
    run cos allow $VMAX_COS_GROUP block $TENANT

    # Create CoS without the MultiVolumeConsistency attribute
    COS_GROUP_INVALID=cos-group-no-multivolumeconsistency"$date"
    run cos create block $COS_GROUP_INVALID                       \
	--description 'Consistency-Group-Block-VirtualPool-with-no-multiVolumeConsistency' true   \
                            --protocols FC iSCSI       \
                            --numpaths 1               \
                            --max_snapshots 10         \
                            --provisionType 'Thin'     \
			    --neighborhoods $NH
}

vplex_cos_setup()
{
    run cos create block cosvplexlocal false                  \
                     --description 'Local-Vpool-for-VPLEX'    \
                     --protocols FC                           \
                     --numpaths 2                             \
                     --provisionType 'Thin'                   \
                     --highavailability vplex_local           \
                     --neighborhoods $NH $NH2                 \
                     --max_snapshots 1                        \
                     --max_mirrors 1                          \
                     --expandable false 

    run cos allow cosvplexlocal block $TENANT
    if [ "${VPLEX_QUICK_PARAM}" = "quick" ]; then
        run cos update block cosvplexlocal --storage $VPLEX_SIM_VMAX1_NATIVEGUID
        run cos update block cosvplexlocal --storage $VPLEX_SIM_VMAX2_NATIVEGUID

        run cos create block vplexdistbackingvpool false                   \
                         --description 'Local-backing-vpool-for-VPLEX'     \
                         --protocols FC                           \
                         --numpaths 2                             \
                         --provisionType 'Thin'                   \
                         --neighborhoods $NH2                     \
                         --expandable false 

    else
        if [ $VPLEX_USE_XIO -eq 1 ]; then
        run cos update block cosvplexlocal --storage $VPLEX_XIO1_NATIVEGUID
        else 
        run cos update block cosvplexlocal --storage $VPLEX_VNX1_NATIVEGUID
        fi
        if [ "$AUTH" != 'ipv6' ] ; then
                if [ $VPLEX_USE_XIO -eq 1 ]; then
                run cos update block cosvplexlocal --storage $VPLEX_XIO2_NATIVEGUID
                else
                run cos update block cosvplexlocal --storage $VPLEX_VNX2_NATIVEGUID
                fi
        fi
        run cos update block cosvplexlocal --storage $VPLEX_VMAX_NATIVEGUID
    fi

    run cos allow cosvplexlocal block $TENANT

    if [ "${VPLEX_QUICK_PARAM}" = "quick" ]; then
        run cos update block vplexdistbackingvpool --storage $VPLEX_SIM_VMAX5_NATIVEGUID

        run cos allow vplexdistbackingvpool block $TENANT

        run cos create block cosvplexdist false                         \
                         --description 'Distributed-Vpool-for-VPLEX'    \
                         --protocols FC                                 \
                         --numpaths 2                                   \
                         --provisionType 'Thin'                         \
                         --highavailability vplex_distributed           \
                         --neighborhoods $NH $NH2                       \
                         --haNeighborhood $NH2                          \
                         --haCos vplexdistbackingvpool                  \
                         --max_snapshots 1                              \
                         --max_mirrors 1                                \
                         --mirror_cos cosvplexlocal                     \
                         --expandable false

        run cos update block cosvplexdist --storage $VPLEX_SIM_VMAX1_NATIVEGUID
        run cos update block cosvplexdist --storage $VPLEX_SIM_VMAX2_NATIVEGUID

    else
        run cos create block cosvplexdist false                     \
                     --description 'Distributed-Vpool-for-VPLEX'    \
                     --protocols FC                                 \
                     --numpaths 2                                   \
                     --provisionType 'Thin'                         \
                     --highavailability vplex_distributed           \
                     --neighborhoods $NH $NH2                       \
                     --haNeighborhood $NH2                          \
                     --max_snapshots 1                              \
                     --max_mirrors 1                                \
                     --mirror_cos cosvplexlocal                     \
                     --expandable false

        run cos allow cosvplexdist block $TENANT
        if [ $VPLEX_USE_XIO -eq 1 ]; then
            run cos update block cosvplexdist --storage $VPLEX_XIO1_NATIVEGUID
        else
            run cos update block cosvplexdist --storage $VPLEX_VNX1_NATIVEGUID
        fi
        run cos update block cosvplexdist --storage $VPLEX_VMAX_NATIVEGUID
    fi
}

vplex_setup()
{
    vplex_setup_once
}

vplex_sim_variable_setup()
{
    VPLEX_GUID=$VPLEX_SIM_VPLEX_GUID
    VPLEX_VNX1_NATIVEGUID=$VPLEX_SIM_VMAX1_NATIVEGUID
    VPLEX_VNX2_NATIVEGUID=$VPLEX_SIM_VMAX2_NATIVEGUID
    VPLEX_VMAX_NATIVEGUID=$VPLEX_SIM_VMAX5_NATIVEGUID
    CLUSTER1NET_NAME=$CLUSTER1NET_SIM_NAME
}

vplex_setup_once()
{
    run syssvc $CONFIG_FILE $BOURNE_IPADDR set_prop controller_vplex_director_min_port_count 1
    if [ "$AUTH" == 'ipv6' ] ; then
        echo "Setting up VPLEX environment for IPv6"
        # Discover the storage systems 
        run smisprovider create $VPLEX_VMAX_SMIS_DEV_NAME $VPLEX_VMAX_SMIS_IP $VPLEX_VMAX_SMIS_PORT $VPLEX_SMIS_USER "$VPLEX_SMIS_PASSWD" $VPLEX_VMAX_SMIS_SSL
        run storageprovider create $VPLEX_DEV_NAME $VPLEX_IP 443 $VPLEX_USER "$VPLEX_PASSWD" vplex
        run storagedevice discover_all
        storagedevice list
        storageport list $VPLEX_VNX1_NATIVEGUID
        storageport list $VPLEX_VMAX_NATIVEGUID
        storageport list $VPLEX_GUID

        # Setup the varrays. $NH contains VPLEX cluster-1 and $NH2 contains VPLEX cluster-2.
        run storageport update $VPLEX_GUID FC --group director-1-1-A --addvarrays $NH
        run storageport update $VPLEX_GUID FC --group director-1-1-B --addvarrays $NH
        run storageport update $VPLEX_GUID FC --group director-2-1-A --addvarrays $NH2
        run storageport update $VPLEX_GUID FC --group director-2-1-B --addvarrays $NH2
        # The arrays are assigned to individual varrays as well.
        run storageport update $VPLEX_VNX1_NATIVEGUID FC --addvarrays $NH
        run storageport update $VPLEX_VMAX_NATIVEGUID FC --addvarrays $NH2        
    elif [ "${VPLEX_QUICK_PARAM}" = "quick" ]; then
        echo "Setting up VPLEX environment connected to simulators on: ${VPLEX_SIM_IP}"

        # Discover the storage systems 
        echo "Discovering back-end storage arrays using ECOM/SMIS simulator on: $VPLEX_SIM_SMIS_IP..."
        smisprovider show $VPLEX_SIM_SMIS_DEV_NAME &> /dev/null && return $?
        run smisprovider create $VPLEX_SIM_SMIS_DEV_NAME $VPLEX_SIM_SMIS_IP $VPLEX_VMAX_SMIS_SIM_PORT $VPLEX_SIM_SMIS_USER "$VPLEX_SIM_SMIS_PASSWD" false

        echo "Discovering VPLEX using simulator on: ${VPLEX_SIM_IP}..."
        storageprovider show $VPLEX_SIM_DEV_NAME &> /dev/null && return $?
        run storageprovider create $VPLEX_SIM_DEV_NAME $VPLEX_SIM_IP 443 $VPLEX_SIM_USER "$VPLEX_SIM_PASSWD" vplex
        run storagedevice discover_all

        # Setup the varrays. $NH contains VPLEX cluster-1 and $NH2 contains VPLEX cluster-2.
        run storageport update $VPLEX_SIM_VPLEX_GUID FC --group director-1-1-A --addvarrays $NH
        run storageport update $VPLEX_SIM_VPLEX_GUID FC --group director-1-1-B --addvarrays $NH
        run storageport update $VPLEX_SIM_VPLEX_GUID FC --group director-1-2-A --addvarrays $NH
        run storageport update $VPLEX_SIM_VPLEX_GUID FC --group director-1-2-B --addvarrays $NH
        run storageport update $VPLEX_SIM_VPLEX_GUID FC --group director-2-1-A --addvarrays $NH2
        run storageport update $VPLEX_SIM_VPLEX_GUID FC --group director-2-1-B --addvarrays $NH2
        run storageport update $VPLEX_SIM_VPLEX_GUID FC --group director-2-2-A --addvarrays $NH2
        run storageport update $VPLEX_SIM_VPLEX_GUID FC --group director-2-2-B --addvarrays $NH2

        # The arrays are assigned to individual varrays as well.
        run storageport update $VPLEX_SIM_VMAX1_NATIVEGUID FC --addvarrays $NH
        run storageport update $VPLEX_SIM_VMAX2_NATIVEGUID FC --addvarrays $NH
        run storageport update $VPLEX_SIM_VMAX5_NATIVEGUID FC --addvarrays $NH2

        vplex_sim_variable_setup
        vplex_cos_setup

        echo "Done setting up VPLEX environment..."
    else
        echo "Setting up VPLEX environment for IPv4"

        #Discover the Brocade SAN switch.
        brocade_setup_once

        # Discover the storage systems 
        echo "Discovering Storage Assets"
        smisprovider show $VPLEX_VMAX_SMIS_DEV_NAME &> /dev/null && return $?
        run smisprovider create $VPLEX_VMAX_SMIS_DEV_NAME $VPLEX_VMAX_SMIS_IP $VPLEX_VMAX_SMIS_PORT $VPLEX_SMIS_USER "$VPLEX_SMIS_PASSWD" $VPLEX_VMAX_SMIS_SSL

        if [ $VPLEX_USE_XIO -eq 1 ]; then
            run storageprovider create $VPLEX_XIO1_SMIS_DEV_NAME $VPLEX_XIO1_SMIS_IP $VPLEX_XIO1_SMIS_PORT $VPLEX_XIO1_SMIS_USER "$VPLEX_XIO1_SMIS_PASSWD" xtremio
            run storageprovider create $VPLEX_XIO2_SMIS_DEV_NAME $VPLEX_XIO2_SMIS_IP $VPLEX_XIO2_SMIS_PORT $VPLEX_XIO2_SMIS_USER "$VPLEX_XIO2_SMIS_PASSWD" xtremio
        else
            run smisprovider create $VPLEX_VNX1_SMIS_DEV_NAME $VPLEX_VNX1_SMIS_IP $VPLEX_VNX1_SMIS_PORT $VPLEX_SMIS_USER "$VPLEX_SMIS_PASSWD" $VPLEX_VNX1_SMIS_SSL
            run smisprovider create $VPLEX_VNX2_SMIS_DEV_NAME $VPLEX_VNX2_SMIS_IP $VPLEX_VNX2_SMIS_PORT $VPLEX_SMIS_USER "$VPLEX_SMIS_PASSWD" $VPLEX_VNX2_SMIS_SSL
            storageprovider show $VPLEX_DEV_NAME &> /dev/null && return $?
        fi

        run storageprovider create $VPLEX_DEV_NAME $VPLEX_IP 443 $VPLEX_USER "$VPLEX_PASSWD" vplex
        run storagedevice discover_all
        storagedevice list

        if [ $VPLEX_USE_XIO -eq 1 ]; then
            storageport list $VPLEX_XIO1_NATIVEGUID --v
            storageport list $VPLEX_XIO2_NATIVEGUID --v
        else
            storageport list $VPLEX_VNX1_NATIVEGUID --v
            storageport list $VPLEX_VMAX_NATIVEGUID --v
            storageport list $VPLEX_VNX2_NATIVEGUID --v
        fi

        storageport list $VPLEX_VMAX_NATIVEGUID --v
        storageport list $VPLEX_GUID --v

        # Setup the varrays. $NH contains VPLEX cluster-1 and $NH2 contains VPLEX cluster-2.
        run storageport update $VPLEX_GUID FC --group director-1-1-A --addvarrays $NH
        run storageport update $VPLEX_GUID FC --group director-1-1-B --addvarrays $NH
        run storageport update $VPLEX_GUID FC --group director-2-1-A --addvarrays $NH2
        run storageport update $VPLEX_GUID FC --group director-2-1-B --addvarrays $NH2
        storageport list $VPLEX_GUID --v

        # The arrays are assigned to individual varrays as well.
        if [ $VPLEX_USE_XIO -eq 1 ]; then
            run storageport update $VPLEX_XIO1_NATIVEGUID FC --addvarrays $NH
            run storageport update $VPLEX_XIO2_NATIVEGUID FC --addvarrays $NH
        else
            run storageport update $VPLEX_VNX1_NATIVEGUID FC --addvarrays $NH
            run storageport update $VPLEX_VNX2_NATIVEGUID FC --addvarrays $NH
        fi

        run storageport update $VPLEX_VMAX_NATIVEGUID FC --addvarrays $NH2
        vplex_cos_setup
        storageport list $VPLEX_GUID --v
    fi
}

#
# vplex tests
#
vplex_tests()
{
    if [ "${VPLEX_QUICK_PARAM}" = "quick" ]; then
        vplex_sim_variable_setup
    fi

    storageport list $VPLEX_GUID --v

    if [ "$AUTH" != 'ipv6' ] ; then
        echo "**** Executing VPLEX tests"

        hname=$(hostname)
        if [ $hname = "standalone" ]; then
            hname=$SHORTENED_HOST
        fi
        echo "hostname is $hname"

        localVolume1=$hname-${RANDOM}-VPlexLocal1
        localVolume2=$hname-${RANDOM}-VPlexLocal2
        localVolume3=$hname-${RANDOM}-VPlexLocal3
        localMirror1=$hname-${RANDOM}-VPlexLocalMirror1
        localMirror2=$hname-${RANDOM}-VPlexLocalMirror2
        sourceSideSuffix=-0
        localSnapshot=$hname-${RANDOM}-VPlexLocalSnap
        distVolume1=$hname-${RANDOM}-VPlexDist1
        distSrcLocalMirror1=$hname-${RANDOM}-srcLocalMirror1
        distSnapshot=$hname-${RANDOM}-VPlexDist1Snap
        distVolume2=$hname-${RANDOM}-VPlexDist2
        distSrcLocalMirror2=$hname-${RANDOM}-srcLocalMirror2
        host=$PROJECT.lss.emc.com
        hostLbl=$PROJECT
        PWWN1=10:00:00:E0:7E:00:00:0F
        WWNN1=20:00:00:E0:7E:00:00:0F
        PWWN2=10:00:00:90:FA:18:0E:99
        WWNN2=20:00:00:90:FA:18:0E:99

        echo "**** Creating VPLEX local volumes"
        run volume create $localVolume1 $PROJECT $NH cosvplexlocal $BLK_SIZE
        run volume create $localVolume2 $PROJECT $NH cosvplexlocal $BLK_SIZE
        run volume create $localVolume3 $PROJECT $NH cosvplexlocal $BLK_SIZE

        echo "**** Creating VPLEX local mirrors"
        run blockmirror attach  $PROJECT/$localVolume1 $localMirror1 1
        run blockmirror attach  $PROJECT/$localVolume2 $localMirror2 1

        echo "**** Creating VPLEX distributed volumes"
        run volume create $distVolume1 $PROJECT $NH cosvplexdist $BLK_SIZE
        run volume create $distVolume2 $PROJECT $NH cosvplexdist $BLK_SIZE

        echo "**** Creating local mirrors for the Distributed volumes on the source side"
        run blockmirror attach  $PROJECT/$distVolume1 $distSrcLocalMirror1 1
        run blockmirror attach  $PROJECT/$distVolume2 $distSrcLocalMirror2 1

        echo "**** Creating VPLEX local volume snapshot"
        run blocksnapshot create $PROJECT/$localVolume1 $localSnapshot
        blocksnapshot list $PROJECT/$localVolume1
        blocksnapshot show $PROJECT/$localVolume1/$localSnapshot

        echo "**** Creating VPLEX distributed volume snapshot"
        run blocksnapshot create $PROJECT/$distVolume1 $distSnapshot
        blocksnapshot list $PROJECT/$distVolume1
        blocksnapshot show $PROJECT/$distVolume1/$distSnapshot

        if [ "$VPLEX_INGEST_PARAM" = "all" -o "$VPLEX_INGEST_PARAM" = "unexported" ] ; then
            vplex_ingest_unexported
            # volume names would have changed after ingestion, based on backend vol with -0 suffix
            localVolume2="$localVolume2$sourceSideSuffix"
            localVolume3="$localVolume3$sourceSideSuffix"
        fi
        
        echo "**** Creating host"
        run hosts create $hostLbl $TENANT Windows $host --port 8111 --username user --password 'password' --osversion 1.0

        echo "**** Creating initiators"
        run initiator create $hostLbl FC $PWWN1 --node $WWNN1
        run initiator create $hostLbl FC $PWWN2 --node $WWNN2

        echo "**** Adding WWNs to Network"
        run transportzone add ${NH}/${CLUSTER1NET_NAME} $PWWN1
        run transportzone add ${NH}/${CLUSTER1NET_NAME} $PWWN2

        echo "**** Exporting VPLEX volumes to host in varray " $NH
        run export_group create $PROJECT $hname-1$host $NH --volspec "$PROJECT/$distVolume1" --inits "$hostLbl/$PWWN1","$hostLbl/$PWWN2"
        run export_group update $PROJECT/$hname-1$host --addVolspec $PROJECT/$distVolume2
        run export_group update $PROJECT/$hname-1$host --addVolspec $PROJECT/$localVolume1
        run export_group update $PROJECT/$hname-1$host --addVolspec $PROJECT/$localVolume2
        run export_group update $PROJECT/$hname-1$host --addVolspec $PROJECT/$localVolume3

        # echo "**** Exporting VPLEX volumes to host in varray " $NH2
        # run export_group create $PROJECT $hname-2$host $NH2 --volspec "$PROJECT/$distVolume1+1" --inits "$hostLbl/$PWWN2"
        echo "Exports for $distVolume1"
        volume exports $PROJECT/$distVolume1 --v
        echo "Exports for $distVolume2"
        volume exports $PROJECT/$distVolume2 --v
        echo "Exports for $localVolume1"
        volume exports $PROJECT/$localVolume1 --v
        echo "Exports for $localVolume2"
        volume exports $PROJECT/$localVolume2 --v

        if [ "$VPLEX_INGEST_PARAM" = "all" -o "$VPLEX_INGEST_PARAM" = "exported" ] ; then
            vplex_ingest_exported
        fi

        echo "**** Restoring VPLEX volume snapshots"
        # Must deactivate mirrors before restoring snapshots now.
        run blockmirror deactivate  $PROJECT/$localVolume1 $localMirror1$sourceSideSuffix
        run blocksnapshot restore $PROJECT/$localVolume1/$localSnapshot
        run blockmirror deactivate  $PROJECT/$distVolume1 $distSrcLocalMirror1$sourceSideSuffix
        if [ $VPLEX_USE_XIO -eq 0 ]; then
            run blocksnapshot restore $PROJECT/$distVolume1/$distSnapshot
        else
            echo "****** Skipping restore snapshot to distributed volume as VPLEX has problem with reattach mirror on XIO ******"
        fi

        echo "**** Deleting VPLEX volume snapshots"
        run blocksnapshot delete $PROJECT/$localVolume1/$localSnapshot
        run blocksnapshot delete $PROJECT/$distVolume1/$distSnapshot

        echo "**** Deleting VPLEX local mirror for local volume $localVolume2"
        if [ "$VPLEX_INGEST_PARAM" != "exported" ] ; then
            run blockmirror deactivate $PROJECT/$localVolume2 $localMirror2$sourceSideSuffix
        fi
        
        echo "**** Deleting local mirror for the VPLEX Distributed volume $distVolume2"
        run blockmirror deactivate $PROJECT/$distVolume2 $distSrcLocalMirror2$sourceSideSuffix

        echo "**** Deleting VPLEX exports"
        # this is likely a shared export group, so we need to turn validation_check flag off
        # so that the volumes added by sanity will be removed without a validation exception thrown.
        set_validation_check false
        run export_group delete $PROJECT/$hname-1$host
        set_validation_check true
        # run export_group delete $PROJECT/$hname-2$host

        echo "**** Deleting VPLEX volumes"
        run volume delete $PROJECT/$localVolume1 --wait
        run volume delete $PROJECT/$localVolume2 --wait
        run volume delete $PROJECT/$localVolume3 --wait

        run volume delete $PROJECT/$distVolume1 --wait
        run volume delete $PROJECT/$distVolume2 --wait
        
        if [ "${VPLEX_QUICK_PARAM}" = "quick" ]; then
            echo "**** Checking no remaining zones"
            fabricid=`echo ${CLUSTER1NET_NAME} | sed 's/VSAN_//'`
            run_noundo verifynozones ${fabricid} ${host}
        fi

        echo "**** Deleting Host"
        hosts delete $hostLbl

        echo "**** Completed VPLEX Tests"
    fi
}

# Run vplexexport test externally
vplexexport_setup() {
    vplex_setup_once
}

vplex_ingest_unexported() {

    # 
    # NOTE: some stuff is commented out here until the vplex-simulator can support it, works on hardware
    # 1) second backend volume of distributed vols gets a "connection refused" from the sim provider, so no distributed yet
    # 2) snapshots are not detected correctly on the backend volumes, so can't ingest volumes with snapshots yet
    # 3) the snapshot label detected doesn't seem to be correct either, but haven't been able to get that far to find out for sure
    # 

    echo "**** Running VPLEX Unexported Volume Ingestion Tests......."
    echo "****"
    echo "**** Inventory Deleting Everything..."
    echo "****"

    echo "**** saving off unexported Volume data for debug purposes to /tmp/vipr_unexported_greenfield_vols.txt"
    /opt/storageos/bin/dbcli list Volume > /tmp/vipr_unexported_greenfield_vols.txt

    #run_noundo volume delete $PROJECT/$localVolume1 --vipronly --force    
    run_noundo volume delete $PROJECT/$localVolume2 --vipronly --force
    run_noundo volume delete $PROJECT/$localVolume3 --vipronly --force
    #run_noundo volume delete $PROJECT/$distVolume1 --vipronly --force
    #run_noundo volume delete $PROJECT/$distVolume2 --vipronly --force

    # NOTE: snapshot ingestion testing disabled until the vplex-simulator can support it, works on hardware
    # VPLEX_LOCAL_SNAP_ARRAY_LABEL=`/opt/storageos/bin/dbutils list BlockSnapshot | grep deviceLabel | awk '{print $3}'`
    # echo "**** VPLEX_LOCAL_SNAP_ARRAY_LABEL is $VPLEX_LOCAL_SNAP_ARRAY_LABEL"
    # run_noundo blocksnapshot delete ${PROJECT}/${localVolume1}/${localSnapshot} --vipronly

    # VPLEX_DIST_SNAP_ARRAY_LABEL=`/opt/storageos/bin/dbutils list BlockSnapshot | grep deviceLabel | awk '{print $3}'`
    # echo "**** VPLEX_DIST_SNAP_ARRAY_LABEL is $VPLEX_DIST_SNAP_ARRAY_LABEL"
    # run_noundo blocksnapshot delete ${PROJECT}/${distVolume1}/${distSnapshot} --vipronly

    echo "**** Discovering Storage Systems for UnManagedVolumes - this may take a while..."
    run storagedevice discover_namespace $VPLEX_VNX1_NATIVEGUID 'UNMANAGED_VOLUMES'
    run storagedevice discover_namespace $VPLEX_VNX2_NATIVEGUID 'UNMANAGED_VOLUMES'
    run storagedevice discover_namespace $VPLEX_VMAX_NATIVEGUID 'UNMANAGED_VOLUMES'
    run storagedevice discover_namespace $VPLEX_GUID 'UNMANAGED_VOLUMES'

    echo "**** saving off unexported UnManagedVolume data for debug purposes to /tmp/vipr_unexported_unmanaged_vols.txt"
    /opt/storageos/bin/dbcli list UnManagedVolume > /tmp/vipr_unexported_unmanaged_vols.txt

    #echo "**** Ingesting $localVolume1 (has backend snapshot)"
    #run unmanagedvolume ingest_unexport ${NH} cosvplexlocal $PROJECT --volspec "$localVolume1"
    echo "**** Ingesting $localVolume2$sourceSideSuffix (has backend mirror)"
    run unmanagedvolume ingest_unexport ${NH} cosvplexlocal $PROJECT --volspec "$localVolume2$sourceSideSuffix"
    echo "**** Ingesting $localVolume3$sourceSideSuffix"
    run unmanagedvolume ingest_unexport ${NH} cosvplexlocal $PROJECT --volspec "$localVolume3$sourceSideSuffix"
    #echo "**** Ingesting $distVolume2"
    #run unmanagedvolume ingest_unexport ${NH} cosvplexdist $PROJECT --volspec "$distVolume2"

    #echo "**** Ingesting $localVolume1 with snap"
    #fail unmanagedvolume ingest_unexport ${NH} cosvplexlocal $PROJECT --volspec "$localVolume1"
    #echo "**** Ingesting $distVolume1 with snap"
    #fail unmanagedvolume ingest_unexport ${NH} cosvplexdist $PROJECT --volspec "$distVolume1"

    #echo "**** Ingesting snapshot $VPLEX_LOCAL_SNAP_ARRAY_LABEL of $localVolume1"
    #run unmanagedvolume ingest_unexport ${NH} cosvplexlocal $PROJECT --volspec "${VPLEX_LOCAL_SNAP_ARRAY_LABEL}"
    #echo "**** Ingesting snapshot $VPLEX_DIST_SNAP_ARRAY_LABEL of $distVolume1"
    #run unmanagedvolume ingest_unexport ${NH} cosvplexdist $PROJECT --volspec "${VPLEX_DIST_SNAP_ARRAY_LABEL}"

    echo "**** Completed VPLEX Unexported Ingestion Tests......."
}

vplex_ingest_exported() {

    # 
    # NOTE: some stuff is commented out here until the vplex-simulator can support it, works on hardware
    # 

    echo "**** Running VPLEX Exported Volume Ingestion Tests......."
    echo "****"
    echo "**** Inventory Deleting Everything..."
    echo "****"

    echo "**** saving off exported Volume data for debug purposes to /tmp/vipr_exported_greenfield_vols.txt"
    /opt/storageos/bin/dbcli list Volume > /tmp/vipr_exported_greenfield_vols.txt

    #run_noundo volume delete $PROJECT/$localVolume1 --vipronly --force
    run_noundo volume delete $PROJECT/$localVolume2 --vipronly --force
    run_noundo volume delete $PROJECT/$localVolume3 --vipronly --force
    #run_noundo volume delete $PROJECT/$distVolume1 --vipronly --force
    #run_noundo volume delete $PROJECT/$distVolume2 --vipronly --force

    echo "**** Discovering Storage Systems for UnManagedVolumes - this may take a while..."
    run storagedevice discover_namespace $VPLEX_VNX1_NATIVEGUID 'UNMANAGED_VOLUMES'
    run storagedevice discover_namespace $VPLEX_VNX2_NATIVEGUID 'UNMANAGED_VOLUMES'
    #run storagedevice discover_namespace $VPLEX_VMAX_NATIVEGUID 'UNMANAGED_VOLUMES'
    run storagedevice discover_namespace $VPLEX_GUID 'UNMANAGED_VOLUMES'

    echo "**** saving off exported UnManagedVolume data for debug purposes to /tmp/vipr_exported_unmanaged_vols.txt"
    /opt/storageos/bin/dbcli list UnManagedVolume > /tmp/vipr_exported_unmanaged_vols.txt
    echo "**** saving off UnManagedExportMask data for debug purposes to /tmp/vipr_exported_unmanaged_export_masks.txt"
    /opt/storageos/bin/dbcli list UnManagedExportMask > /tmp/vipr_exported_unmanaged_export_masks.txt

    #echo "exiting"
    #exit 0


    #echo "**** Ingesting Exported $localVolume2 (has backend snapshot)"
    #run unmanagedvolume ingest_export ${NH} cosvplexlocal $PROJECT --volspec "$localVolume1" --host $hostLbl
    echo "**** Ingesting Exported $localVolume2$sourceSideSuffix (has backend mirror)"
    run unmanagedvolume ingest_export ${NH} cosvplexlocal $PROJECT --volspec "$localVolume2" --host $hostLbl
    echo "**** Ingesting Exported $localVolume3$sourceSideSuffix"
    run unmanagedvolume ingest_export ${NH} cosvplexlocal $PROJECT --volspec "$localVolume3" --host $hostLbl
    #echo "**** Ingesting Exported $distVolume1"
    #run unmanagedvolume ingest_export ${NH} cosvplexdist $PROJECT --volspec "$distVolume1" --host $hostLbl
    #echo "**** Ingesting Exported $distVolume2"
    #run unmanagedvolume ingest_export ${NH} cosvplexdist $PROJECT --volspec "$distVolume2" --host $hostLbl

    # Remove the export group that was used for ingestion and re-export because they have different names
    run export_group update $PROJECT/$hname-1$host --addVolspec $PROJECT/$localVolume2
    run export_group update $PROJECT/$hname-1$host --addVolspec $PROJECT/$localVolume3
    run export_group delete $PROJECT/$host
    
    echo "**** Completed VPLEX Exported Volume Ingestion Tests......."
}

vplexexport_tests() {
    echo "************* Running export-tests/vplexexport.sh  *****************"
    run export-tests/vplexexport.sh test1
}

sanzoning_setup()
{
    sanzoning_setup_once
}

sanzoning_setup_once()
{
    vmaxblock_discovery
    vnxblock_discovery
}

# Sanzoning Tests
sanzoning_tests()
{
    echo "************* Running export-tests/sanzoning.sh  *****************"
#    run export-tests/sanzoning.sh addvolumezonecheck
#   run export-tests/sanzoning.sh cleanup
    run export-tests/sanzoning.sh sanzonereuse
}

vmaxblock_discovery()
{
    # do this only once
    smisprovider show $VMAX_SMIS_DEV &> /dev/null && return $?

    if [ $QUICK -eq 0 ]; then
       run smisprovider create $VMAX_SMIS_DEV $VMAX_SMIS_IP $VMAX_SMIS_PORT $SMIS_USER "$SMIS_PASSWD" $VMAX_SMIS_SSL
    fi

    run storagedevice discover_all --ignore_error
}

vnxblock_discovery()
{
    # Discover VNX block array
    smisprovider show $VNX_SMIS_DEV &> /dev/null && return $?

    if [ $QUICK -eq 0 ]; then
       run smisprovider create $VNX_SMIS_DEV $VNX_SMIS_IP $VNX_SMIS_PORT $SMIS_USER "$SMIS_PASSWD" $VNX_SMIS_SSL
    else
       run smisprovider create $VNX_SMIS_DEV $SIMULATOR_SMIS_IP 6088 $SMIS_USER "$SMIS_PASSWD" false
    fi

    run storagedevice discover_all --ignore_error
}

######################### Start of RecoverPoint ############################
#
# RecoverPoint can be run in a simulated and physical environment.
#

#############################
# RecoverPoint Vpool create #
#############################

# RP Targets
rp_targets()
{
    run cos create block rp_targets$1 $POOLS_AUTO_MATCH \
        --description 'RP-Targets' \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --multiVolumeConsistency \
        --neighborhoods $RECOVERPOINT_VARRAY1 $RECOVERPOINT_VARRAY2 $RECOVERPOINT_VARRAY3 $RECOVERPOINT_VARRAY4 \
        --max_snapshots 10

    if [ "${POOLS_AUTO_MATCH}" = "false" ]; then
        if [ "$STORAGE_ARRAY_1_NATIVEGUID" != "NONE" ]; then
            run cos update block rp_targets --storage $STORAGE_ARRAY_1_NATIVEGUID
        fi
        if [ "$STORAGE_ARRAY_2_NATIVEGUID" != "NONE" ]; then
            run cos update block rp_targets --storage $STORAGE_ARRAY_2_NATIVEGUID
        fi
        if [ "$STORAGE_ARRAY_3_NATIVEGUID" != "NONE" ]; then
            run cos update block rp_targets --storage $STORAGE_ARRAY_3_NATIVEGUID
        fi
        if [ "$STORAGE_ARRAY_4_NATIVEGUID" != "NONE" ]; then
            run cos update block rp_targets --storage $STORAGE_ARRAY_4_NATIVEGUID
        fi
    fi
}

# RP XIO Targets
rpxio_targets()
{
    run cos create block rpxio_targets$1 $POOLS_AUTO_MATCH \
        --description 'RP-XIO-Targets' \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --multiVolumeConsistency \
        --neighborhoods $RECOVERPOINT_VARRAY1 $RECOVERPOINT_VARRAY2 $RECOVERPOINT_VARRAY3 $RECOVERPOINT_VARRAY4 \
        --max_snapshots 10

    if [ "${POOLS_AUTO_MATCH}" = "false" ]; then
        if [ "$STORAGE_ARRAY_1_NATIVEGUID" != "NONE" ]; then
            run cos update block rpxio_targets --storage $STORAGE_ARRAY_1_NATIVEGUID
        fi
        if [ "$STORAGE_ARRAY_2_NATIVEGUID" != "NONE" ]; then
            run cos update block rpxio_targets --storage $STORAGE_ARRAY_2_NATIVEGUID
        fi
        if [ "$STORAGE_ARRAY_3_NATIVEGUID" != "NONE" ]; then
            run cos update block rpxio_targets --storage $STORAGE_ARRAY_3_NATIVEGUID
        fi
        if [ "$STORAGE_ARRAY_4_NATIVEGUID" != "NONE" ]; then
            run cos update block rpxio_targets --storage $STORAGE_ARRAY_4_NATIVEGUID
        fi
    fi
}

# All varrays for RP+VPLEX targets
rpvplex_targets()
{
    run cos create block rpvplex_targets$1 $POOLS_AUTO_MATCH \
        --description 'RP+VPLEX-Targets' \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --multiVolumeConsistency \
        --neighborhoods $RECOVERPOINT_VARRAY1 $RECOVERPOINT_VARRAY2 $RECOVERPOINT_VARRAY3 $RECOVERPOINT_VARRAY4 \
        --max_snapshots 10 \
        --highavailability vplex_local

    if [ "${POOLS_AUTO_MATCH}" = "false" ]; then
        if [ "$STORAGE_ARRAY_1_NATIVEGUID" != "NONE" ]; then
            run cos update block rpvplex_targets --storage $STORAGE_ARRAY_1_NATIVEGUID
        fi
        if [ "$STORAGE_ARRAY_2_NATIVEGUID" != "NONE" ]; then
            run cos update block rpvplex_targets --storage $STORAGE_ARRAY_2_NATIVEGUID
        fi
        if [ "$STORAGE_ARRAY_3_NATIVEGUID" != "NONE" ]; then
            run cos update block rpvplex_targets --storage $STORAGE_ARRAY_3_NATIVEGUID
        fi
        if [ "$STORAGE_ARRAY_4_NATIVEGUID" != "NONE" ]; then
            run cos update block rpvplex_targets --storage $STORAGE_ARRAY_4_NATIVEGUID
        fi
    fi

    # Migrate will always use auto pool match to force migrations
    run cos create block rpvplex_targets$1-migrate $POOLS_AUTO_MATCH_MIGRATE \
        --description 'RP+VPLEX-Targets-MIGRATE' \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --multiVolumeConsistency \
        --neighborhoods $RECOVERPOINT_VARRAY1 $RECOVERPOINT_VARRAY2 $RECOVERPOINT_VARRAY3 $RECOVERPOINT_VARRAY4 \
        --max_snapshots 10 \
        --highavailability vplex_local

    if [ "${POOLS_AUTO_MATCH_MIGRATE}" = "false" ]; then
        if [ "$STORAGE_ARRAY_1_NATIVEGUID" != "NONE" ]; then
            run cos update block rpvplex_targets-migrate --storage $STORAGE_ARRAY_1_NATIVEGUID
        fi
        if [ "$STORAGE_ARRAY_2_NATIVEGUID" != "NONE" ]; then
            run cos update block rpvplex_targets-migrate --storage $STORAGE_ARRAY_2_NATIVEGUID
        fi
        if [ "$STORAGE_ARRAY_3_NATIVEGUID" != "NONE" ]; then
            run cos update block rpvplex_targets-migrate --storage $STORAGE_ARRAY_3_NATIVEGUID
        fi
        if [ "$STORAGE_ARRAY_4_NATIVEGUID" != "NONE" ]; then
            run cos update block rpvplex_targets-migrate --storage $STORAGE_ARRAY_4_NATIVEGUID
        fi
    fi
}

# All varrays for HA
rpvplex_ha()
{
    run cos create block rpvplex_ha$1 $POOLS_AUTO_MATCH \
        --description 'HA' \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --multiVolumeConsistency \
        --neighborhoods $RECOVERPOINT_VARRAY1 $RECOVERPOINT_VARRAY2 $RECOVERPOINT_VARRAY3 $RECOVERPOINT_VARRAY4 \
        --max_snapshots 10 \
        --highavailability vplex_local

    if [ "${POOLS_AUTO_MATCH}" = "false" ]; then
        if [ "$STORAGE_ARRAY_1_NATIVEGUID" != "NONE" ]; then
                run cos update block rpvplex_ha --storage $STORAGE_ARRAY_1_NATIVEGUID
        fi
        if [ "$STORAGE_ARRAY_2_NATIVEGUID" != "NONE" ]; then
                run cos update block rpvplex_ha --storage $STORAGE_ARRAY_2_NATIVEGUID
        fi
        if [ "$STORAGE_ARRAY_3_NATIVEGUID" != "NONE" ]; then
                run cos update block rpvplex_ha --storage $STORAGE_ARRAY_3_NATIVEGUID
        fi
        if [ "$STORAGE_ARRAY_4_NATIVEGUID" != "NONE" ]; then
                run cos update block rpvplex_ha --storage $STORAGE_ARRAY_4_NATIVEGUID
        fi
    fi

    run cos create block rpvplex_ha$1-migrate $POOLS_AUTO_MATCH_MIGRATE \
        --description 'HA-MIGRATE' \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --multiVolumeConsistency \
        --neighborhoods $RECOVERPOINT_VARRAY1 $RECOVERPOINT_VARRAY2 $RECOVERPOINT_VARRAY3 $RECOVERPOINT_VARRAY4 \
        --max_snapshots 10 \
        --highavailability vplex_local

    if [ "${POOLS_AUTO_MATCH_MIGRATE}" = "false" ]; then
        if [ "$STORAGE_ARRAY_1_NATIVEGUID" != "NONE" ]; then
            run cos update block rpvplex_ha-migrate --storage $STORAGE_ARRAY_1_NATIVEGUID
        fi
        if [ "$STORAGE_ARRAY_2_NATIVEGUID" != "NONE" ]; then
            run cos update block rpvplex_ha-migrate --storage $STORAGE_ARRAY_2_NATIVEGUID
        fi
        if [ "$STORAGE_ARRAY_3_NATIVEGUID" != "NONE" ]; then
            run cos update block rpvplex_ha-migrate --storage $STORAGE_ARRAY_3_NATIVEGUID
        fi
        if [ "$STORAGE_ARRAY_4_NATIVEGUID" != "NONE" ]; then
            run cos update block rpvplex_ha-migrate --storage $STORAGE_ARRAY_4_NATIVEGUID
        fi
    fi
}

# RP CDP
rp_cdp()
{
    run cos create block rp_cdp$1 $POOLS_AUTO_MATCH \
        --description 'RP-Source-CDP'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --expandable true \
        --protectionCoS $RECOVERPOINT_VARRAY1':rp_targets'$1':0.25x:'$RECOVERPOINT_VARRAY1':rp_targets'$1 \
        --max_snapshots 10 \
        --rp_copy_mode ASYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --sourceJournalSize 0.25x

    run cos create block rp_cdp$1-sync $POOLS_AUTO_MATCH \
        --description 'RP-Source-CDP-with-Sync'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --expandable true \
        --protectionCoS $RECOVERPOINT_VARRAY1':rp_targets'$1':0.25x:'$RECOVERPOINT_VARRAY1':rp_targets'$1 \
        --max_snapshots 10 \
        --rp_copy_mode SYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --sourceJournalSize 0.25x

    if [ "${POOLS_AUTO_MATCH}" = "false" ]; then
        if [ "$STORAGE_ARRAY_1_NATIVEGUID" != "NONE" ]; then
            run cos update block rp_cdp --storage $STORAGE_ARRAY_1_NATIVEGUID
            run cos update block rp_cdp-sync --storage $STORAGE_ARRAY_1_NATIVEGUID
        fi
    fi
}

# RP XIO CDP
rpxio_cdp()
{
    run cos create block rpxio_cdp$1 $POOLS_AUTO_MATCH \
        --description 'RP-XIO-Source-CDP'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --protectionCoS $RECOVERPOINT_VARRAY1':rpxio_targets'$1':0.25x:'$RECOVERPOINT_VARRAY1':rpxio_targets'$1 \
        --max_snapshots 10 \
        --rp_copy_mode ASYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --sourceJournalSize 0.25x

    run cos create block rpxio_cdp$1-sync $POOLS_AUTO_MATCH \
        --description 'RP-XIO-Source-CDP-with-Sync'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --protectionCoS $RECOVERPOINT_VARRAY1':rpxio_targets'$1':0.25x:'$RECOVERPOINT_VARRAY1':rpxio_targets'$1 \
        --max_snapshots 10 \
        --rp_copy_mode SYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --sourceJournalSize 0.25x

    if [ "${POOLS_AUTO_MATCH}" = "false" ]; then
        if [ "$STORAGE_ARRAY_1_NATIVEGUID" != "NONE" ]; then
            run cos update block rpxio_cdp --storage $STORAGE_ARRAY_1_NATIVEGUID
            run cos update block rpxio_cdp-sync --storage $STORAGE_ARRAY_1_NATIVEGUID
        fi
    fi
}

# RP CRR
rp_crr()
{
    run cos create block rp_crr$1 $POOLS_AUTO_MATCH \
        --description 'RP-Source-CRR'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --expandable true \
        --protectionCoS $RECOVERPOINT_VARRAY3':rp_targets'$1':min' \
        --rp_copy_mode ASYNCHRONOUS \
        --max_snapshots 10 \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES

    run cos create block rp_crr$1-sync $POOLS_AUTO_MATCH \
        --description 'RP-Source-CRR'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --expandable true \
        --protectionCoS $RECOVERPOINT_VARRAY3':rp_targets'$1':min' \
        --rp_copy_mode SYNCHRONOUS \
        --max_snapshots 10 \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES

    if [ "${POOLS_AUTO_MATCH}" = "false" ]; then
        if [ "$STORAGE_ARRAY_1_NATIVEGUID" != "NONE" ]; then
            run cos update block rp_crr --storage $STORAGE_ARRAY_1_NATIVEGUID
            run cos update block rp_crr-sync --storage $STORAGE_ARRAY_1_NATIVEGUID
        fi
    fi
}

# RP CLR
rp_clr()
{
    run cos create block rp_clr$1 $POOLS_AUTO_MATCH \
        --description 'RP-Source-CLR'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --expandable true \
        --protectionCoS $RECOVERPOINT_VARRAY1':rp_targets'$1':min',$RECOVERPOINT_VARRAY3':rp_targets'$1':min' \
        --max_snapshots 10 \
        --rp_copy_mode ASYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES

    run cos create block rp_clr$1-sync $POOLS_AUTO_MATCH \
        --description 'RP-Source-CLR'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --expandable true \
        --protectionCoS $RECOVERPOINT_VARRAY1':rp_targets'$1':min',$RECOVERPOINT_VARRAY3':rp_targets'$1':min' \
        --max_snapshots 10 \
        --rp_copy_mode SYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES

    if [ "${POOLS_AUTO_MATCH}" = "false" ]; then
        if [ "$STORAGE_ARRAY_1_NATIVEGUID" != "NONE" ]; then
            run cos update block rp_clr --storage $STORAGE_ARRAY_1_NATIVEGUID
            run cos update block rp_clr-sync --storage $STORAGE_ARRAY_1_NATIVEGUID
        fi
    fi
}

# RP+VPLEX CDP
rpvplex_cdp()
{
    run cos create block rpvplex_cdp$1 $POOLS_AUTO_MATCH \
        --description 'RP+VPLEX-Source-CDP'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --expandable true \
        --protectionCoS $RECOVERPOINT_VARRAY1':rpvplex_targets'$1':0.25x:'$RECOVERPOINT_VARRAY1':rpvplex_targets'$1 \
        --max_snapshots 10 \
        --rp_copy_mode ASYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --journalVpool rpvplex_targets \
        --journalVarray $RECOVERPOINT_VARRAY1 \
        --highavailability vplex_local \
        --sourceJournalSize 0.25x

    run cos create block rpvplex_cdp$1-sync $POOLS_AUTO_MATCH \
        --description 'RP+VPLEX-Source-CDP'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --expandable true \
        --protectionCoS $RECOVERPOINT_VARRAY1':rpvplex_targets'$1':0.25x:'$RECOVERPOINT_VARRAY1':rpvplex_targets'$1 \
        --max_snapshots 10 \
        --rp_copy_mode SYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --journalVpool rpvplex_targets \
        --journalVarray $RECOVERPOINT_VARRAY1 \
        --highavailability vplex_local \
        --sourceJournalSize 0.25x

    if [ "${POOLS_AUTO_MATCH}" = "false" ]; then
        if [ "$STORAGE_ARRAY_1_NATIVEGUID" != "NONE" ]; then
            run cos update block rpvplex_cdp --storage $STORAGE_ARRAY_1_NATIVEGUID
            run cos update block rpvplex_cdp-sync --storage $STORAGE_ARRAY_1_NATIVEGUID
        fi
    fi

    run cos create block rpvplex_cdp$1-migrate $POOLS_AUTO_MATCH_MIGRATE \
        --description 'RP+VPLEX-Source-CDP-MIGRATE'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --expandable true \
        --protectionCoS $RECOVERPOINT_VARRAY1':rpvplex_targets'$1'-migrate:0.25x:'$RECOVERPOINT_VARRAY1':rpvplex_targets'$1'-migrate' \
        --max_snapshots 10 \
        --rp_copy_mode ASYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --journalVpool rpvplex_targets-migrate \
        --journalVarray $RECOVERPOINT_VARRAY1 \
        --highavailability vplex_local \
        --sourceJournalSize 0.25x

    if [ "${POOLS_AUTO_MATCH_MIGRATE}" = "false" ]; then
        if [ "$STORAGE_ARRAY_1_NATIVEGUID" != "NONE" ]; then
            run cos update block rpvplex_cdp-migrate --storage $STORAGE_ARRAY_1_NATIVEGUID
        fi
    fi
}

# RP+VPLEX CRR
rpvplex_crr()
{
    run cos create block rpvplex_crr$1 $POOLS_AUTO_MATCH \
        --description 'RP+VPLEX-Source-CRR'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --expandable true \
        --protectionCoS $RECOVERPOINT_VARRAY3':rp_targets'$1':min' \
        --rp_copy_mode ASYNCHRONOUS \
        --max_snapshots 10 \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --journalVpool rpvplex_targets \
        --highavailability vplex_distributed \
        --haNeighborhood $RECOVERPOINT_VARRAY2 \
        --haCos rpvplex_ha

    run cos create block rpvplex_crr$1-sync $POOLS_AUTO_MATCH \
        --description 'RP+VPLEX-Source-CRR'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --expandable true \
        --protectionCoS $RECOVERPOINT_VARRAY3':rp_targets'$1':min' \
        --max_snapshots 10 \
        --rp_copy_mode SYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --journalVpool rpvplex_targets \
        --highavailability vplex_distributed \
        --haNeighborhood $RECOVERPOINT_VARRAY2 \
        --haCos rpvplex_ha

    if [ "${POOLS_AUTO_MATCH}" = "false" ]; then
        if [ "$STORAGE_ARRAY_1_NATIVEGUID" != "NONE" ]; then
            run cos update block rpvplex_crr --storage $STORAGE_ARRAY_1_NATIVEGUID
            run cos update block rpvplex_crr-sync --storage $STORAGE_ARRAY_1_NATIVEGUID
        fi
    fi

    run cos create block rpvplex_crr$1-migrate $POOLS_AUTO_MATCH_MIGRATE \
        --description 'RP+VPLEX-Source-CRR-MIGRATE'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --expandable true \
        --protectionCoS $RECOVERPOINT_VARRAY3':rp_targets'$1':min' \
        --rp_copy_mode ASYNCHRONOUS \
        --max_snapshots 10 \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --journalVpool rpvplex_targets-migrate \
        --highavailability vplex_distributed \
        --haNeighborhood $RECOVERPOINT_VARRAY2 \
        --haCos rpvplex_ha-migrate

    if [ "${POOLS_AUTO_MATCH_MIGRATE}" = "false" ]; then
        if [ "$STORAGE_ARRAY_1_NATIVEGUID" != "NONE" ]; then
            run cos update block rpvplex_crr-migrate --storage $STORAGE_ARRAY_1_NATIVEGUID
        fi
    fi
}

# RP+VPLEX CLR
rpvplex_clr()
{
    run cos create block rpvplex_clr$1 $POOLS_AUTO_MATCH \
        --description 'RP+VPLEX-Source-CLR'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --expandable true \
        --protectionCoS $RECOVERPOINT_VARRAY1':rpvplex_targets'$1':min',$RECOVERPOINT_VARRAY3':rp_targets'$1':min' \
        --max_snapshots 10 \
        --rp_copy_mode ASYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --highavailability vplex_distributed \
        --haNeighborhood $RECOVERPOINT_VARRAY2 \
        --journalVpool rpvplex_targets \
        --journalVarray $RECOVERPOINT_VARRAY1 \
        --haCos rpvplex_ha

    run cos create block rpvplex_clr$1-sync $POOLS_AUTO_MATCH \
        --description 'RP+VPLEX-Source-CLR'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --expandable true \
        --protectionCoS $RECOVERPOINT_VARRAY1':rpvplex_targets'$1':min',$RECOVERPOINT_VARRAY3':rp_targets'$1':min' \
        --max_snapshots 10 \
        --rp_copy_mode SYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --highavailability vplex_distributed \
        --haNeighborhood $RECOVERPOINT_VARRAY2 \
        --journalVpool rpvplex_targets \
        --journalVarray $RECOVERPOINT_VARRAY1 \
        --haCos rpvplex_ha

    if [ "${POOLS_AUTO_MATCH}" = "false" ]; then
        if [ "$STORAGE_ARRAY_1_NATIVEGUID" != "NONE" ]; then
            run cos update block rpvplex_clr --storage $STORAGE_ARRAY_1_NATIVEGUID
            run cos update block rpvplex_clr-sync --storage $STORAGE_ARRAY_1_NATIVEGUID
        fi
    fi

    run cos create block rpvplex_clr$1-migrate $POOLS_AUTO_MATCH_MIGRATE \
        --description 'RP+VPLEX-Source-CLR-MIGRATE'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --expandable true \
        --protectionCoS $RECOVERPOINT_VARRAY1':rpvplex_targets'$1'-migrate:min',$RECOVERPOINT_VARRAY3':rp_targets'$1':min' \
        --max_snapshots 10 \
        --rp_copy_mode ASYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --highavailability vplex_distributed \
        --haNeighborhood $RECOVERPOINT_VARRAY2 \
        --journalVpool rpvplex_targets-migrate \
        --journalVarray $RECOVERPOINT_VARRAY1 \
        --haCos rpvplex_ha-migrate

    if [ "${POOLS_AUTO_MATCH_MIGRATE}" = "false" ]; then
        if [ "$STORAGE_ARRAY_1_NATIVEGUID" != "NONE" ]; then
            run cos update block rpvplex_clr-migrate --storage $STORAGE_ARRAY_1_NATIVEGUID
        fi
    fi
}

# MetroPoint CDP
rpmp_cdp()
{
    run cos create block rpmp_cdp$1 $POOLS_AUTO_MATCH \
        --description 'MetroPoint-Source-CDP'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --expandable true \
        --protectionCoS $RECOVERPOINT_VARRAY1':rpvplex_targets'$1':0.25x:'$RECOVERPOINT_VARRAY1':rpvplex_targets'$1,$RECOVERPOINT_VARRAY2':rpvplex_targets'$1':0.25x:'$RECOVERPOINT_VARRAY2':rpvplex_targets'$1 \
        --max_snapshots 10 \
        --rp_copy_mode ASYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --highavailability vplex_distributed \
        --haNeighborhood $RECOVERPOINT_VARRAY2 \
        --haCos rpvplex_ha \
        --metropoint true \
        --activeProtectionAtHASite false \
        --sourceJournalSize 0.25x

    run cos create block rpmp_cdp$1-sync $POOLS_AUTO_MATCH \
        --description 'MetroPoint-Source-CDP'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --expandable true \
        --protectionCoS $RECOVERPOINT_VARRAY1':rpvplex_targets'$1':0.25x:'$RECOVERPOINT_VARRAY1':rpvplex_targets'$1,$RECOVERPOINT_VARRAY2':rpvplex_targets'$1':0.25x:'$RECOVERPOINT_VARRAY2':rpvplex_targets'$1 \
        --max_snapshots 10 \
        --rp_copy_mode SYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --highavailability vplex_distributed \
        --haNeighborhood $RECOVERPOINT_VARRAY2 \
        --haCos rpvplex_ha \
        --metropoint true \
        --activeProtectionAtHASite false \
        --sourceJournalSize 0.25x

    if [ "${POOLS_AUTO_MATCH}" = "false" ]; then
        if [ "$STORAGE_ARRAY_1_NATIVEGUID" != "NONE" ]; then
            run cos update block rpmp_cdp --storage $STORAGE_ARRAY_1_NATIVEGUID
            run cos update block rpmp_cdp-sync --storage $STORAGE_ARRAY_1_NATIVEGUID
        fi
    fi

    run cos create block rpmp_cdp$1-migrate $POOLS_AUTO_MATCH_MIGRATE \
        --description 'MetroPoint-Source-CDP-MIGRATE'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --expandable true \
        --protectionCoS $RECOVERPOINT_VARRAY1':rpvplex_targets'$1'-migrate:0.25x:'$RECOVERPOINT_VARRAY1':rpvplex_targets'$1'-migrate',$RECOVERPOINT_VARRAY2':rpvplex_targets'$1'-migrate:0.25x:'$RECOVERPOINT_VARRAY2':rpvplex_targets'$1'-migrate' \
        --max_snapshots 10 \
        --rp_copy_mode ASYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --highavailability vplex_distributed \
        --haNeighborhood $RECOVERPOINT_VARRAY2 \
        --haCos rpvplex_ha-migrate \
        --metropoint true \
        --activeProtectionAtHASite false \
        --sourceJournalSize 0.25x

    if [ "${POOLS_AUTO_MATCH_MIGRATE}" = "false" ]; then
        if [ "$STORAGE_ARRAY_1_NATIVEGUID" != "NONE" ]; then
            run cos update block rpmp_cdp-migrate --storage $STORAGE_ARRAY_1_NATIVEGUID
        fi
    fi
}

# MetroPoint CRR
rpmp_crr()
{
    run cos create block rpmp_crr$1 $POOLS_AUTO_MATCH \
        --description 'MetroPoint-Source-CRR'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --expandable true \
        --protectionCoS $RECOVERPOINT_VARRAY3':rp_targets'$1':min' \
        --max_snapshots 10 \
        --rp_copy_mode ASYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --highavailability vplex_distributed \
        --haNeighborhood $RECOVERPOINT_VARRAY2 \
        --haCos rpvplex_ha \
        --metropoint true \
        --activeProtectionAtHASite false

    run cos create block rpmp_crr$1-sync $POOLS_AUTO_MATCH \
        --description 'MetroPoint-Source-CRR'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --expandable true \
        --protectionCoS $RECOVERPOINT_VARRAY3':rp_targets'$1':min' \
        --max_snapshots 10 \
        --rp_copy_mode SYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --highavailability vplex_distributed \
        --haNeighborhood $RECOVERPOINT_VARRAY2 \
        --haCos rpvplex_ha \
        --metropoint true \
        --activeProtectionAtHASite false

    # Post RP+VPLEX CRR Migrate we will need a vpool to test Upgrade to MP
    run cos create block rpmp_crr$1-upgradeMP $POOLS_AUTO_MATCH \
        --description 'MetroPoint-Source-CRR-UPGRADE-MP'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --expandable true \
        --protectionCoS $RECOVERPOINT_VARRAY3':rp_targets'$1':min' \
        --max_snapshots 10 \
        --rp_copy_mode ASYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --highavailability vplex_distributed \
        --haNeighborhood $RECOVERPOINT_VARRAY2 \
        --haCos rpvplex_ha-migrate \
        --metropoint true \
        --activeProtectionAtHASite false

    if [ "${POOLS_AUTO_MATCH}" = "false" ]; then
        if [ "$STORAGE_ARRAY_1_NATIVEGUID" != "NONE" ]; then
            run cos update block rpmp_crr --storage $STORAGE_ARRAY_1_NATIVEGUID
            run cos update block rpmp_crr-sync --storage $STORAGE_ARRAY_1_NATIVEGUID
            run cos update block rpmp_crr-upgradeMP --storage $STORAGE_ARRAY_1_NATIVEGUID
        fi
    fi

    run cos create block rpmp_crr$1-migrate $POOLS_AUTO_MATCH_MIGRATE \
        --description 'MetroPoint-Source-CRR-MIGRATE'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --expandable true \
        --protectionCoS $RECOVERPOINT_VARRAY3':rp_targets'$1':min' \
        --max_snapshots 10 \
        --rp_copy_mode ASYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --highavailability vplex_distributed \
        --haNeighborhood $RECOVERPOINT_VARRAY2 \
        --haCos rpvplex_ha \
        --metropoint true \
        --activeProtectionAtHASite false

    if [ "${POOLS_AUTO_MATCH_MIGRATE}" = "false" ]; then
        if [ "$STORAGE_ARRAY_1_NATIVEGUID" != "NONE" ]; then
            run cos update block rpmp_crr-migrate --storage $STORAGE_ARRAY_1_NATIVEGUID
        fi
    fi 
}

# MetroPoint CLR
rpmp_clr()
{
    run cos create block rpmp_clr$1 $POOLS_AUTO_MATCH \
        --description 'MetroPoint-CLR'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --expandable true \
        --protectionCoS $RECOVERPOINT_VARRAY1':rp_targets'$1':min',$RECOVERPOINT_VARRAY2':rp_targets'$1':min',$RECOVERPOINT_VARRAY3':rp_targets'$1':min' \
        --max_snapshots 10 \
        --rp_copy_mode ASYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --highavailability vplex_distributed \
        --haNeighborhood $RECOVERPOINT_VARRAY2 \
        --haCos rpvplex_ha \
        --metropoint true \
        --activeProtectionAtHASite false

    run cos create block rpmp_clr$1-sync $POOLS_AUTO_MATCH \
        --description 'MetroPoint-CLR'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --expandable true \
        --protectionCoS $RECOVERPOINT_VARRAY1':rp_targets'$1':min',$RECOVERPOINT_VARRAY2':rp_targets'$1':min',$RECOVERPOINT_VARRAY3':rp_targets'$1':min' \
        --max_snapshots 10 \
        --rp_copy_mode SYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --highavailability vplex_distributed \
        --haNeighborhood $RECOVERPOINT_VARRAY2 \
        --haCos rpvplex_ha \
        --metropoint true \
        --activeProtectionAtHASite false

    if [ "${POOLS_AUTO_MATCH}" = "false" ]; then
        if [ "$STORAGE_ARRAY_1_NATIVEGUID" != "NONE" ]; then
            run cos update block rpmp_clr --storage $STORAGE_ARRAY_1_NATIVEGUID
            run cos update block rpmp_clr-sync --storage $STORAGE_ARRAY_1_NATIVEGUID
        fi
    fi

    run cos create block rpmp_clr$1-migrate $POOLS_AUTO_MATCH_MIGRATE \
        --description 'MetroPoint-CLR-MIGRATE'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --expandable true \
        --protectionCoS $RECOVERPOINT_VARRAY1':rp_targets'$1':min',$RECOVERPOINT_VARRAY2':rp_targets'$1':min',$RECOVERPOINT_VARRAY3':rp_targets'$1':min' \
        --max_snapshots 10 \
        --rp_copy_mode ASYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --highavailability vplex_distributed \
        --haNeighborhood $RECOVERPOINT_VARRAY2 \
        --haCos rpvplex_ha \
        --metropoint true \
        --activeProtectionAtHASite false

    if [ "${POOLS_AUTO_MATCH_MIGRATE}" = "false" ]; then
        if [ "$STORAGE_ARRAY_1_NATIVEGUID" != "NONE" ]; then
            run cos update block rpmp_clr-migrate --storage $STORAGE_ARRAY_1_NATIVEGUID
        fi
    fi
}

# RP - NO PROTECTION
rp_noprotection()
{
    run cos create block rp_noprotection$1 $POOLS_AUTO_MATCH \
        --description 'RP---NO-PROTECTION'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --max_snapshots 10 \
        --expandable true \
        --multiVolumeConsistency

    if [ "${POOLS_AUTO_MATCH}" = "false" ]; then
        if [ "$STORAGE_ARRAY_1_NATIVEGUID" != "NONE" ]; then
            run cos update block rp_noprotection --storage $STORAGE_ARRAY_1_NATIVEGUID
        fi
    fi
}

# RP+VPLEX LOCAL - NO PROTECTION
rpvplexlocal_noprotection()
{
    run cos create block rpvplexlocal_noprotection$1 $POOLS_AUTO_MATCH \
        --description 'RP+VPLEX-LOCAL---NO-PROTECTION'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --max_snapshots 10 \
        --expandable true \
        --multiVolumeConsistency \
        --highavailability vplex_local

    if [ "${POOLS_AUTO_MATCH}" = "false" ]; then
        if [ "$STORAGE_ARRAY_1_NATIVEGUID" != "NONE" ]; then
            run cos update block rpvplexlocal_noprotection --storage $STORAGE_ARRAY_1_NATIVEGUID
        fi
    fi

    run cos create block rpvplexlocal_noprotection$1-migrate $POOLS_AUTO_MATCH_MIGRATE \
        --description 'RP+VPLEX-LOCAL---NO-PROTECTION-MIGRATE'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --max_snapshots 10 \
        --expandable true \
        --multiVolumeConsistency \
        --highavailability vplex_local

    if [ "${POOLS_AUTO_MATCH_MIGRATE}" = "false" ]; then
        if [ "$STORAGE_ARRAY_1_NATIVEGUID" != "NONE" ]; then
            run cos update block rpvplexlocal_noprotection-migrate --storage $STORAGE_ARRAY_1_NATIVEGUID
        fi
    fi
}


# RP+VPLEX DIST - NO PROTECTION
rpvplexdist_noprotection()
{
    run cos create block rpvplexdist_noprotection$1 $POOLS_AUTO_MATCH \
        --description 'RP+VPLEX-DIST---NO-PROTECTION'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --max_snapshots 10 \
        --expandable true \
        --highavailability vplex_distributed \
        --haNeighborhood $RECOVERPOINT_VARRAY2 \
        --haCos rpvplex_ha

    if [ "${POOLS_AUTO_MATCH}" = "false" ]; then
        if [ "$STORAGE_ARRAY_1_NATIVEGUID" != "NONE" ]; then
            run cos update block rpvplexdist_noprotection --storage $STORAGE_ARRAY_1_NATIVEGUID
        fi
    fi

    run cos create block rpvplexdist_noprotection$1-migrate $POOLS_AUTO_MATCH_MIGRATE \
        --description 'RP+VPLEX-DIST---NO-PROTECTION-MIGRATE'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --max_snapshots 10 \
        --expandable true \
        --highavailability vplex_distributed \
        --haNeighborhood $RECOVERPOINT_VARRAY2 \
        --haCos rpvplex_ha-migrate

    if [ "${POOLS_AUTO_MATCH_MIGRATE}" = "false" ]; then
        if [ "$STORAGE_ARRAY_1_NATIVEGUID" != "NONE" ]; then
            run cos update block rpvplexdist_noprotection-migrate --storage $STORAGE_ARRAY_1_NATIVEGUID
        fi
    fi
}

#
# End RP vpool create
#

#
# Start RP setup functions
#

rp_vpool_setup()
{
    rp_targets
    rp_noprotection

    if [ "${RPVPLEX_TESTS}" = "1" -o "${RPMP_TESTS}" = "1" ]; then
        rpvplex_targets
        rpvplex_ha
        rpvplexlocal_noprotection
        rpvplexdist_noprotection
    fi

    if [ "${RP_TESTS}" = "1" ]; then
        if [ "${RP_CDP}" = "1" ]; then
            rp_cdp
        fi
        if [ "${RP_CRR}" = "1" ]; then
            rp_crr
        fi
        if [ "${RP_CLR}" = "1" ]; then
            rp_clr
        fi
    fi

    if [ "${RPVPLEX_TESTS}" = "1" ]; then
        if [ "${RP_CDP}" = "1" ]; then
            rpvplex_cdp
        fi
        if [ "${RP_CRR}" = "1" ]; then
            rpvplex_crr
        fi
        if [ "${RP_CLR}" = "1" ]; then
            rpvplex_clr
        fi
    fi

    if [ "${RPMP_TESTS}" = "1" ]; then
        if [ "${RP_CDP}" = "1" ]; then
            rpmp_cdp
        fi
        if [ "${RP_CRR}" = "1" ]; then
            rpmp_crr
        fi
        if [ "${RP_CLR}" = "1" ]; then
            rpmp_clr
        fi
    fi

    if [ "${RPXIO_TESTS}" = "1" ]; then
        rpxio_targets
        rp_noprotection
        if [ "${RP_CDP}" = "1" ]; then
            rpxio_cdp
        fi
    fi
}

rp_network_setup()
{
    if [ $DISCOVER_SAN -eq 1 ]; then
        # Do once
        nsys=`networksystem list | wc -l`
        [ "$nsys" -gt 0 ] && return;
        #Discover the Brocade SAN switches.
        secho "RecoverPoint network setup"

        run networksystem create $RECOVERPOINT_BROCADE_NETWORK brocade \
            --smisip $RECOVERPOINT_BROCADE_NETWORK_PROVIDER \
            --smisport $RECOVERPOINT_BROCADE_NETWORK_PROVIDER_PORT \
            --smisuser $RECOVERPOINT_BROCADE_NETWORK_PROVIDER_USERNAME \
            --smispw $RECOVERPOINT_BROCADE_NETWORK_PROVIDER_PASSWORD \
            --smisssl $RECOVERPOINT_BROCADE_NETWORK_PROVIDER_SSL
        
        run networksystem create $RECOVERPOINT_BROCADE_NETWORK2 brocade \
            --smisip $RECOVERPOINT_BROCADE_NETWORK2_PROVIDER \
            --smisport $RECOVERPOINT_BROCADE_NETWORK2_PROVIDER_PORT \
            --smisuser $RECOVERPOINT_BROCADE_NETWORK2_PROVIDER_USERNAME \
            --smispw $RECOVERPOINT_BROCADE_NETWORK2_PROVIDER_PASSWORD \
            --smisssl $RECOVERPOINT_BROCADE_NETWORK2_PROVIDER_SSL
            
        secho "Sleeping 30 seconds..."
        sleep 30
    fi
}

rp_varray_setup()
{
    secho 'RecoverPoint varray setup'

    # Create Virtual Array
    run neighborhood create $RECOVERPOINT_VARRAY1
    run neighborhood allow $RECOVERPOINT_VARRAY1 $TENANT
    run neighborhood allow $RECOVERPOINT_VARRAY1 `tenant root`

    run neighborhood create $RECOVERPOINT_VARRAY2
    run neighborhood allow $RECOVERPOINT_VARRAY2 $TENANT
    run neighborhood allow $RECOVERPOINT_VARRAY2 `tenant root`

    run neighborhood create $RECOVERPOINT_VARRAY3
    run neighborhood allow $RECOVERPOINT_VARRAY3 $TENANT
    run neighborhood allow $RECOVERPOINT_VARRAY3 `tenant root`

    run neighborhood create $RECOVERPOINT_VARRAY4
    run neighborhood allow $RECOVERPOINT_VARRAY4 $TENANT
    run neighborhood allow $RECOVERPOINT_VARRAY4 `tenant root`
}

rp_storage_setup()
{
    secho 'RecoverPoint storage setup'

    run syssvc $CONFIG_FILE $BOURNE_IPADDR set_prop controller_max_thin_pool_subscription_percentage 600

    if [ "${RP_TESTS}" = "1" -o "${RPVPLEX_TESTS}" = "1" -o "${RPMP_TESTS}" = "1" ]; then
        for var in A B C D
        do
           provider='RECOVERPOINT_SMIS_PROVIDER_'$var
           provider=${!provider}

           provider_ip='RECOVERPOINT_SMIS_PROVIDER_'$var'_IP'
           provider_ip=${!provider_ip}

           array_guid='RECOVERPOINT_STORAGE_ARRAY_'$var'_GUID'
           array_guid=${!array_guid}

           smis_port='RECOVERPOINT_STORAGE_ARRAY_'$var'_SMIS_PORT'          
           smis_port=${!smis_port}

           smis_ssl='RECOVERPOINT_STORAGE_ARRAY_'$var'_SMIS_SSL'
           smis_ssl=${!smis_ssl}
 
           if [ "$array_guid" != "NONE" ]; then
               run smisprovider create $provider $provider_ip $smis_port $SMIS_USER $SMIS_PASSWORD $smis_ssl
           fi
        done
    fi

    if [ "${RPVPLEX_TESTS}" = "1" -o "${RPMP_TESTS}" = "1" ]; then
        rp_vplex_setup
    fi

    if [ "${RPXIO_TESTS}" = "1" ]; then
        rp_xio_setup
    fi

    secho 'RecoverPoint discover storage systems'
    run storagedevice discover_all
    storagedevice list
}

rp_xio_setup()
{
    secho 'RecoverPoint XIO setup'
    run storageprovider create xtremeio1 $RECOVERPOINT_XTREMIO_IP 443 $RECOVERPOINT_XTREMIO_USER $RECOVERPOINT_XTREMIO_PASSWD xtremio
}

rp_vplex_setup()
{
    secho 'RecoverPoint VPLEX setup'

    # Create VPLEX
    if [ "$RECOVERPOINT_VPLEX_A" != "NONE" ]; then
        storageprovider create $RECOVERPOINT_VPLEX_A $RECOVERPOINT_VPLEX_A_IP $RECOVERPOINT_VPLEX_A_PORT $RECOVERPOINT_VPLEX_A_USER $RECOVERPOINT_VPLEX_A_PASSWORD vplex
    fi

    if [ "$RECOVERPOINT_VPLEX_B" != "NONE" ]; then
        if [ "${RP_CRR}" = "1" -o "${RP_CLR}" = "1" ]; then
            storageprovider create $RECOVERPOINT_VPLEX_B $RECOVERPOINT_VPLEX_B_IP $RECOVERPOINT_VPLEX_B_PORT $RECOVERPOINT_VPLEX_B_USER $RECOVERPOINT_VPLEX_B_PASSWORD vplex
        fi
    fi
    sleep 15
}

rp_port_setup()
{
    secho 'RecoverPoint port setup'

    if [ "${RP_TESTS}" = "1" -o "${RPVPLEX_TESTS}" = "1" -o "${RPMP_TESTS}" = "1" ]; then
        if [ "$RECOVERPOINT_STORAGE_ARRAY_A_GUID" != "NONE" ]; then
            run storageport update $RECOVERPOINT_STORAGE_ARRAY_A_GUID FC --addvarrays $RECOVERPOINT_VARRAY1
        fi

        if [ "$RECOVERPOINT_STORAGE_ARRAY_B_GUID" != "NONE" ]; then
            run storageport update $RECOVERPOINT_STORAGE_ARRAY_B_GUID FC --addvarrays $RECOVERPOINT_VARRAY2
        fi
        
        if [ "$RECOVERPOINT_STORAGE_ARRAY_C_GUID" != "NONE" ]; then
            run storageport update $RECOVERPOINT_STORAGE_ARRAY_C_GUID FC --addvarrays $RECOVERPOINT_VARRAY3
        fi
        
        if [ "$RECOVERPOINT_STORAGE_ARRAY_D_GUID" != "NONE" ]; then
            run storageport update $RECOVERPOINT_STORAGE_ARRAY_D_GUID FC --addvarrays $RECOVERPOINT_VARRAY4
        fi
    fi

    # VPLEX
    # Override port to varray assigments for VPLEX clusters to prevent port mixing in clusters
    # Source Site
    # Cluster 1
    if [ "${RPVPLEX_TESTS}" = "1" -o "${RPMP_TESTS}" = "1" ]; then
        if [ "$RECOVERPOINT_VPLEX_A_GUID" != "NONE" ] && [ "$RECOVERPOINT_STORAGE_ARRAY_A_GUID" != "NONE" ]; then
            run storageport update $RECOVERPOINT_VPLEX_A_GUID FC --group director-1-1-A --addvarrays $RECOVERPOINT_VARRAY1
            run storageport update $RECOVERPOINT_VPLEX_A_GUID FC --group director-1-1-B --addvarrays $RECOVERPOINT_VARRAY1
        fi
            # Cluster 2
        if [ "$RECOVERPOINT_VPLEX_A" != "NONE" ] && [ "$RECOVERPOINT_STORAGE_ARRAY_B_GUID" != "NONE" ]; then
            run storageport update $RECOVERPOINT_VPLEX_A_GUID FC --group director-2-1-A --addvarrays $RECOVERPOINT_VARRAY2
            run storageport update $RECOVERPOINT_VPLEX_A_GUID FC --group director-2-1-B --addvarrays $RECOVERPOINT_VARRAY2
        fi

        if [ "${RP_CRR}" = "1" -o "${RP_CLR}" = "1" ]; then
            # Target Site
            # Cluster 1
            if [ "$RECOVERPOINT_VPLEX_B" != "NONE" ] && [ "$RECOVERPOINT_STORAGE_ARRAY_C_GUID" != "NONE" ]; then
                run storageport update $RECOVERPOINT_VPLEX_B_GUID FC --group director-1-1-A --addvarrays $RECOVERPOINT_VARRAY3
                run storageport update $RECOVERPOINT_VPLEX_B_GUID FC --group director-1-1-B --addvarrays $RECOVERPOINT_VARRAY3
            fi

            # Cluster 2
            if [ "$RECOVERPOINT_VPLEX_B" != "NONE" ] && [ "$RECOVERPOINT_STORAGE_ARRAY_D_GUID" != "NONE" ]; then
                run storageport update $RECOVERPOINT_VPLEX_B_GUID FC --group director-2-1-A --addvarrays $RECOVERPOINT_VARRAY4
                run storageport update $RECOVERPOINT_VPLEX_B_GUID FC --group director-2-1-B --addvarrays $RECOVERPOINT_VARRAY4
            fi
        fi
    fi

    if [ "${RPXIO_TESTS}" = "1" ]; then
        run storageport update $RECOVERPOINT_XTREMIO_GUID FC --addvarrays $RECOVERPOINT_VARRAY1
    fi

}

rp_protection_system_setup()
{
    secho 'RecoverPoint protection system setup'

    protectionsystem show $RECOVERPOINT &> /dev/null && return $?

    run protectionsystem create $RECOVERPOINT \
    $RP_SYSTEM_TYPE \
    $RECOVERPOINT_IP \
    $RECOVERPOINT_PORT \
    $RECOVERPOINT_USER \
    $RECOVERPOINT_PASSWORD \
    1
}

rp_cg_setup()
{
    secho 'RecoverPoint CG setup'

    if [ "${RP_TESTS}" = "1" ]; then
        run blockconsistencygroup create $PROJECT $RP_CONSISTENCY_GROUP
    fi

    if [ "${RPVPLEX_TESTS}" = "1" ]; then
        run blockconsistencygroup create $PROJECT $RP_VPLEX_CONSISTENCY_GROUP
    fi

    if [ "${RPMP_TESTS}" = "1" ]; then
        run blockconsistencygroup create $PROJECT $RP_METROPOINT_CONSISTENCY_GROUP
    fi

    if [ "${RPXIO_TESTS}" = "1" ]; then
        run blockconsistencygroup create $PROJECT $RP_XIO_CONSISTENCY_GROUP
    fi
}

rp_isolate_rpa_clusters()
{
    if [ "$RP_USE_RPA_ISOLATION" = "1" ]; then
        secho 'RecoverPoint isolate RPA clusters'
        if [ "${RPXIO_TESTS}" = "0" ]; then
            run protectionsystem update --ip ${remote_ip} --name $RECOVERPOINT --cluster $RECOVERPOINT_RPA_CLUSTER1 --addvarrays $RECOVERPOINT_VARRAY1
            run protectionsystem update --ip ${remote_ip} --name $RECOVERPOINT --cluster $RECOVERPOINT_RPA_CLUSTER2 --addvarrays $RECOVERPOINT_VARRAY2
            run protectionsystem update --ip ${remote_ip} --name $RECOVERPOINT --cluster $RECOVERPOINT_RPA_CLUSTER3 --addvarrays $RECOVERPOINT_VARRAY3
        else
            run protectionsystem update --ip ${remote_ip} --name $RECOVERPOINT --cluster $RECOVERPOINT_XIORPA_CLUSTER1 --addvarrays $RECOVERPOINT_VARRAY1
        fi
    fi
}

recoverpoint_ingest_host_setup()
{
    vpool=${1}

    if [ "$RP_QUICK_PARAM" == "quick" ]; then
        CLUSTER1NET_NAME=VSAN_11
    else
        CLUSTER1NET_NAME="FABRIC_losam082-fabric"
    fi

    # Make different initiators for different hosts
    if [ "${rptypetag}" = "" ]; then
        DD="AA";
    elif [ "${rptypetag}" = "vplex" ]; then
        DD="BB";
    elif [ "${rptypetag}" = "mp" ]; then
        DD="CC";
    elif [ "${rptypetag}" = "xio" ]; then
        DD="DD";
    fi
    
    if [ "${vpool}" = "rp${rptypetag}_cdp" ]; then
        PWWN1=10:00:00:E0:E0:E0:${DD}:E0
        WWNN1=20:00:00:E0:E0:E0:${DD}:E0
        PWWN2=10:00:00:E0:E0:E0:${DD}:E1
        WWNN2=20:00:00:E0:E0:E0:${DD}:E1
        RP_HOSTNAME=${RECOVERPOINT_HOST_VARRAY1}${rptypetag}cdp
    elif [ "${vpool}" = "rp${rptypetag}_crr" ]; then
        PWWN1=10:00:00:E0:E0:E0:${DD}:F0
        WWNN1=20:00:00:E0:E0:E0:${DD}:F0
        PWWN2=10:00:00:E0:E0:E0:${DD}:F1
        WWNN2=20:00:00:E0:E0:E0:${DD}:F1
        RP_HOSTNAME=${RECOVERPOINT_HOST_VARRAY1}${rptypetag}crr
    elif [ "${vpool}" = "rp${rptypetag}_clr" ]; then
        PWWN1=10:00:00:E0:E0:E0:${DD}:D0
        WWNN1=20:00:00:E0:E0:E0:${DD}:D0
        PWWN2=10:00:00:E0:E0:E0:${DD}:D1
        WWNN2=20:00:00:E0:E0:E0:${DD}:D1
        RP_HOSTNAME=${RECOVERPOINT_HOST_VARRAY1}${rptypetag}clr
    fi

    run hosts create ${RP_HOSTNAME}1 $TENANT Windows ${RP_HOSTNAME}1 --port 8111 --username user --password 'password' --osversion 1.0 

    run initiator create ${RP_HOSTNAME}1 FC ${PWWN1} --node ${WWNN1}
    run initiator create ${RP_HOSTNAME}1 FC ${PWWN2} --node ${WWNN2}

    run transportzone add ${RECOVERPOINT_VARRAY1}/${CLUSTER1NET_NAME} ${PWWN1}
    run transportzone add ${RECOVERPOINT_VARRAY1}/${CLUSTER1NET_NAME} ${PWWN2}

    if [ "${vpool}" = "rp${rptypetag}_cdp" ]; then
        PWWN1=10:00:00:E0:E0:E0:${DD}:C0
        WWNN1=20:00:00:E0:E0:E0:${DD}:C0
        PWWN2=10:00:00:E0:E0:E0:${DD}:C1
        WWNN2=20:00:00:E0:E0:E0:${DD}:C1
    elif [ "${vpool}" = "rp${rptypetag}_crr" ]; then
        PWWN1=10:00:00:E0:E0:E0:${DD}:B0
        WWNN1=20:00:00:E0:E0:E0:${DD}:B0
        PWWN2=10:00:00:E0:E0:E0:${DD}:B1
        WWNN2=20:00:00:E0:E0:E0:${DD}:B1
    elif [ "${vpool}" = "rp${rptypetag}_clr" ]; then
        PWWN1=10:00:00:E0:E0:E0:${DD}:A0
        WWNN1=20:00:00:E0:E0:E0:${DD}:A0
        PWWN2=10:00:00:E0:E0:E0:${DD}:A1
        WWNN2=20:00:00:E0:E0:E0:${DD}:A1
    fi

    run hosts create ${RP_HOSTNAME}2 $TENANT Windows ${RP_HOSTNAME}2 --port 8111 --username user --password 'password' --osversion 1.0 

    run initiator create ${RP_HOSTNAME}2 FC ${PWWN1} --node ${WWNN1}
    run initiator create ${RP_HOSTNAME}2 FC ${PWWN2} --node ${WWNN2}

    run transportzone add ${RECOVERPOINT_VARRAY1}/${CLUSTER1NET_NAME} ${PWWN1}
    run transportzone add ${RECOVERPOINT_VARRAY1}/${CLUSTER1NET_NAME} ${PWWN2}
}

recoverpoint_ingest_discovery()
{
    INGEST_NATIVEGUID=$VMAX1_SIMULATOR_NATIVEGUID
    RP_NATIVEGUID=$RECOVERPOINT_RP_SIM_NATIVEGUID

    if [ "$RP_QUICK_PARAM" != "quick" ]; then
        if [ "${RPXIO_TESTS}" = "1" ]; then
            INGEST_NATIVEGUID=$RECOVERPOINT_XTREMIO_GUID
        else
            INGEST_NATIVEGUID=$RECOVERPOINT_STORAGE_ARRAY_A_GUID
        fi
        RP_NATIVEGUID=$RECOVERPOINT_RP_NATIVEGUID
    elif [ "${RPXIO_TESTS}" = "1" ]; then
        INGEST_NATIVEGUID=$XIO_4X_SIM_NATIVEGUID
    fi

    if [ ${RP_CRR} = "1" -o ${RP_CLR} = "1" -o "${RPMP_TESTS}" = "1" ]; then
        INGEST_NATIVEGUID2=$RECOVERPOINT_STORAGE_ARRAY_B_GUID
        INGEST_NATIVEGUID3=$RECOVERPOINT_STORAGE_ARRAY_C_GUID
        if [ "$RP_QUICK_PARAM" == "quick" ]; then
            INGEST_NATIVEGUID2=$VMAX2_SIMULATOR_NATIVEGUID
            INGEST_NATIVEGUID3=$VMAX3_SIMULATOR_NATIVEGUID
        fi
    fi

    secho 'RP ingest discovery'

    # Discover unmanaged volumes on the array
    run storagedevice discover_namespace $INGEST_NATIVEGUID 'UNMANAGED_VOLUMES'

    if [ ${RP_CRR} = "1" -o ${RP_CLR} = "1" -o "${RPMP_TESTS}" = "1" ]; then
        run storagedevice discover_namespace $INGEST_NATIVEGUID2 'UNMANAGED_VOLUMES'
    fi

    if [ ${RP_CRR} = "1" -o ${RP_CLR} = "1" ]; then
        run storagedevice discover_namespace $INGEST_NATIVEGUID3 'UNMANAGED_VOLUMES'
    fi

    if [ ${RPVPLEX_TESTS} = "1" -o "${RPMP_TESTS}" = "1" ]; then
        run storagedevice discover_namespace $RECOVERPOINT_VPLEX_A_GUID 'UNMANAGED_VOLUMES'
        if [ ${RP_CRR} = "1" -o ${RP_CLR} = "1" ]; then
            run storagedevice discover_namespace $RECOVERPOINT_VPLEX_B_GUID 'UNMANAGED_VOLUMES'
        fi
    fi

    # Especially with the simulator, the previous RP discovery may still be running
    state=`protectionsystem list | grep ${RP_SIMULATOR} | awk '{print $(NF-1)}'`
    while [ "$state" = "IN_PROGRESS" ]; do
        echo "Waiting for RP protection discovery to complete..."
        sleep 10
        state=`protectionsystem list | grep ${RP_SIMULATOR} | awk '{print $(NF-1)}'`
    done

    # Sleep 5 to get past discovery required timeframe
    sleep 10

    run protectionsystem discover_namespace $RP_NATIVEGUID 'UNMANAGED_CGS'
}

recoverpoint_export_host_setup()
{
    vpool=${1}
    rptype=${2}

    if [ "${vpool}" = "rp${rptype}_cdp" ]; then
        PWWN1=`pwwn D5`
        WWNN1=`nwwn D5`
        PWWN2=`pwwn D6`
        WWNN2=`nwwn D6`
        PWWN3=`pwwn D7`
        WWNN3=`nwwn D7`
        PWWN4=`pwwn D8`
        WWNN4=`nwwn D8`
        PWWN5="30:02:42:74:E2:26:D5:A1"
        WWNN5="40:02:42:74:E2:26:D5:A1"
        PWWN6="30:02:42:74:E2:26:D6:A1"
        WWNN6="40:02:42:74:E2:26:D6:A1"
        RP_HOSTNAME=${RP_EXPORT_GROUP_HOST}-${RANDOM}-cdp
    elif [ "${vpool}" = "rp${rptype}_crr" ]; then
        PWWN1=`pwwn E5`
        WWNN1=`nwwn E5`
        PWWN2=`pwwn E6`
        WWNN2=`nwwn E6`
        PWWN3=`pwwn E7`
        WWNN3=`nwwn E7`
        PWWN4=`pwwn E8`
        WWNN4=`nwwn E8`
        PWWN5="30:02:42:74:E2:26:E5:A1"
        WWNN5="40:02:42:74:E2:26:E5:A1"
        PWWN6="30:02:42:74:E2:26:E6:A1"
        WWNN6="40:02:42:74:E2:26:E6:A1"
        RP_HOSTNAME=${RP_EXPORT_GROUP_HOST}-${RANDOM}-crr
    elif [ "${vpool}" = "rp${rptype}_clr" ]; then
        PWWN1=`pwwn F5`
        WWNN1=`nwwn F5`
        PWWN2=`pwwn F6`
        WWNN2=`nwwn F6`
        PWWN3=`pwwn F7`
        WWNN3=`nwwn F7`
        PWWN4=`pwwn F8`
        WWNN4=`nwwn F8`
        PWWN5="30:02:42:74:E2:26:F5:A1"
        WWNN5="40:02:42:74:E2:26:F5:A1"
        PWWN6="30:02:42:74:E2:26:F6:A1"
        WWNN6="40:02:42:74:E2:26:F6:A1"
        RP_HOSTNAME=${RP_EXPORT_GROUP_HOST}-${RANDOM}-clr
    fi

    run hosts create ${RP_HOSTNAME}1 $TENANT Windows ${RP_HOSTNAME}1 --port 8111 --username user --password 'password' --osversion 1.0
    run hosts create ${RP_HOSTNAME}2 $TENANT Windows ${RP_HOSTNAME}2 --port 8111 --username user --password 'password' --osversion 1.0
  
    run initiator create ${RP_HOSTNAME}1 FC ${PWWN1} --node ${WWNN1}
    run initiator create ${RP_HOSTNAME}2 FC ${PWWN2} --node ${WWNN2}

    if [ "$RP_QUICK_PARAM" == "quick" ]; then
        # Creating additional initiator for sanity quick
        run initiator create ${RP_HOSTNAME}1 FC ${PWWN3} --node ${WWNN3}
        run initiator create ${RP_HOSTNAME}2 FC ${PWWN4} --node ${WWNN4}      

        run initiator create ${RP_HOSTNAME}1 FC ${PWWN5} --node ${WWNN5}
        run initiator create ${RP_HOSTNAME}2 FC ${PWWN6} --node ${WWNN6}

        run transportzone add ${RECOVERPOINT_VARRAY1}/VSAN_11 ${PWWN1}
        run transportzone add ${RECOVERPOINT_VARRAY1}/VSAN_11 ${PWWN2}  

        run transportzone add ${RECOVERPOINT_VARRAY2}/VSAN_12 ${PWWN3}
        run transportzone add ${RECOVERPOINT_VARRAY3}/VSAN_12 ${PWWN4}

        run transportzone add ${RECOVERPOINT_VARRAY2}/VSAN_13 ${PWWN5}
        run transportzone add ${RECOVERPOINT_VARRAY3}/VSAN_13 ${PWWN6}
    else
        run transportzone add "${RECOVERPOINT_VARRAY1}/FABRIC_losam082-fabric" ${PWWN1}
        run transportzone add "${RECOVERPOINT_VARRAY1}/FABRIC_losam082-fabric" ${PWWN2}     
    fi
}

recoverpoint_simulator_varsetup() {
    if [ "${RPVPLEX_TESTS}" = "1" -o "${RPMP_TESTS}" = "1" ]; then
        # VPLEX
        VPLEX_DEV_NAME=$VPLEX_SIMULATOR
        VPLEX_IP=$SIMULATOR_IP
        RECOVERPOINT_VPLEX_A=${VPLEX_DEV_NAME}-local
        RECOVERPOINT_VPLEX_B=${VPLEX_DEV_NAME}-remote
        RECOVERPOINT_VPLEX_A_GUID=${VPLEX1_SIMULATOR_GUID}
        RECOVERPOINT_VPLEX_B_GUID=${VPLEX2_SIMULATOR_GUID}
        RECOVERPOINT_STORAGE_ARRAY_A_GUID=${VMAX1_SIMULATOR_NATIVEGUID}
        RECOVERPOINT_STORAGE_ARRAY_B_GUID=${VMAX2_SIMULATOR_NATIVEGUID}
        RECOVERPOINT_STORAGE_ARRAY_C_GUID=${VMAX3_SIMULATOR_NATIVEGUID}
        RECOVERPOINT_STORAGE_ARRAY_D_GUID=${VMAX4_SIMULATOR_NATIVEGUID}
    fi
    if [ "${RPXIO_TESTS}" = "1" ]; then
        RECOVERPOINT_XTREMIO_GUID=${XIO_4X_SIM_NATIVEGUID}
    fi
}

# Simulator setup
recoverpoint_simulator_setup()
{
    secho 'RecoverPoint Simulator running'
    RECOVERPOINT_STORAGE_ARRAY_A_GUID=$VMAX1_SIMULATOR_NATIVEGUID
    RECOVERPOINT_STORAGE_ARRAY_B_GUID=$VMAX2_SIMULATOR_NATIVEGUID
    RECOVERPOINT_STORAGE_ARRAY_C_GUID=$VMAX3_SIMULATOR_NATIVEGUID
    RECOVERPOINT_STORAGE_ARRAY_D_GUID=$VMAX4_SIMULATOR_NATIVEGUID
    
    project_setup
    rp_varray_setup

    run syssvc $CONFIG_FILE $BOURNE_IPADDR set_prop controller_discovery_refresh_interval 5

    # Simulators being used
    run networksystem create $FABRIC_SIMULATOR  mds --devip $SIMULATOR_CISCO_MDS_IP --devport 22 --username $RP_FABRIC_SIM_USER --password $RP_FABRIC_SIM_PW
    run smisprovider create $PROVIDER_SIMULATOR $RP_PROVIDER_SIMULATOR_IP $RP_VMAX_SMIS_SIM_PORT $SMIS_USER "$SMIS_PASSWD" false
    
    if [ "${RPVPLEX_TESTS}" = "1" -o "${RPMP_TESTS}" = "1" ]; then
        storageprovider show ${RECOVERPOINT_VPLEX_A} &> /dev/null && return $?
        run storageprovider create ${RECOVERPOINT_VPLEX_A} $VPLEX_IP ${VPLEX1_SIMULATOR_PORT} $VPLEX_USER "$VPLEX_PASSWD" vplex
        if [ "${RP_CRR}" = "1" -o "${RP_CLR}" = "1" ]; then
            run storageprovider create ${RECOVERPOINT_VPLEX_B} $VPLEX_IP ${VPLEX2_SIMULATOR_PORT} $VPLEX_USER "$VPLEX_PASSWD" vplex
        fi        
    fi
    if [ "${RPXIO_TESTS}" = "1" ]; then
        run storageprovider create xtremeio1 $XIO_SIMULATOR_IP ${XIO_4X_SIMULATOR_PORT} root root xtremio
    fi

    run storagedevice discover_all
    
    rp_port_setup
    
    # For the time being, use implicit network assignments for the varrays instead of
    # explicit assignments.
    #run transportzone assign VSAN_11 ${RECOVERPOINT_VARRAY1}
    #run transportzone assign VSAN_12 ${RECOVERPOINT_VARRAY2}
    #run transportzone assign VSAN_13 ${RECOVERPOINT_VARRAY3}

    RECOVERPOINT_IP=$RP_SIMULATOR_IP
    rp_protection_system_setup

    # RPA Isolation
    run protectionsystem update --ip ${remote_ip} --name $RECOVERPOINT --cluster site_1rp1 --addvarrays $RECOVERPOINT_VARRAY1
    run protectionsystem update --ip ${remote_ip} --name $RECOVERPOINT --cluster site_2rp1 --addvarrays $RECOVERPOINT_VARRAY2
    run protectionsystem update --ip ${remote_ip} --name $RECOVERPOINT --cluster site_3rp1 --addvarrays $RECOVERPOINT_VARRAY3

    run storagepool update $VMAX1_SIMULATOR_NATIVEGUID --nhadd $RECOVERPOINT_VARRAY1 --type block
    run storagepool update $VMAX2_SIMULATOR_NATIVEGUID --nhadd $RECOVERPOINT_VARRAY2 --type block
    run storagepool update $VMAX3_SIMULATOR_NATIVEGUID --nhadd $RECOVERPOINT_VARRAY3 --type block
    if [ "${RPXIO_TESTS}" = "1" ]; then
        run storagepool update $XIO_4X_SIM_NATIVEGUID --nhadd $RECOVERPOINT_VARRAY1 --type block
    fi
}

recoverpoint_common_setup()
{
    project_setup
    rp_network_setup
    rp_varray_setup
    rp_storage_setup
    rp_port_setup
    rp_protection_system_setup
    rp_isolate_rpa_clusters
}

recoverpoint_setup()
{
    # Either we are using real arrays or arrays from the simulator.
    # Default is using real arrays.
    STORAGE_ARRAY_1_NATIVEGUID=$RECOVERPOINT_STORAGE_ARRAY_A_GUID
    STORAGE_ARRAY_2_NATIVEGUID=$RECOVERPOINT_STORAGE_ARRAY_B_GUID
    STORAGE_ARRAY_3_NATIVEGUID=$RECOVERPOINT_STORAGE_ARRAY_C_GUID
    STORAGE_ARRAY_4_NATIVEGUID=$RECOVERPOINT_STORAGE_ARRAY_D_GUID

    # Var to determine whether or not to use auto-matching
    # storgae pools on the vpools or to use manual selection
    # for storage pools.
    POOLS_AUTO_MATCH=true
    # To properly test migrations always set this variable as the
    # opposite of POOLS_AUTO_MATCH for migrations.
    POOLS_AUTO_MATCH_MIGRATE=false

    if [ "$RP_QUICK_PARAM" = "quick" ]; then
        recoverpoint_simulator_varsetup
        if [ "$RP_RUN_SETUP" = "1" ]; then
            # Using simulator arrays
            STORAGE_ARRAY_1_NATIVEGUID=$VMAX1_SIMULATOR_NATIVEGUID
            STORAGE_ARRAY_2_NATIVEGUID=$VMAX2_SIMULATOR_NATIVEGUID
            STORAGE_ARRAY_3_NATIVEGUID=$VMAX3_SIMULATOR_NATIVEGUID
            STORAGE_ARRAY_4_NATIVEGUID=$VMAX4_SIMULATOR_NATIVEGUID

            # Best to use auto match false for simulator
            POOLS_AUTO_MATCH=false
            POOLS_AUTO_MATCH_MIGRATE=true

            secho 'RecoverPoint Simulator setup'
            recoverpoint_simulator_setup
        else
            # This is intentionally a local-only execution for now.  Regular sanity for RP should always have RP_RUN_SETUP turned on.
            RP_SANITY_RANDOM=`/opt/storageos/bin/dbutils list UnManagedVolume | grep label | awk '{print $3}' | cut -c10-12 | head -1`
            secho "Determine existing volume label uses random value: ${RP_SANITY_RANDOM}"
        fi
    elif [ "$RP_RUN_SETUP" = "1" ]; then
        secho 'RecoverPoint setup'
        recoverpoint_common_setup
    fi

    if [ "$RP_RUN_SETUP" = "1" ]; then
        # Setup RP vpools
        rp_vpool_setup
    fi
}

##########################
# End RP setup functions #
##########################

###########################
# Start RP test functions #
###########################
recoverpoint_exports_tests()
{
    hecho 'Running RecoverPoint Export Tests'
    
    rpsnap=rpsnap-${RP_MODIFIED_HOSTNAME}-${RANDOM}
    blocksnap=blocksnap-${RP_MODIFIED_HOSTNAME}-${RANDOM}
    rpexport=$PROJECT/$RP_EXPORT_GROUP

    #Create RP/Block snapshots
    secho 'Create snapshots'
    run blocksnapshot create ${PROJECT}/${rpvolume} ${rpsnap} --type rp
    run blocksnapshot create ${PROJECT}/${rpvolume} ${blocksnap}
    run blocksnapshot activate ${PROJECT}/${rpvolume}/${blocksnap}
    run blocksnapshot delete ${PROJECT}/${rpvolume}/${blocksnap}
    run blocksnapshot create ${PROJECT}/${rpvolume}-target-${rp_src_varray} ${blocksnap}-${rp_src_varray}
    run blocksnapshot delete ${PROJECT}/${rpvolume}-target-${rp_src_varray}/${blocksnap}-${rp_src_varray}-${rp_src_varray}

    PWWN1=`pwwn 6F`;
    NWWN1=`pwwn 7F`;
    run hosts create ${RP_EXPORT_GROUP_HOST} $TENANT Windows ${RP_EXPORT_GROUP_HOST} --port 8111 --username $RP_HOST_USER --password '$RP_HOST_PW' --osversion 1.0
    run initiator create ${RP_EXPORT_GROUP_HOST} FC $PWWN1 --node $NWWN1

    if [ "$PARAM" = "vmaxblock" ]; then
        run transportzone add ${rp_src_varray}/${RP_VMAXB_VSAN} $PWWN1
    elif [ "$PARAM" = "vnxblock" ]; then
        run transportzone add ${rp_src_varray}/${RP_VNXB_VSAN} $PWWN1
    else
        # simulators being used
        run transportzone add ${rp_src_varray}/${SIMULATOR_VSAN_11} $PWWN1
    fi

    sleep 20

    secho 'recoverpoint exports'
    run export_group create $PROJECT ${RP_EXPORT_GROUP} ${rp_src_varray} --volspec ${PROJECT}/${rpvolume}/${rpsnap}-${rp_src_varray} --inits "${RP_EXPORT_GROUP_HOST}/$PWWN1"
    run export_group show ${rpexport}
    run export_group update ${rpexport} --remVol ${PROJECT}/${rpvolume}/${rpsnap}-${rp_src_varray}
    run export_group update ${rpexport} --addVolspec ${PROJECT}/${rpvolume}/${rpsnap}-${rp_src_varray}
    run export_group show ${rpexport}
    # CTRL-4638: --remInits will not call RP device controller to disable bookmark.
    #run export_group update ${rpexport} --remInits "${RP_EXPORT_GROUP_HOST}/$PWWN1"
    #run export_group update ${rpexport} --addInits "${RP_EXPORT_GROUP_HOST}/$PWWN1"
    run export_group show ${rpexport}
    run export_group delete ${rpexport}

    # currently having a volume and an RP snap in the same export group and deleting that export group doesn't work (CTRL?)
    #echo 'Running export of RP snapshot when volume already exists in mask'
    #run export_group create $PROJECT ${RP_EXPORT_GROUP}2 $NH --volspec ${PROJECT}/${rpvolume} --inits "${RP_EXPORT_GROUP_HOST}/$PWWN1"
    #run export_group update ${rpexport}2 --addVolspec ${PROJECT}/${rpvolume}/${rpsnap}-${NH}
    #run export_group update ${rpexport}2 --remVol ${PROJECT}/${rpvolume}/${rpsnap}-${NH}
    #run export_group update ${rpexport}2 --addVolspec ${PROJECT}/${rpvolume}/${rpsnap}-${NH}
    #run export_group delete ${rpexport}2

    secho 'done with RP exports, cleaning up'
    run blocksnapshot delete ${PROJECT}/${rpvolume}/${rpsnap}-${NH}
    run hosts delete ${RP_EXPORT_GROUP_HOST}
    secho 'done with recoverpoint exports cleanup'
}

recoverpoint_export_bookmark_tests()
{
    if [ "$RP_EXPORT_BOOKMARK_TESTS" = "1" ]; then
        hecho 'Running RecoverPoint Export Bookmark Tests'

        # The source source volume - an arbitrary source volume from the CG
        volume=${1}
        # The virtual array corresonding to the source volume (src_volume)
        rp_src_varray=${2}
        # The target virtual arrays
        rp_tgt_varrays=${3}
        # The virtual pool
        vpool=${4}
        # RP type
        rptype=${5}

        rpsnap=rpsnap-${RP_MODIFIED_HOSTNAME}-${RANDOM}
        blocksnap=blocksnap-${RP_MODIFIED_HOSTNAME}-${RANDOM}
        rpexport=$PROJECT/$RP_EXPORT_GROUP

        # Have to account for "-1" being added to volume name if
        # volume count is greater than 1.
        volume_name_modifier=''
        if [ "$RP_VOLUME_COUNT" = "1" ]; then
            volume_name_modifier=''
        else
            volume_name_modifier='-1'
        fi

        #Create RP/Block snapshots (only create for the first volume if there is more than 1)
        secho 'Creating RP bookmarks for each source volume'
	
        all_bookmarks=()
  
        for i in `seq $RP_VOLUME_COUNT`
        do
            #Create/show the snapshot/bookmark that was created for each target virtual array
        
            #If we only have a single volume we do not want to append anything to volume name.  Otherwise we want to
            #append a volume count identifier.
            volume_identifier=''
            if [ "$RP_VOLUME_COUNT" -gt "1" ]; then
                volume_identifier='-'${i}
            fi    
        
            run blocksnapshot create ${PROJECT}/${volume}${volume_identifier} ${rpsnap} --type rp
            for tgt_varray in ${rp_tgt_varrays}
            do
                #NOTE: RP bookmarks get created for each target copy (virtual array)
                all_bookmarks=("${all_bookmarks[@]}" "${PROJECT}/${volume}${volume_identifier}/${rpsnap}-${tgt_varray}")
                foundbookmark=`blocksnapshot show ${PROJECT}/${volume}${volume_identifier}/${rpsnap}-${tgt_varray} | grep snapset_label`
                if [[ "${foundbookmark}" = "" ]]; then
                    echo "+++ FAILURE - Bookmark ${rpsnap}-${tgt_varray} not created."
                    exit 1
                else
                    echo "+++ SUCCESS - Bookmark ${rpsnap}-${tgt_varray} created."   
                fi
	       done
        done

        recoverpoint_export_host_setup ${vpool} ${rptype}

        sleep 20
	
        secho 'Running export RP bookmark failure tests'

        if [ "$RP_VOLUME_COUNT" -gt "1" ]; then	
            secho 'Adding multiple bookmarks for the same target during export group create - expected failure'
            #join the list of all the bookmarks created with commas
            volspec="$(printf "%s,"  ${all_bookmarks[@]} | cut -d "," -f 1-${#all_bookmarks[@]})"
            fail export_group create $PROJECT ${RP_EXPORT_GROUP} ${rp_src_varray} --type Host --volspec ${volspec} --hosts "${RP_HOSTNAME}1"
        fi

        secho 'Creating 2 RP bookmarks for the same volume.'
        
        # Fail test bookmark 1
        run blocksnapshot create ${PROJECT}/${volume}${volume_name_modifier} ${rpsnap}-failtest1 --type rp        
        foundbookmark=`blocksnapshot show ${PROJECT}/${volume}${volume_name_modifier}/${rpsnap}-failtest1-${rp_tgt_varrays[0]} | grep snapset_label`
        if [[ "${foundbookmark}" = "" ]]; then
            echo "+++ FAILURE - Bookmark ${rpsnap}-failtest1-${rp_tgt_varrays[0]} not created."
            exit 1
        else
            echo "+++ SUCCESS - Bookmark ${rpsnap}-failtest1-${rp_tgt_varrays[0]} created."   
        fi        
        
        # Fail test bookmark 2
        run blocksnapshot create ${PROJECT}/${volume}${volume_name_modifier} ${rpsnap}-failtest2 --type rp
        blocksnapshot show ${PROJECT}/${volume}${volume_name_modifier}/${rpsnap}-failtest2-${rp_tgt_varrays[0]}
        foundbookmark=`blocksnapshot show ${PROJECT}/${volume}${volume_name_modifier}/${rpsnap}-failtest2-${rp_tgt_varrays[0]} | grep snapset_label`
        if [[ "${foundbookmark}" = "" ]]; then
            echo "+++ FAILURE - Bookmark ${rpsnap}-failtest2-${rp_tgt_varrays[0]} not created."
            exit 1
        else
            echo "+++ SUCCESS - Bookmark ${rpsnap}-failtest2-${rp_tgt_varrays[0]} created."   
        fi 
    
        run export_group create $PROJECT ${RP_EXPORT_GROUP} ${rp_tgt_varrays[0]} --type Host --volspec ${PROJECT}/${volume}${volume_name_modifier}/${rpsnap}-failtest1-${rp_tgt_varrays[0]} --hosts "${RP_HOSTNAME}1"
        secho 'Verifying the access state of the target volume is set to READWRITE'
        run volume verify ${PROJECT}/${volume}${volume_name_modifier}-target-${rp_tgt_varrays[0]} access_state READWRITE        

        export_group show ${rpexport}
        secho 'Exporting RP bookmark for the same target copy as a bookmark already exported - expected failure'
        fail export_group update ${rpexport} --addVolspec ${PROJECT}/${volume}${volume_name_modifier}/${rpsnap}-failtest2-${rp_tgt_varrays[0]}
        export_group show ${rpexport}

        secho 'Attempting to perform a swap of a consistency group when a bookmark is exported - expected failure'
        fail volume change_link ${PROJECT}/${volume}${volume_name_modifier} swap ${PROJECT}/${volume}-target-${rp_tgt_varrays[0]} rp 

        run export_group delete ${rpexport}
        secho 'Verifying the access state of the target volume is set to NOT_READY'
        run volume verify ${PROJECT}/${volume}${volume_name_modifier}-target-${rp_tgt_varrays[0]} access_state NOT_READY   

        secho 'Finished running export RP bookmark failure tests'

        secho 'Exporting RP bookmark to 2 different hosts'
        run export_group create $PROJECT ${RP_EXPORT_GROUP} ${rp_tgt_varrays[0]} --type Host --volspec ${PROJECT}/${volume}${volume_name_modifier}/${rpsnap}-${rp_tgt_varrays[0]} --hosts "${RP_HOSTNAME}1"
        run export_group create $PROJECT ${RP_EXPORT_GROUP}2 ${rp_tgt_varrays[0]} --type Host --volspec ${PROJECT}/${volume}${volume_name_modifier}/${rpsnap}-${rp_tgt_varrays[0]} --hosts "${RP_HOSTNAME}2"        

        secho 'Verifying the access state of the target volume is set to READWRITE'
        run volume verify ${PROJECT}/${volume}${volume_name_modifier}-target-${rp_tgt_varrays[0]} access_state READWRITE

        secho 'Unexport RP bookmark from host '${RP_HOSTNAME}2
        run export_group delete ${rpexport}2

        secho 'Verifying the access state of the target volume is set to READWRITE'
        run volume verify ${PROJECT}/${volume}${volume_name_modifier}-target-${rp_tgt_varrays[0]} access_state READWRITE

        secho 'Unexport RP bookmark from host '${RP_HOSTNAME}1
        run export_group delete ${rpexport}

        secho 'Verifying the access state of the target volume is set to NOT_READY'
        run volume verify ${PROJECT}/${volume}${volume_name_modifier}-target-${rp_tgt_varrays[0]} access_state NOT_READY

        secho 'Finished with export RP bookmark tests, cleaning up'

        blocksnapshot delete ${PROJECT}/${volume}${volume_name_modifier}/${rpsnap}-failtest1-${rp_tgt_varrays[0]}
        blocksnapshot delete ${PROJECT}/${volume}${volume_name_modifier}/${rpsnap}-failtest2-${rp_tgt_varrays[0]}

        for i in ${RP_VOLUME_COUNT}
        do
            for tgt_varray in ${rp_tgt_varrays}
            do
                volume_identifier=''
                if [ "$RP_VOLUME_COUNT" -gt "1" ]; then
                    volume_identifier='-'${i}
                fi
                blocksnapshot delete ${PROJECT}/${volume}${volume_identifier}/${rpsnap}-${tgt_varray}
            done
        done

        secho 'Deleting host '${RP_HOSTNAME}1
        run hosts delete ${RP_HOSTNAME}1
        secho 'Deleting host '${RP_HOSTNAME}2
        run hosts delete ${RP_HOSTNAME}2
        secho 'Done with RecoverPoint bookmark export cleanup'
    else
        secho 'Skipping RecoverPoint Export Bookmark Tests'
    fi
}

recoverpoint_auto_snapshot_cleanup_test()
{
    if [ "$RP_AUTO_SNAP_CLEANUP_TESTS" = "1" ]; then    
        hecho 'Running RecoverPoint Auto Snapshot Cleanup Tests'
        rpsnap=rpsnap_for_cleanup-${RP_MODIFIED_HOSTNAME}-${RANDOM}
        blocksnap=blocksnap-${RP_MODIFIED_HOSTNAME}-${RANDOM}

        src_volume=${1}
        tgt_volume=${2}
        tgt_varray=${3}
        cg_name=${4}
        tgt2_volume=${5}

        # Create RP/Block snapshot
        secho 'Create snapshot'
        run blocksnapshot create ${PROJECT}/${src_volume} ${rpsnap} --type rp
        blocksnapshot show ${PROJECT}/${src_volume}/${rpsnap}-${tgt_varray} | grep inactive | grep false
        run protectionsystem discover $RECOVERPOINT

        secho 'Unaffected RP bookmark is being checked after RP discovery to ensure it is still active'
        blocksnapshot show ${PROJECT}/${src_volume}/${rpsnap}-${tgt_varray} | grep inactive | grep false

        secho 'Stopping protection for the copy and then re-enabling'
        run volume change_link ${PROJECT}/${src_volume} stop ${PROJECT}/${tgt_volume} rp
    
        sleep 15
        run volume change_link ${PROJECT}/${src_volume} start ${PROJECT}/${tgt_volume} rp

        if [ "${tgt2_volume}" != "" ]; then
            secho 'Stopping protection for the copy and then re-enabling'
            run volume change_link ${PROJECT}/${src_volume} stop ${PROJECT}/${tgt2_volume} rp
            sleep 15
            run volume change_link ${PROJECT}/${src_volume} start ${PROJECT}/${tgt2_volume} rp
        fi

        secho 'Re-run discovery, RP bookmark should be cleaned up automatically'
        sleep 10
        run protectionsystem discover $RECOVERPOINT

        secho 'Removed RP bookmark is being checked after RP discovery to ensure it is no longer active'
        # Use "fail" instead of run because we expect this bookmark to fail.
        fail blocksnapshot show ${PROJECT}/${src_volume}/${rpsnap}-${tgt_varray}

        secho 'Creating RP bookmark'
        run blocksnapshot create ${PROJECT}/${src_volume} ${rpsnap}2 --type rp
        blocksnapshot show ${PROJECT}/${src_volume}/${rpsnap}2-${tgt_varray} | grep inactive | grep false       

        # do a restore before swap to show that the lock is removed after restore
        secho 'Performing restore of RP bookmark'
        run blocksnapshot restore ${PROJECT}/${src_volume}/${rpsnap}2-${tgt_varray}
        sleep 10

        secho 'Performing swap operation to verify auto snapshot cleanup'
        run volume change_link ${PROJECT}/${src_volume} swap ${PROJECT}/${tgt_volume} rp

        secho 'Verifying that RP bookmark was removed after swap'
        fail blocksnapshot show ${PROJECT}/${src_volume}/${rpsnap}2-${tgt_varray}

        secho 'Performing swap back'
        run volume change_link ${PROJECT}/${tgt_volume} swap ${PROJECT}/${src_volume} rp

        # Removing the tests below because of COP-25080
        #sleep 10

        #secho 'Creating RP bookmark'
        #run blocksnapshot create ${PROJECT}/${src_volume} ${rpsnap} --type rp
        #blocksnapshot show ${PROJECT}/${src_volume}/${rpsnap}-${tgt_varray} | grep inactive | grep false

        #secho 'Performing consistency group swap operation to verify auto snapshot cleanup'
        #run blockconsistencygroup swap $cg_name --copyType rp --targetVarray $tgt_varray

        #secho 'Verifying that RP bookmark was removed after consistency group swap'
        #fail blocksnapshot show ${PROJECT}/${src_volume}/${rpsnap}-${tgt_varray}
 
        #secho 'Performing consistency group swap back'
        #run blockconsistencygroup swap $cg_name --copyType rp --targetVarray $RECOVERPOINT_VARRAY1
    else
        secho 'Skipping RecoverPoint Auto Snapshot Cleanup Tests'
    fi
}

recoverpoint_snapshot_test()
{
    srcvolume=$1
    tgtvarray=$2
    rpsnapname=$3
    blocksnapname=$4

    # Bypassing snapshot tests for two reasons
    secho 'Bypassing snapshot tests until:'
    secho '- Simulator is not removing snapshots, so unmanaged discovery is pulling in snapshots we deleted'
    secho '- Deleting the snapshot in ViPR is not removing the BlockSnapshotSession reference, so inventory-only delete is failing'
    return

    # Create RP/Block snapshots
    hecho 'Running RecoverPoint Create Snapshots Tests'

    # Create array-based snapshot off of the RP target
    run_noundo blocksnapshot create ${PROJECT}/${srcvolume}-target-${tgtvarray} ${blocksnapname}-${tgtvarray}
    run_noundo blocksnapshot delete ${PROJECT}/${srcvolume}-target-${tgtvarray}/${blocksnapname}-${tgtvarray}

    # Create array-based snapshot off of the RP source
    run_noundo blocksnapshot create ${PROJECT}/${srcvolume} ${blocksnapname}

    # RP Simulator can't handle restore operation at this time.
    if [ "$RP_QUICK_PARAM" != "quick" ]; then
        run_noundo blocksnapshot restore ${PROJECT}/${srcvolume}/${blocksnapname}
    fi

    run_noundo blocksnapshot delete ${PROJECT}/${srcvolume}/${blocksnapname}

    # When we create these snapshots, the name will be augemented with the target varray, therefore we can't unroll it.
    run_noundo blocksnapshot create ${PROJECT}/${srcvolume} ${rpsnapname} --type rp
    run_noundo blocksnapshot delete ${PROJECT}/${srcvolume}/${rpsnapname}-${tgtvarray}
}

recoverpoint_ingest_test()
{
    hecho 'Running RecoverPoint Ingest Test'

    vpool=${1}

    # Debug only.  If you're diagnosing basic issues in ingestion, this flag cuts down the extra tests and keeps the testing 
    # to the basics of ingestion itself.
    shorttest=0

    # Labels
    RECOVERPOINT_INGEST_VOL1_CG=${RECOVERPOINT_INGEST_VOL1_CGBASE}${RP_SANITY_RANDOM}_${vpool}
    RECOVERPOINT_INGEST_VOLBASE=${RECOVERPOINT_INGEST_VOL1BASE}${RP_SANITY_RANDOM}
    RECOVERPOINT_INGEST_VOL1_SRC=${RECOVERPOINT_INGEST_VOLBASE}_${vpool}

    RECOVERPOINT_INGEST_VOL1_SRC_JRNL=${RECOVERPOINT_INGEST_VOL1_SRC}-${RECOVERPOINT_VARRAY1}-journal-1
    RECOVERPOINT_INGEST_SNAPSHOT_VARRAY=${RECOVERPOINT_VARRAY1}

    if [ "${vpool}" = "rp${rptypetag}_cdp" ]; then
        RECOVERPOINT_INGEST_VOL1_TGT=${RECOVERPOINT_INGEST_VOL1_SRC}-target-${RECOVERPOINT_VARRAY1}
        RECOVERPOINT_INGEST_VOL1_TGT_JRNL=${RECOVERPOINT_INGEST_VOL1_SRC}-${RECOVERPOINT_VARRAY1}-journal-2
        RECOVERPOINT_TGT_VARRAY=${RECOVERPOINT_VARRAY1}

        if [ "${vpool}" = "rpmp_cdp" ]; then
            # Also need to ingest the MP standby production journal, varray2 target and journal volumes as well.
            RECOVERPOINT_INGEST_VOL1_STANDBY_SRC_JRNL=${RECOVERPOINT_INGEST_VOL1_SRC}-${RECOVERPOINT_VARRAY2}-journal-1
            RECOVERPOINT_INGEST_VOL1_STANDBY_TGT=${RECOVERPOINT_INGEST_VOL1_SRC}-target-${RECOVERPOINT_VARRAY2}
            RECOVERPOINT_INGEST_VOL1_STANDBY_TGT_JRNL=${RECOVERPOINT_INGEST_VOL1_SRC}-${RECOVERPOINT_VARRAY2}-journal-2
        fi
    elif [ "${vpool}" = "rp${rptypetag}_crr" ]; then
        RECOVERPOINT_INGEST_VOL1_TGT=${RECOVERPOINT_INGEST_VOL1_SRC}-target-${RECOVERPOINT_VARRAY3}
        RECOVERPOINT_INGEST_VOL1_TGT_JRNL=${RECOVERPOINT_INGEST_VOL1_SRC}-${RECOVERPOINT_VARRAY3}-journal-1
        RECOVERPOINT_INGEST_SNAPSHOT_VARRAY=${RECOVERPOINT_VARRAY3}
        RECOVERPOINT_TGT_VARRAY=${RECOVERPOINT_VARRAY3}

        if [ "${vpool}" = "rpmp_crr" ]; then
            # Also need to ingest the MP standby production journal, varray2 target and journal volumes as well.
            RECOVERPOINT_INGEST_VOL1_STANDBY_SRC_JRNL=${RECOVERPOINT_INGEST_VOL1_SRC}-${RECOVERPOINT_VARRAY2}-journal-1
        fi
    elif [ "${vpool}" = "rp${rptypetag}_clr" ]; then
        RECOVERPOINT_INGEST_VOL1_V1TGT=${RECOVERPOINT_INGEST_VOL1_SRC}-target-${RECOVERPOINT_VARRAY1}
        RECOVERPOINT_INGEST_VOL1_V1TGT_JRNL=${RECOVERPOINT_INGEST_VOL1_SRC}-${RECOVERPOINT_VARRAY1}-journal-2
        RECOVERPOINT_INGEST_VOL1_V3TGT=${RECOVERPOINT_INGEST_VOL1_SRC}-target-${RECOVERPOINT_VARRAY3}
        RECOVERPOINT_INGEST_VOL1_V3TGT_JRNL=${RECOVERPOINT_INGEST_VOL1_SRC}-${RECOVERPOINT_VARRAY3}-journal-1

        if [ "${vpool}" = "rpmp_clr" ]; then
            # Also need to ingest the MP standby production journal, varray2 target and journal volumes as well.
            RECOVERPOINT_INGEST_VOL1_STANDBY_SRC_JRNL=${RECOVERPOINT_INGEST_VOL1_SRC}-${RECOVERPOINT_VARRAY2}-journal-1
            RECOVERPOINT_INGEST_VOL1_STANDBY_TGT=${RECOVERPOINT_INGEST_VOL1_SRC}-target-${RECOVERPOINT_VARRAY2}
            RECOVERPOINT_INGEST_VOL1_STANDBY_TGT_JRNL=${RECOVERPOINT_INGEST_VOL1_SRC}-${RECOVERPOINT_VARRAY2}-journal-2
        fi

        # For post-ingest operations
        RECOVERPOINT_INGEST_VOL1_TGT=${RECOVERPOINT_INGEST_VOL1_V1TGT}
        RECOVERPOINT_INGEST_SNAPSHOT_VARRAY=${RECOVERPOINT_VARRAY3}
        RECOVERPOINT_TGT_VARRAY=${RECOVERPOINT_VARRAY1}
    fi

    rp_target_vpool=rp_targets
    if [ "${RPXIO_TESTS}" = "1" ]; then
        rp_target_vpool=rpxio_targets
    fi

    # Create the consistency group + volume
    run blockconsistencygroup create ${PROJECT} ${RECOVERPOINT_INGEST_VOL1_CG} --noarrayconsistency
    run volume create ${RECOVERPOINT_INGEST_VOL1_SRC} ${PROJECT} ${RECOVERPOINT_VARRAY1} ${vpool} 1GB --consistencyGroup ${RECOVERPOINT_INGEST_VOL1_CG}

    if [ "${shorttest}" -eq "0" ]; then
        # Run snapshot tests
        recoverpoint_snapshot_test ${RECOVERPOINT_INGEST_VOL1_SRC} ${RECOVERPOINT_TGT_VARRAY} rpsnap1-${RP_MODIFIED_HOSTNAME}-${RANDOM} blocksnap1-${RP_MODIFIED_HOSTNAME}-${RANDOM}
    fi

    # Delete the volume and CG
    run volume delete ${PROJECT}/${RECOVERPOINT_INGEST_VOL1_SRC} --vipronly
    run blockconsistencygroup delete ${RECOVERPOINT_INGEST_VOL1_CG} --vipronly

    # Run discovery
    recoverpoint_ingest_discovery

    rpsnap=rpsnap_for_validation-${RP_MODIFIED_HOSTNAME}-${RANDOM}
    if [ "${vpool}" = "rp${rptypetag}_cdp" ]; then
        if [ "${shorttest}" -eq "0" ]; then
            # Make sure it fails against a different sync policy than what's on the RPA
            fail unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rp${rptypetag}_cdp-sync $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC}"
            
            # Ingest the volume, but then immediately inventory-only delete it, rediscover, and ingest it again.
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rp${rptypetag}_cdp $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC}"
            run_noundo volume delete ${PROJECT}/"${RECOVERPOINT_INGEST_VOL1_SRC}" --vipronly --force
            # Recreate unmanaged volumes/protection sets
 
            # Run discovery
            recoverpoint_ingest_discovery
        fi

        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_TGT}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_TGT_JRNL}"

        if [ "${shorttest}" -eq "0" ]; then
            # Make sure you fail to create an RP bookmark because you're not fully ingested yet
            fail blocksnapshot create ${PROJECT}/${RECOVERPOINT_INGEST_VOL1_SRC} ${rpsnap}1 --type rp
            # Make sure you fail to create an array snapshot because you're not fully ingested yet
            fail blocksnapshot create ${PROJECT}/${RECOVERPOINT_INGEST_VOL1_TGT} ${rpsnap}2
        fi

        if [ "${vpool}" = "rpmp_cdp" ]; then
            # MP active production journal
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC_JRNL}"
            # MP standby production journal
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_STANDBY_SRC_JRNL}"
            # MP target CDP on varray2
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rp_targets $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_STANDBY_TGT}"
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rp_targets $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_STANDBY_TGT_JRNL}"
        else
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC_JRNL}"
        fi
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rp${rptypetag}_cdp $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC}"
    elif [ "${vpool}" = "rp${rptypetag}_crr" ]; then
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY3} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_TGT}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY3} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_TGT_JRNL}"
        if [ "${vpool}" = "rpvplex_crr" -o "${vpool}" = "rpmp_crr" ]; then
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC_JRNL}"
        else
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC_JRNL}"
        fi
        if [ "${vpool}" = "rpmp_crr" ]; then
            # MP standby production journal
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_STANDBY_SRC_JRNL}"
        fi
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rp${rptypetag}_crr $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC}"
    elif [ "${vpool}" = "rp${rptypetag}_clr" ]; then
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_V1TGT}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_V1TGT_JRNL}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY3} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_V3TGT}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY3} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_V3TGT_JRNL}"

        if [ "${vpool}" = "rpmp_clr" ]; then
            # MP active production journal
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC_JRNL}"
            # MP standby production journal
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_STANDBY_SRC_JRNL}"
            # MP target CDP on varray2
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rp_targets $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_STANDBY_TGT}"
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rp_targets $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_STANDBY_TGT_JRNL}"
        else
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC_JRNL}"
        fi

        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rp${rptypetag}_clr $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC}"
    fi

    # Verify the CG got created; fail script if this fails.
    run blockconsistencygroup show ViPR-"${RECOVERPOINT_INGEST_VOL1_CG}"

    # Run snapshot tests
    if [ "${shorttest}" -eq "0" ]; then
        recoverpoint_snapshot_test ${RECOVERPOINT_INGEST_VOL1_SRC} ${RECOVERPOINT_TGT_VARRAY} rpsnap2-${RP_MODIFIED_HOSTNAME}-${RANDOM} blocksnap2-${RP_MODIFIED_HOSTNAME}-${RANDOM}

        # now perform failover tests
        recoverpoint_failover_test "${RECOVERPOINT_INGEST_VOL1_SRC}" "${RECOVERPOINT_INGEST_VOL1_TGT}"
    fi

    # Add a volume to the protection
    run volume create "${RECOVERPOINT_INGEST_VOL1_SRC}"-2 ${PROJECT} ${RECOVERPOINT_VARRAY1} ${vpool} 8GB --consistencyGroup ViPR-"${RECOVERPOINT_INGEST_VOL1_CG}"

    if [ "${shorttest}" -eq "0" ]; then
        # Run snapshot tests
        recoverpoint_snapshot_test ${RECOVERPOINT_INGEST_VOL1_SRC} ${RECOVERPOINT_TGT_VARRAY} rpsnap3-${RP_MODIFIED_HOSTNAME}-${RANDOM} blocksnap3-${RP_MODIFIED_HOSTNAME}-${RANDOM}
    fi

    # Remove a volume from the protection
    run_noundo volume delete ${PROJECT}/"${RECOVERPOINT_INGEST_VOL1_SRC}"-2 --wait

    # Inventory-only remove of RP volume 
    run_noundo volume delete ${PROJECT}/"${RECOVERPOINT_INGEST_VOL1_SRC}" --vipronly
    # COP-21592: inventory-only delete of the last volume deletes the CG
    # blockconsistencygroup show ViPR-${RECOVERPOINT_INGEST_VOL1_CG}
    run_noundo blockconsistencygroup delete ViPR-${RECOVERPOINT_INGEST_VOL1_CG} --vipronly

    # Run discovery
    recoverpoint_ingest_discovery

    # Re-ingest
    if [ "${vpool}" = "rp${rptypetag}_cdp" ]; then
        if [ "${shorttest}" -eq "0" ]; then
            # Ingest the volume, but then immediately inventory-only delete it, rediscover, and ingest it again.
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_TGT}"
            run_noundo volume delete ${PROJECT}/"${RECOVERPOINT_INGEST_VOL1_TGT}" --vipronly --force
            # Recreate unmanaged volumes/protection sets

            # Run discovery
            recoverpoint_ingest_discovery
        fi

        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_TGT}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_TGT_JRNL}"
        if [ "${vpool}" = "rpmp_cdp" ]; then
            # MP active production journal
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC_JRNL}"
            # MP standby production journal
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_STANDBY_SRC_JRNL}"
            # MP target CDP on varray2
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rp_targets $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_STANDBY_TGT}"
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rp_targets $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_STANDBY_TGT_JRNL}"
        else
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC_JRNL}"
        fi
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rp${rptypetag}_cdp $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC}"
        # Verify the CG got created; fail script if this fails.
        run blockconsistencygroup show ViPR-"${RECOVERPOINT_INGEST_VOL1_CG}"
        if [ "${shorttest}" -eq "0" ]; then
            recoverpoint_auto_snapshot_cleanup_test "${RECOVERPOINT_INGEST_VOL1_SRC}" "${RECOVERPOINT_INGEST_VOL1_TGT}" ${RECOVERPOINT_INGEST_SNAPSHOT_VARRAY} ${RECOVERPOINT_INGEST_VOL1_CG} ${RECOVERPOINT_INGEST_VOL1_STANDBY_TGT}
        fi
    elif [ "${vpool}" = "rp${rptypetag}_crr" ]; then
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rp${rptypetag}_crr $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY3} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_TGT}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY3} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_TGT_JRNL}"

        if [ "${vpool}" = "rpmp_crr" ]; then
            # MP standby production journal
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_STANDBY_SRC_JRNL}"
        fi

        if [ "${vpool}" = "rpvplex_crr" -o "${vpool}" = "rpmp_crr" ]; then
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC_JRNL}"
        else
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC_JRNL}"
        fi

        # Verify the CG got created; fail script if this fails.
        run blockconsistencygroup show ViPR-"${RECOVERPOINT_INGEST_VOL1_CG}"
        if [ "${shorttest}" -eq "0" ]; then
            recoverpoint_auto_snapshot_cleanup_test "${RECOVERPOINT_INGEST_VOL1_SRC}" "${RECOVERPOINT_INGEST_VOL1_TGT}" ${RECOVERPOINT_INGEST_SNAPSHOT_VARRAY} ${RECOVERPOINT_INGEST_VOL1_CG}
        fi
    elif [ "${vpool}" = "rp${rptypetag}_clr" ]; then
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rp${rptypetag}_clr $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_V1TGT}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_V1TGT_JRNL}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY3} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_V3TGT}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY3} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_V3TGT_JRNL}"

        if [ "${vpool}" = "rpmp_clr" ]; then
            # MP active production journal
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC_JRNL}"
            # MP standby production journal
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_STANDBY_SRC_JRNL}"
            # MP target CDP on varray2
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rp_targets $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_STANDBY_TGT}"
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rp_targets $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_STANDBY_TGT_JRNL}"
        else
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC_JRNL}"
        fi

        # Verify the CG got created; fail script if this fails.
        run blockconsistencygroup show ViPR-"${RECOVERPOINT_INGEST_VOL1_CG}"
        if [ "${shorttest}" -eq "0" ]; then
            recoverpoint_auto_snapshot_cleanup_test "${RECOVERPOINT_INGEST_VOL1_SRC}" "${RECOVERPOINT_INGEST_VOL1_TGT}" ${RECOVERPOINT_INGEST_SNAPSHOT_VARRAY} ${RECOVERPOINT_INGEST_VOL1_CG} "${RECOVERPOINT_INGEST_VOL1_V3TGT}"
        fi
    fi

    if [ "${shorttest}" -eq "0" ]; then
        # Run snapshot tests
        recoverpoint_snapshot_test ${RECOVERPOINT_INGEST_VOL1_SRC} ${RECOVERPOINT_TGT_VARRAY} rpsnap4-${RP_MODIFIED_HOSTNAME}-${RANDOM} blocksnap4-${RP_MODIFIED_HOSTNAME}-${RANDOM}
    fi

    # Real removal of volume/CG
    run_noundo volume delete ${PROJECT}/"${RECOVERPOINT_INGEST_VOL1_SRC}" --wait
    run_noundo blockconsistencygroup delete ViPR-${RECOVERPOINT_INGEST_VOL1_CG}
}

recoverpoint_ingest_export_test()
{
    hecho 'Running RecoverPoint Ingest Export Tests'

    # Some more thought needs to go into this, how to integrate this into the regular test suite
    # and efficiently take into the account the test variables for RP.

    vpool=${1}

    # Debug only.  If you're diagnosing basic issues in ingestion, this flag cuts down the extra tests and keeps the testing 
    # to the basics of ingestion itself.
    shorttest=0

    # Create a host to export volumes to
    recoverpoint_ingest_host_setup ${vpool}

    # Set up labels
    RECOVERPOINT_INGEST_EXPORT_VOL1_CG=${RECOVERPOINT_INGEST_VOL1_CGBASE}${RP_SANITY_RANDOM}exp_${vpool}
    RECOVERPOINT_INGEST_EXPORT_VOLBASE=${RECOVERPOINT_INGEST_VOL1BASE}${RP_SANITY_RANDOM}exp
    RECOVERPOINT_INGEST_EXPORT_VOL1_SRC=${RECOVERPOINT_INGEST_EXPORT_VOLBASE}_${vpool}

    # Create the consistency group and volume
    run blockconsistencygroup create ${PROJECT} ${RECOVERPOINT_INGEST_EXPORT_VOL1_CG}  --noarrayconsistency
    run volume create ${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC} ${PROJECT} ${RECOVERPOINT_VARRAY1} ${vpool} 1GB --consistencyGroup ${RECOVERPOINT_INGEST_EXPORT_VOL1_CG}

    # Export the volume to a host
    run export_group create ${PROJECT} EG${vpool} ${RECOVERPOINT_VARRAY1} --type Host --volspec "${PROJECT}/${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}" --hosts "${RP_HOSTNAME}1,${RP_HOSTNAME}2"

    # Inventory-only delete the volume, export, and CG
    run volume delete ${PROJECT}/${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC} --vipronly
    run export_group delete ${PROJECT}/EG${vpool}
    run blockconsistencygroup delete ${RECOVERPOINT_INGEST_EXPORT_VOL1_CG} --vipronly

    RECOVERPOINT_INGEST_EXPORT_VOL1_SRC_JRNL=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-${RECOVERPOINT_VARRAY1}-journal-1
    RECOVERPOINT_INGEST_SNAPSHOT_VARRAY=${RECOVERPOINT_VARRAY1}

    if [ "${vpool}" = "rp${rptypetag}_cdp" ]; then
        RECOVERPOINT_INGEST_EXPORT_VOL1_TGT=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-target-${RECOVERPOINT_VARRAY1}
        RECOVERPOINT_INGEST_EXPORT_VOL1_TGT_JRNL=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-${RECOVERPOINT_VARRAY1}-journal-2

        if [ "${vpool}" = "rpmp_cdp" ]; then
                # Also need to ingest the MP standby production journal, varray2 target and journal volumes as well.
            RECOVERPOINT_INGEST_EXPORT_VOL1_STANDBY_SRC_JRNL=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-${RECOVERPOINT_VARRAY2}-journal-1
            RECOVERPOINT_INGEST_EXPORT_VOL1_STANDBY_TGT=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-target-${RECOVERPOINT_VARRAY2}
            RECOVERPOINT_INGEST_EXPORT_VOL1_STANDBY_TGT_JRNL=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-${RECOVERPOINT_VARRAY2}-journal-2
        fi
    elif [ "${vpool}" = "rp${rptypetag}_crr" ]; then
        RECOVERPOINT_INGEST_EXPORT_VOL1_TGT=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-target-${RECOVERPOINT_VARRAY3}
        RECOVERPOINT_INGEST_EXPORT_VOL1_TGT_JRNL=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-${RECOVERPOINT_VARRAY3}-journal-1
        RECOVERPOINT_INGEST_SNAPSHOT_VARRAY=${RECOVERPOINT_VARRAY3}
        if [ "${vpool}" = "rpmp_crr" ]; then
            # Also need to ingest the MP standby production journal, varray2 target and journal volumes as well.
            RECOVERPOINT_INGEST_EXPORT_VOL1_STANDBY_SRC_JRNL=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-${RECOVERPOINT_VARRAY2}-journal-1
        fi
    elif [ "${vpool}" = "rp${rptypetag}_clr" ]; then
        RECOVERPOINT_INGEST_EXPORT_VOL1_V1TGT=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-target-${RECOVERPOINT_VARRAY1}
        RECOVERPOINT_INGEST_EXPORT_VOL1_V1TGT_JRNL=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-${RECOVERPOINT_VARRAY1}-journal-2
        RECOVERPOINT_INGEST_EXPORT_VOL1_V3TGT=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-target-${RECOVERPOINT_VARRAY3}
        RECOVERPOINT_INGEST_EXPORT_VOL1_V3TGT_JRNL=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-${RECOVERPOINT_VARRAY3}-journal-1

        if [ "${vpool}" = "rpmp_clr" ]; then
            # Also need to ingest the MP standby production journal, varray2 target and journal volumes as well.
            RECOVERPOINT_INGEST_EXPORT_VOL1_STANDBY_SRC_JRNL=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-${RECOVERPOINT_VARRAY2}-journal-1
            RECOVERPOINT_INGEST_EXPORT_VOL1_STANDBY_TGT=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-target-${RECOVERPOINT_VARRAY2}
            RECOVERPOINT_INGEST_EXPORT_VOL1_STANDBY_TGT_JRNL=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-${RECOVERPOINT_VARRAY2}-journal-2
        fi

        # For post-ingest operations
        RECOVERPOINT_INGEST_EXPORT_VOL1_TGT=${RECOVERPOINT_INGEST_EXPORT_VOL1_V1TGT}
        RECOVERPOINT_INGEST_SNAPSHOT_VARRAY=${RECOVERPOINT_VARRAY3}
    fi

    rp_target_vpool=rp_targets
    if [ "${RPXIO_TESTS}" = "1" ]; then
        rp_target_vpool=rpxio_targets
    fi

    # Run discovery
    recoverpoint_ingest_discovery

    rpsnap=rpsnap_for_validation-${RP_MODIFIED_HOSTNAME}-${RANDOM}
    if [ "${vpool}" = "rp${rptypetag}_cdp" ]; then
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_TGT}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_TGT_JRNL}"
        if [ "${vpool}" = "rpmp_cdp" ]; then
            # MP active production journal
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC_JRNL}"
            # MP standby production journal
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_STANDBY_SRC_JRNL}"
            # MP target CDP on varray2
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rp_targets $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_STANDBY_TGT}"
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rp_targets $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_STANDBY_TGT_JRNL}"
        else
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC_JRNL}"
        fi
        run unmanagedvolume ingest_export ${RECOVERPOINT_VARRAY1} rp${rptypetag}_cdp $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}" --host ${RP_HOSTNAME}1
        run unmanagedvolume ingest_export ${RECOVERPOINT_VARRAY1} rp${rptypetag}_cdp $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}" --host ${RP_HOSTNAME}2
        RECOVERPOINT_TGT_VARRAY=${RECOVERPOINT_VARRAY1}
    elif [ "${vpool}" = "rp${rptypetag}_crr" ]; then
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY3} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_TGT}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY3} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_TGT_JRNL}"
        if [ "${vpool}" = "rpmp_crr" ]; then
            # MP standby production journal
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_STANDBY_SRC_JRNL}"
        fi
        if [ "${vpool}" = "rpvplex_crr" -o "${vpool}" = "rpmp_crr" ]; then
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC_JRNL}"
        else
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC_JRNL}"
        fi
        run unmanagedvolume ingest_export ${RECOVERPOINT_VARRAY1} rp${rptypetag}_crr $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}" --host ${RP_HOSTNAME}1
        run unmanagedvolume ingest_export ${RECOVERPOINT_VARRAY1} rp${rptypetag}_crr $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}" --host ${RP_HOSTNAME}2
        RECOVERPOINT_TGT_VARRAY=${RECOVERPOINT_VARRAY3}
    elif [ "${vpool}" = "rp${rptypetag}_clr" ]; then
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_V1TGT}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_V1TGT_JRNL}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY3} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_V3TGT}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY3} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_V3TGT_JRNL}"

        if [ "${vpool}" = "rpmp_clr" ]; then
            # MP active production journal
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC_JRNL}"
            # MP standby production journal
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_STANDBY_SRC_JRNL}"
            # MP target CDP on varray2
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rp_targets $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_STANDBY_TGT}"
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rp_targets $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_STANDBY_TGT_JRNL}"
        else
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC_JRNL}"
        fi

        run unmanagedvolume ingest_export ${RECOVERPOINT_VARRAY1} rp${rptypetag}_clr $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}" --host ${RP_HOSTNAME}1
        run unmanagedvolume ingest_export ${RECOVERPOINT_VARRAY1} rp${rptypetag}_clr $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}" --host ${RP_HOSTNAME}2
        RECOVERPOINT_TGT_VARRAY=${RECOVERPOINT_VARRAY1}
    fi

    # Verify the CG got created; fail script if this fails.
    run blockconsistencygroup show ViPR-"${RECOVERPOINT_INGEST_EXPORT_VOL1_CG}"

    SRCUID=`volume list ${PROJECT} | grep ${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC} | awk '{print $7}'`

    if [ "${shorttest}" -eq "0" ]; then
        # Run snapshot tests
        recoverpoint_snapshot_test ${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC} ${RECOVERPOINT_TGT_VARRAY} rpsnap5-${RP_MODIFIED_HOSTNAME}-${RANDOM} blocksnap5-${RP_MODIFIED_HOSTNAME}-${RANDOM}
    fi

    # TBD: Add volume group snapshot creation, snap, delete support here.
    #run volumegroup create source-vgroup copy1 COPY
    #run volumegroup add-volumes source-vgroup ${SRCUID}
    #run blocksnapshot create ${PROJECT}/${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC} ${blocksnap}
    #run blocksnapshot activate ${PROJECT}/${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}/${blocksnap}
    #run blocksnapshot delete ${PROJECT}/${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}/${blocksnap}

    # Real removal of volume/CG
    run_noundo export_group delete ${PROJECT}/${RP_HOSTNAME}1
    run_noundo export_group delete ${PROJECT}/${RP_HOSTNAME}2
    run_noundo volume delete ${PROJECT}/"${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}" --wait
    run_noundo blockconsistencygroup delete ViPR-${RECOVERPOINT_INGEST_EXPORT_VOL1_CG}
}

recoverpoint_ingest_tests()
{
    rptypetag=${1}

    # Ingest specific tests
    hecho "RP ingest ${rptypetag} volumes"

    if [ "$RP_CDP" = "1" ] ; then
        recoverpoint_ingest_test rp${rptypetag}_cdp
        recoverpoint_ingest_export_test rp${rptypetag}_cdp
    fi

    if [ "$RP_CRR" = "1" ] ; then
        recoverpoint_ingest_test rp${rptypetag}_crr
        recoverpoint_ingest_export_test rp${rptypetag}_crr
    fi
    
    if [ "$RP_CLR" = "1" ] ; then
        if [ "$RPMP_CLR" = "1" ] ; then
            secho "RP MetroPoint CLR Test is currently not working due to a placement error.  Bypassing."
            return;
        fi
        recoverpoint_ingest_test rp${rptypetag}_clr
        recoverpoint_ingest_export_test rp${rptypetag}_clr        
    fi

    secho "RP ingest ${rptypetag} tests complete"
}

recoverpoint_failover_test()
{
    if [ "$RP_FAILOVER_TESTS" = "1" ]; then
        hecho 'Running RecoverPoint Failover Tests'

        src_volume=${1}
        tgt_volume=${2}

        secho 'Verify the current state of the source and target vols'
        run volume verify ${PROJECT}/${src_volume} personality SOURCE
        run volume verify ${PROJECT}/${src_volume} access_state READWRITE
        run volume verify ${PROJECT}/${src_volume} link_status IN_SYNC
        run volume verify ${PROJECT}/${tgt_volume} personality TARGET
        run volume verify ${PROJECT}/${tgt_volume} access_state NOT_READY
        run volume verify ${PROJECT}/${tgt_volume} link_status IN_SYNC

        secho 'Failover'
        run volume change_link ${PROJECT}/${src_volume} failover ${PROJECT}/${tgt_volume} rp

        secho 'Verify post failover'
        run volume verify ${PROJECT}/${src_volume} personality SOURCE
        run volume verify ${PROJECT}/${src_volume} access_state READWRITE
        run volume verify ${PROJECT}/${src_volume} link_status FAILED_OVER
        run volume verify ${PROJECT}/${tgt_volume} personality TARGET
        run volume verify ${PROJECT}/${tgt_volume} access_state READWRITE
        run volume verify ${PROJECT}/${tgt_volume} link_status FAILED_OVER

        secho 'Cancel the failover'
        run volume change_link ${PROJECT}/${src_volume} failover-cancel ${PROJECT}/${tgt_volume} rp

        secho 'Verify post cancel, everything should be back to the start'
        run volume verify ${PROJECT}/${src_volume} personality SOURCE
        run volume verify ${PROJECT}/${src_volume} access_state READWRITE
        run volume verify ${PROJECT}/${src_volume} link_status IN_SYNC
        run volume verify ${PROJECT}/${tgt_volume} personality TARGET
        run volume verify ${PROJECT}/${tgt_volume} access_state NOT_READY
        run volume verify ${PROJECT}/${tgt_volume} link_status IN_SYNC

        secho 'Failover again'
        run volume change_link ${PROJECT}/${src_volume} failover ${PROJECT}/${tgt_volume} rp

        secho 'Verify post failover'
        run volume verify ${PROJECT}/${src_volume} personality SOURCE
        run volume verify ${PROJECT}/${src_volume} access_state READWRITE
        run volume verify ${PROJECT}/${src_volume} link_status FAILED_OVER
        run volume verify ${PROJECT}/${tgt_volume} personality TARGET
        run volume verify ${PROJECT}/${tgt_volume} access_state READWRITE
        run volume verify ${PROJECT}/${tgt_volume} link_status FAILED_OVER

        secho 'Swap'
        run volume change_link ${PROJECT}/${src_volume} swap ${PROJECT}/${tgt_volume} rp

        secho 'Verify post swap'
        run volume verify ${PROJECT}/${src_volume} personality TARGET
        run volume verify ${PROJECT}/${src_volume} access_state NOT_READY
        run volume verify ${PROJECT}/${src_volume} link_status IN_SYNC
        run volume verify ${PROJECT}/${tgt_volume} personality SOURCE
        run volume verify ${PROJECT}/${tgt_volume} access_state READWRITE
        run volume verify ${PROJECT}/${tgt_volume} link_status IN_SYNC

        # Expand not working yet, might make separate test for just expand
        #run volume expand ${PROJECT}/${tgt_volume} ${RP_SIZE_EXPAND}

        secho 'Failover once again, post swap'
        run volume change_link ${PROJECT}/${tgt_volume} failover ${PROJECT}/${src_volume} rp

        secho 'Verify post failover, post swap'
        run volume verify ${PROJECT}/${src_volume} personality TARGET
        run volume verify ${PROJECT}/${src_volume} access_state READWRITE
        run volume verify ${PROJECT}/${src_volume} link_status FAILED_OVER
        run volume verify ${PROJECT}/${tgt_volume} personality SOURCE
        run volume verify ${PROJECT}/${tgt_volume} access_state READWRITE
        run volume verify ${PROJECT}/${tgt_volume} link_status FAILED_OVER

        secho 'Swap back'
        run volume change_link ${PROJECT}/${tgt_volume} swap ${PROJECT}/${src_volume} rp

        secho 'Verify post swap back, everything should be back to normal'
        run volume verify ${PROJECT}/${src_volume} personality SOURCE
        run volume verify ${PROJECT}/${src_volume} access_state READWRITE
        run volume verify ${PROJECT}/${src_volume} link_status IN_SYNC
        run volume verify ${PROJECT}/${tgt_volume} personality TARGET
        run volume verify ${PROJECT}/${tgt_volume} access_state NOT_READY
        run volume verify ${PROJECT}/${tgt_volume} link_status IN_SYNC
    else
        secho 'Skipping RecoverPoint Failover Tests'
   fi
}

recoverpoint_cg_failover_test()
{
    if [ "$RP_CG_FAILOVER_TESTS" = "1" ]; then
        hecho 'Running RecoverPoint Failover CG Tests'

        # The name of the consistency group to swap
        cg_name=${1}
        # The source source volume - an arbitrary source volume from the CG
        src_volume=${2}
        # The virtual array corresonding to the source volume (src_volume)
        src_varray=${3}
        # The target volume name used to identify all target volumes
        tgt_volumes=${4}
        # The swap target virtual arrays
        tgt_varrays=${5}

        secho 'RP consistency group failover/swap tests'

        # Test the CG swap and swap-back for every target virtual array
        for tgt_varray in ${tgt_varrays}
        do
          secho 'Verify the current state of the source and target vols'
          run volume verify ${PROJECT}/${src_volume} personality SOURCE
          run volume verify ${PROJECT}/${src_volume} access_state READWRITE
          run volume verify ${PROJECT}/${src_volume} link_status IN_SYNC
      
          for tgt_volume in ${tgt_volumes}
          do
            run volume verify ${PROJECT}/${tgt_volume} personality TARGET
            run volume verify ${PROJECT}/${tgt_volume} access_state NOT_READY
            run volume verify ${PROJECT}/${tgt_volume} link_status IN_SYNC
          done
      
          run blockconsistencygroup failover $cg_name --copyType rp --targetVarray $tgt_varray
      
          run volume verify ${PROJECT}/${src_volume} personality SOURCE
          run volume verify ${PROJECT}/${src_volume} access_state READWRITE
          run volume verify ${PROJECT}/${src_volume} link_status FAILED_OVER
      
          # Post-failover target verification
          for tgt_volume in ${tgt_volumes}
          do
            # If the volume corresponds to the target varray used for the failover we
            # need to verify different values
            if [[ $tgt_volume == *$tgt_varray* ]]; then
                run volume verify ${PROJECT}/${tgt_volume} personality TARGET
                run volume verify ${PROJECT}/${tgt_volume} access_state READWRITE
                run volume verify ${PROJECT}/${tgt_volume} link_status FAILED_OVER
            else
                run volume verify ${PROJECT}/${tgt_volume} personality TARGET
                run volume verify ${PROJECT}/${tgt_volume} access_state NOT_READY
                run volume verify ${PROJECT}/${tgt_volume} link_status IN_SYNC
            fi
          done
      
          run blockconsistencygroup failover_cancel $cg_name --copyType rp --targetVarray $tgt_varray

          run volume verify ${PROJECT}/${src_volume} personality SOURCE
          run volume verify ${PROJECT}/${src_volume} access_state READWRITE
          run volume verify ${PROJECT}/${src_volume} link_status IN_SYNC
      
          for tgt_volume in ${tgt_volumes}
          do
            run volume verify ${PROJECT}/${tgt_volume} personality TARGET
            run volume verify ${PROJECT}/${tgt_volume} access_state NOT_READY
            run volume verify ${PROJECT}/${tgt_volume} link_status IN_SYNC
          done
      
          run blockconsistencygroup failover $cg_name --copyType rp --targetVarray $tgt_varray
      
          run volume verify ${PROJECT}/${src_volume} personality SOURCE
          run volume verify ${PROJECT}/${src_volume} access_state READWRITE
          run volume verify ${PROJECT}/${src_volume} link_status FAILED_OVER
      
          # Post-failover target verification
          for tgt_volume in ${tgt_volumes}
          do
            # If the volume corresponds to the target varray used for the failover we
            # need to verify different values
            if [[ $tgt_volume == *$tgt_varray* ]]; then
                run volume verify ${PROJECT}/${tgt_volume} personality TARGET
                run volume verify ${PROJECT}/${tgt_volume} access_state READWRITE
                run volume verify ${PROJECT}/${tgt_volume} link_status FAILED_OVER
            else
                run volume verify ${PROJECT}/${tgt_volume} personality TARGET
                run volume verify ${PROJECT}/${tgt_volume} access_state NOT_READY
                run volume verify ${PROJECT}/${tgt_volume} link_status IN_SYNC
            fi
          done
      
          run blockconsistencygroup swap $cg_name --copyType rp --targetVarray $tgt_varray

          # Post-swap source verification
          run volume verify ${PROJECT}/${src_volume} personality TARGET
          run volume verify ${PROJECT}/${src_volume} access_state NOT_READY
          run volume verify ${PROJECT}/${src_volume} link_status IN_SYNC
      
          # Post-swap target verification
          for tgt_volume in ${tgt_volumes}
          do
            run volume verify ${PROJECT}/${tgt_volume} personality SOURCE
            run volume verify ${PROJECT}/${tgt_volume} access_state READWRITE
            run volume verify ${PROJECT}/${tgt_volume} link_status IN_SYNC
          done
      
          run blockconsistencygroup failover $cg_name --copyType rp --targetVarray $tgt_varray

          run volume verify ${PROJECT}/${src_volume} personality TARGET
          run volume verify ${PROJECT}/${src_volume} access_state READWRITE
          run volume verify ${PROJECT}/${src_volume} link_status FAILED_OVER
      
          for tgt_volume in ${tgt_volumes}
          do
            # If the volume corresponds to the target varray used for the failover we
            # need to verify different values
            if [[ $tgt_volume == *$tgt_varray* ]]; then
                run volume verify ${PROJECT}/${tgt_volume} personality SOURCE
                run volume verify ${PROJECT}/${tgt_volume} access_state READWRITE
                run volume verify ${PROJECT}/${tgt_volume} link_status FAILED_OVER
            else
                run volume verify ${PROJECT}/${tgt_volume} personality SOURCE
                run volume verify ${PROJECT}/${tgt_volume} access_state NOT_READY
                run volume verify ${PROJECT}/${tgt_volume} link_status IN_SYNC
            fi
          done
      
          run blockconsistencygroup swap $cg_name --copyType rp --targetVarray $src_varray

          # swap-back source verification
          run volume verify ${PROJECT}/${src_volume} personality SOURCE
          run volume verify ${PROJECT}/${src_volume} access_state READWRITE
          run volume verify ${PROJECT}/${src_volume} link_status IN_SYNC
      
          # swap-back target verification
          for tgt_volume in ${tgt_volumes}
          do
            run volume verify ${PROJECT}/${tgt_volume} personality TARGET
            run volume verify ${PROJECT}/${tgt_volume} access_state NOT_READY
            run volume verify ${PROJECT}/${tgt_volume} link_status IN_SYNC
          done
        done
    else
        secho 'Skipping RecoverPoint Failover CG Tests'
    fi
}

recoverpoint_direct_access_test()
{
    if [ "$RP_DIRECT_ACCESS_TESTS" = "1" ]; then
        hecho 'Running RecoverPoint Direct Access Tests'

        # The source source volume - an arbitrary source volume from the CG
        src_volume=${1}
        # The target virtual arrays
        rp_tgt_varrays=${2}
        # The source virtual array
        rp_src_varray=${3}
        # The source virtual pool
        rp_src_vpool=${4}
        # The RP consistency group
        consistency_group=${5}
        # RecoverPoint type
        rp_type=${6}

        rp_bookmark=direct_access_bookmark-${RP_MODIFIED_HOSTNAME}-${RANDOM}
        array_snapshot=direct_access_array_snap-${RP_MODIFIED_HOSTNAME}-${RANDOM}
        direct_access_mode_varray=${rp_tgt_varrays[0]}
        tgt_volume=${src_volume}-target-${direct_access_mode_varray}
        new_volume=${src_volume}-newvol

        secho 'Setting direct access mode on target copy pre-failover - expected failure'
        fail volume change_link ${PROJECT}/${src_volume} accessmode ${PROJECT}/${tgt_volume} rp --am DIRECT_ACCESS
        
        secho 'Setting access mode to some unknown value on target copy - expected failure'
        fail volume change_link ${PROJECT}/${src_volume} accessmode ${PROJECT}/${tgt_volume} rp --am UNKNOWN

        secho 'Setting access mode with no access mode value - expected failure'
        fail volume change_link ${PROJECT}/${src_volume} accessmode ${PROJECT}/${tgt_volume} rp

        secho 'Creating RP bookmark prior to direct access mode being set'
        run blocksnapshot create ${PROJECT}/${src_volume} ${rp_bookmark} --type rp
		
        secho 'Failover'
        run volume change_link ${PROJECT}/${src_volume} failover ${PROJECT}/${tgt_volume} rp		

        secho 'Verify the post failover state of the source and target vols'
        run volume verify ${PROJECT}/${src_volume} personality SOURCE
        run volume verify ${PROJECT}/${src_volume} access_state READWRITE
        run volume verify ${PROJECT}/${src_volume} link_status FAILED_OVER
        run volume verify ${PROJECT}/${tgt_volume} personality TARGET
        run volume verify ${PROJECT}/${tgt_volume} access_state READWRITE
        run volume verify ${PROJECT}/${tgt_volume} link_status FAILED_OVER

        secho 'Setting access mode to direct access'
        run volume change_link ${PROJECT}/${src_volume} accessmode ${PROJECT}/${tgt_volume} rp --am DIRECT_ACCESS

        secho 'Setting access mode to direct access a second time'
        run volume change_link ${PROJECT}/${src_volume} accessmode ${PROJECT}/${tgt_volume} rp --am DIRECT_ACCESS

        # TODO: Would be nice to verify the copy is in DIRECT_ACCESS mode but there is currently no way to do
        # that querying volume attributes.

        for tgt_varray in ${rp_tgt_varrays[@]}
        do
            if [ "$tgt_varray" == "$direct_access_mode_varray" ]; then
                secho 'Verifying that the bookmark associated with the target copy in direct access mode has been removed'
                fail blocksnapshot show ${PROJECT}/${src_volume}/${rp_bookmark}-${direct_access_mode_varray}
            else
                secho 'Verifying that the bookmark associated with target copy NOT in direct access mode still exists'
                run blocksnapshot show ${PROJECT}/${src_volume}/${rp_bookmark}-${tgt_varray}
            fi
        done	 

        # Test creating snapshots after target copy is in direct access mode
        if [ "${#rp_tgt_varrays[@]}" -eq "1" ]; then
            secho 'Attempt to create a RP bookmark on CG with single copy in direct access mode - expected failure'
            fail blocksnapshot create ${PROJECT}/${src_volume} ${rp_bookmark} --type rp
        else 
            secho 'Creating RP bookmark when 1 CG copy is in direct access mode'
            run blocksnapshot create ${PROJECT}/${src_volume} ${rp_bookmark}-2 --type rp

            for tgt_varray in ${rp_tgt_varrays[@]}
            do
                if [ "$tgt_varray" == "$direct_access_mode_varray" ]; then
                    secho 'Verifying that the bookmark associated with the target copy has not been created'
                    fail blocksnapshot show ${PROJECT}/${src_volume}/${rp_bookmark}-2-${direct_access_mode_varray}
                else
                    secho 'Verifying that the bookmark associated with target copy NOT in direct access mode exists'
                    run blocksnapshot show ${PROJECT}/${src_volume}/${rp_bookmark}-2-${tgt_varray}
                fi
            done
        fi

        if [[ "$rp_type" = "" || "$RP_VOLUME_COUNT" = "1" ]]; then
            # Only run these tests for single volume tests or non-VPlex.  RP+VPlex with multiple volumes creates unpredicable
            # local array snapshot names so it's hard to match up the source volume name with the snapshot name
            secho 'Creating local array snapshot of source volume when one target copy is in direct access mode'
            run blocksnapshot create ${PROJECT}/${src_volume} ${array_snapshot} --type native
            secho 'Verifying the local array snapshot of the source volume was created'
            run blocksnapshot show ${PROJECT}/${src_volume}/${array_snapshot}-1
            secho 'Removing array snapshot after it has been verified'
            run blocksnapshot delete ${PROJECT}/${src_volume}/${array_snapshot}-1
        fi

        for tgt_varray in ${rp_tgt_varrays[@]}
        do
            if [ "$tgt_varray" == "$direct_access_mode_varray" ]; then
                secho 'Attempt to create an array snapshot of target volume whose copy is in direct access mode - expected failure'
                fail blocksnapshot create ${PROJECT}/${src_volume}-target-${tgt_varray} ${array_snapshot} --type native
            else
                secho 'Attempt to create an array snapshot of target volume whose copy is not in direct access mode'
                run blocksnapshot create ${PROJECT}/${src_volume}-target-${tgt_varray} ${array_snapshot} --type native 
                secho 'Verifying that the array snapshot associated with target copy NOT in direct access mode exists'
                run blocksnapshot show ${PROJECT}/${src_volume}-target-${tgt_varray}/${array_snapshot}-1
                secho 'Removing array snapshot after it has been verified'
                run blocksnapshot delete ${PROJECT}/${src_volume}-target-${tgt_varray}/${array_snapshot}-1
            fi 
        done 

        if [ "${#rp_tgt_varrays[@]}" -gt "1" ]; then
            secho 'Add replication set to consistency group while a copy is in direct access mode'
            run volume create ${new_volume} $PROJECT ${rp_src_varray} ${rp_src_vpool} $RP_VOLUME_SIZE --consistencyGroup ${consistency_group} --count=1
            secho 'Ensure the target volume on the direct access RP copy has the appropriate access state and link status'
            run volume verify ${PROJECT}/${new_volume}-target-${direct_access_mode_varray} access_state READWRITE
            run volume verify ${PROJECT}/${new_volume}-target-${direct_access_mode_varray} link_status FAILED_OVER
            secho 'Remove replication set from consistency group while a copy is in direct access mode (failover) - expected failure'		
            fail volume delete $PROJECT/${new_volume} --wait
        fi
        
        secho 'Expand source volume when one of the RP target copies is in direct access mode'
        run volume expand ${PROJECT}/${src_volume} ${RP_SIZE_EXPAND}

        secho 'Export/unexport target volume whose copy is in DIRECT_ACCESS mode'
        recoverpoint_export_host_setup ${rp_src_vpool} ${rp_type}
        sleep 20
        run export_group create ${PROJECT} ${RP_EXPORT_GROUP} ${direct_access_mode_varray} --type Host --volspec ${PROJECT}/${tgt_volume} --hosts "${RP_HOSTNAME}1"
        run export_group delete ${PROJECT}/${RP_EXPORT_GROUP}

        secho 'Removing export host '${RP_HOSTNAME}1
        run hosts delete ${RP_HOSTNAME}1
        secho 'Removing export host '${RP_HOSTNAME}2
        run hosts delete ${RP_HOSTNAME}2

        secho 'Swap/swap-back while single target volume is in direct access mode'
        run volume change_link ${PROJECT}/${src_volume} swap ${PROJECT}/${tgt_volume} rp       
        sleep 10
        run volume change_link ${PROJECT}/${tgt_volume} swap ${PROJECT}/${src_volume} rp

        # Failover to each of the remaining target copies and set access mode to direct access.  Test swap/swap-back.
        if [ "${#rp_tgt_varrays[@]}" -gt "1" ]; then
            for tgt_varray in ${rp_tgt_varrays[@]}
            do
                if [ "$tgt_varray" != "$direct_access_mode_varray" ]; then        
                    target_vol=${src_volume}-target-${tgt_varray}
                    secho 'Failover to target virtual array '${tgt_varray}
                    run volume change_link ${PROJECT}/${src_volume} failover ${PROJECT}/${target_vol} rp
                    secho 'Setting access mode to direct access for target virtual array '${tgt_varray}
                    run volume change_link ${PROJECT}/${src_volume} accessmode ${PROJECT}/${target_vol} rp --am DIRECT_ACCESS
                    
                    secho 'Swap/swap-back while each target copies are in direct access mode'
                    run volume change_link ${PROJECT}/${src_volume} swap ${PROJECT}/${target_vol} rp
                    sleep 10
                    run volume change_link ${PROJECT}/${target_vol} swap ${PROJECT}/${src_volume} rp
                fi
            done
        fi
       
        secho 'Failover'
        sleep 10
        run volume change_link ${PROJECT}/${src_volume} failover ${PROJECT}/${tgt_volume} rp
        secho 'Setting access mode to direct access'
        run volume change_link ${PROJECT}/${src_volume} accessmode ${PROJECT}/${tgt_volume} rp --am DIRECT_ACCESS
 
        secho 'Failover Cancel - returning copy image acess to no_access for target copy '${direct_access_mode_varray}
        run volume change_link ${PROJECT}/${src_volume} failover-cancel ${PROJECT}/${tgt_volume} rp

        secho 'Verify the current state of the source and target vols'
        run volume verify ${PROJECT}/${src_volume} personality SOURCE
        run volume verify ${PROJECT}/${src_volume} access_state READWRITE
        run volume verify ${PROJECT}/${src_volume} link_status IN_SYNC
        run volume verify ${PROJECT}/${tgt_volume} personality TARGET
        run volume verify ${PROJECT}/${tgt_volume} access_state NOT_READY
        run volume verify ${PROJECT}/${tgt_volume} link_status IN_SYNC

        secho 'Failover using consistency group API'
        run blockconsistencygroup failover $consistency_group --copyType rp --targetVarray ${direct_access_mode_varray}
        secho 'Setting access mode to direct access using consistency group API'
        run blockconsistencygroup accessmode $consistency_group --copyType rp --targetVarray ${direct_access_mode_varray} --am DIRECT_ACCESS

        secho 'Failover Cancel - returning copy image acess to no_access for target copy '${direct_access_mode_varray}
        run blockconsistencygroup failover_cancel $consistency_group --copyType rp --targetVarray ${direct_access_mode_varray}

        secho 'Verify the current state of the source and target vols'
        run volume verify ${PROJECT}/${src_volume} personality SOURCE
        run volume verify ${PROJECT}/${src_volume} access_state READWRITE
        run volume verify ${PROJECT}/${src_volume} link_status IN_SYNC
        run volume verify ${PROJECT}/${tgt_volume} personality TARGET
        run volume verify ${PROJECT}/${tgt_volume} access_state NOT_READY
        run volume verify ${PROJECT}/${tgt_volume} link_status IN_SYNC
   else
        secho 'Skipping RecoverPoint Direct Access Tests'
   fi
}

recoverpoint_change_vpool_test()
{
    # Run change vpool tests on the volume if the flag is set.
    # These tests will:
    #  1. Remove protection from the RP volume.
    #  2. Add protection to the now unprotected volume.
    #  3. Upgrade to MP (only in the case of MP CRR tests - for now)
    #  4. VPLEX Data Migrations (only for RP+VPLEX and MP volumes)

    if [ "$RP_CHANGE_VPOOL" = "1" ]; then
        hecho 'Running RecoverPoint Change Virtual Pool Tests'

        base_volume=${1}
        noprotection_vpool=${2}
        rp_vpool=${3}
        rp_cg=${4}
        rp_migrate_vpool=${5}
        upgrade_to_mp=${6}

        # Perform Upgrade to MP if the variable is present,
        # Only RP+VPLEX Distributed CRR is eligible for this operation (for now).
        if [ a${upgrade_to_mp} != a ] ; then
            secho 'Change Virtual Pool - Upgrade To MetroPoint'
            # Temporarily disabled
            #run volume change_cos ${PROJECT}/${base_volume} ${upgrade_to_mp}
        fi

        if [ a${rp_migrate_vpool} != a ] ; then
            secho 'Change Virtual Pool - VPLEX Data Migration (RecoverPoint)'
            run volume change_cos ${PROJECT}/${base_volume} ${rp_migrate_vpool}

            secho 'Change Virtual Pool - Remove RecoverPoint Protection'
            run volume change_cos ${PROJECT}/${base_volume} ${noprotection_vpool}'-migrate'

            secho 'Change Virtual Pool - Plain VPLEX Migration to prepare for adding RP Protection'
        else
            secho 'Change Virtual Pool - Remove RecoverPoint Protection'
        fi

        # This call will either remove protection from the RP volume OR perform
        # a plain VPLEX Data Migration depending on what has occured before
        # getting here.
        run volume change_cos ${PROJECT}/${base_volume} ${noprotection_vpool}

        secho 'Change Virtual Pool - Add RecoverPoint Protection'
        run volume change_cos ${PROJECT}/${base_volume} ${rp_vpool} --consistencyGroup ${rp_cg}

    else
        secho 'Skipping RecoverPoint Change Virtual Pool Tests'
    fi
}

recoverpoint_vpool_change_replication_mode_test()
{
    if [ "$RP_CHANGE_REPLICATION_MODE_TESTS" = "1" ]; then
        hecho 'Running RecoverPoint Change Replication Mode Tests'

        base_volume=${1}
        target_vpool=${2}

        secho 'Change Virtual Pool - Update Replication Mode'
        run volume change_cos ${PROJECT}/${base_volume} ${target_vpool}
    else
        secho 'Skipping RecoverPoint Change Replication Mode Tests'
    fi
}

recoverpoint_add_journal_volume_test()
{
    if [ "$RP_ADD_JOURNAL_TESTS" = "1" ]; then    
        hecho 'Running RecoverPoint Add Journal Volume Tests'

        copy_name=${1}
        journ_varray=${2} 
        journ_vpool=${3}     
        rp_cg=${4}

        secho 'RP add journal volume'
        run volume add_journal ${copy_name} $PROJECT ${journ_varray} ${journ_vpool} 10GB --consistencyGroup ${rp_cg} --count=1  
    else
        secho 'Skipping RecoverPoint Add Journal Volume Tests'
    fi
}

recoverpoint_cleanup_volumes()
{
    vol=${1}
    volcount=${2}

    # Cleanup volumes
    if [ "$volcount" = "1" ]; then
        run volume delete $PROJECT/${vol} --wait
    else
        for (( i=1; i<=$volcount; i++ ))
        do
            run volume delete $PROJECT/${vol}-$i --wait
        done
    fi
}

recoverpoint_cleanup_cgs()
{
    # Cleanup CGs
    if [ "${RP_INGESTTEST}" = "0" ]; then
        if [ "${RP_TESTS}" = "1" ]; then
            run blockconsistencygroup delete $RP_CONSISTENCY_GROUP
        fi

        if [ "${RPVPLEX_TESTS}" = "1" ]; then
            run blockconsistencygroup delete $RP_VPLEX_CONSISTENCY_GROUP
        fi

        if [ "${RPMP_TESTS}" = "1" ]; then
            run blockconsistencygroup delete $RP_METROPOINT_CONSISTENCY_GROUP
        fi

        if [ "${RPXIO_TESTS}" = "1" ]; then
            run blockconsistencygroup delete $RP_XIO_CONSISTENCY_GROUP
        fi
    fi
}
#########################
# End RP test functions #
#########################

#########################
# Main RP test function #
#########################
recoverpoint_tests()
{
    if [ "$RP_RUN_TESTS" = "1" ]; then
        hecho 'Run RP tests'

        # Setup CGs
        rp_cg_setup

        # Default source varrays
        rp_src_varray=$RECOVERPOINT_VARRAY1
        rpvplex_src_varray=$RECOVERPOINT_VARRAY1
        rpmp_src_varray=$RECOVERPOINT_VARRAY1
        rpxio_src_varray=$RECOVERPOINT_VARRAY1

        # Vpools for change vpool - Remove Protection
        rp_noprotection_vpool='rp_noprotection'
        rpvplex_noprotection_vpool='rpvplexdist_noprotection'
        rpmp_noprotection_vpool='rpvplexdist_noprotection'
        rpxio_noprotection_vpool='rp_noprotection'

        rp_type=''

        if [ "$RP_CDP" = "1" ]; then
            hecho 'RP CDP Tests'
            rp_type='cdp'

            # RP
            rp_tgt_varray=$RECOVERPOINT_VARRAY1
            rp_tgt_varrays=($RECOVERPOINT_VARRAY1)
            rp_tgt_vpool='rp_targets'

            # RP+VPLEX
            rpvplex_tgt_varray=$RECOVERPOINT_VARRAY1
            rpvplex_tgt_varrays=($RECOVERPOINT_VARRAY1)
            rpvplex_tgt_vpool='rpvplex_targets'
            # RP+VPLEX CDP uses VPLEX local, set as the no protection vpool
            rpvplex_noprotection_vpool='rpvplexlocal_noprotection'

            # MP
            rpmp_active_tgt_varray=$RECOVERPOINT_VARRAY1
            rpmp_standby_tgt_varray=$RECOVERPOINT_VARRAY2
            rpmp_tgt_varrays=($RECOVERPOINT_VARRAY1 $RECOVERPOINT_VARRAY2)
            rpmp_tgt_vpool='rp_targets'
            
            # XIO
            rpxio_tgt_varray=$RECOVERPOINT_VARRAY1
            rpxio_tgt_varrays=($RECOVERPOINT_VARRAY1)
            rpxio_tgt_vpool='rpxio_targets'
        fi

        if [ "$RP_CRR" = "1" ]; then
            hecho 'RP CRR Tests'
            rp_type='crr'

            # RP
            rp_tgt_varray=$RECOVERPOINT_VARRAY3
            rp_tgt_varrays=($RECOVERPOINT_VARRAY3)
            rp_tgt_vpool='rp_targets'

            # RP+VPLEX
            rpvplex_tgt_varray=$RECOVERPOINT_VARRAY3
            rpvplex_tgt_varrays=($RECOVERPOINT_VARRAY3)
            rpvplex_tgt_vpool='rpvplex_targets'

            # MP
            rpmp_active_tgt_varray=$RECOVERPOINT_VARRAY3
            rpmp_tgt_varrays=($RECOVERPOINT_VARRAY3)
            rpmp_standby_tgt_varray=''
            rpmp_tgt_vpool='rp_targets'

            # XIO
            rpxio_tgt_varray=$RECOVERPOINT_VARRAY1
            rpxio_tgt_varrays=($RECOVERPOINT_VARRAY1)
            rpxio_tgt_vpool='rpxio_targets'
        fi

        if [ "$RP_CLR" = "1" ]; then
            hecho 'RP CLR Tests'
            rp_type='clr'

            # RP
            rp_tgt_varray=$RECOVERPOINT_VARRAY3
            rp_tgt_varrays=($RECOVERPOINT_VARRAY1 $RECOVERPOINT_VARRAY3)
            rp_tgt_vpool='rp_targets'

            # RP+VPLEX
            rpvplex_tgt_varray=$RECOVERPOINT_VARRAY1
            rpvplex_tgt_varrays=($RECOVERPOINT_VARRAY1 $RECOVERPOINT_VARRAY3)
            rpvplex_tgt_vpool='rpvplex_targets'

            # MP
            rpmp_active_tgt_varray=$RECOVERPOINT_VARRAY3
            rpmp_tgt_varrays=($RECOVERPOINT_VARRAY1 $RECOVERPOINT_VARRAY2 $RECOVERPOINT_VARRAY3)
            rpmp_standby_tgt_varray=''
            rpmp_tgt_vpool='rp_targets'

            # XIO
            rpxio_tgt_varray=$RECOVERPOINT_VARRAY3
            rpxio_tgt_varrays=($RECOVERPOINT_VARRAY1 $RECOVERPOINT_VARRAY3)
            rpxio_tgt_vpool='rpxio_targets'
        fi

        # Have to account for "-1" being added to volume name if
        # volume count is greater than 1.
        volume_name_modifier=''
        if [ "$RP_VOLUME_COUNT" = "1" ]; then
            volume_name_modifier=''
        else
            volume_name_modifier='-1'
        fi

        # RP
        rp_src_vpool='rp_'$rp_type
        rp_src_vpool_sync=${rp_src_vpool}'-sync'
        rpvolume=${RP_VOLUME}
        rpvolumetarget=${rpvolume}${volume_name_modifier}'-target-'${rp_tgt_varray}

        # RP+VPLEX
        rpvplex_src_vpool='rpvplex_'$rp_type
        rpvplex_src_vpool_sync=${rpvplex_src_vpool}'-sync'
        rpvplex_migrate_vpool=${rpvplex_src_vpool}'-migrate'
        rpvplexvolume=${RP_VPLEX_VOLUME}
        rpvplexvolumetarget=${rpvplexvolume}${volume_name_modifier}'-target-'${rpvplex_tgt_varray}

        # MP
        rpmp_src_vpool='rpmp_'$rp_type
        rpmp_src_vpool_sync=${rpmp_src_vpool}'-sync'
        rpmp_migrate_vpool=${rpmp_src_vpool}'-migrate'
        rpmpvolume=${RP_METROPOINT_VOLUME}
        rpmpvolumetarget=${mpvolume}${volume_name_modifier}'-target-'${rpmp_active_tgt_varray}

        # XIO
        rpxio_src_vpool='rpxio_'$rp_type
        rpxio_src_vpool_sync=${rpxio_src_vpool}'-sync'
        rpxiovolume=${RP_XIO_VOLUME}
        rpxiovolumetarget=${xiovolume}${volume_name_modifier}'-target-'${rpxio_active_tgt_varray}

        # Ingestion Tests
        if [ "$RP_INGESTTESTS" = "1" ]; then
            if [ "$RP_TESTS" = "1" ]; then
                recoverpoint_ingest_tests ""
            fi

            if [ "$RPVPLEX_TESTS" = "1" ]; then
                recoverpoint_ingest_tests "vplex"
            fi
            
            if [ "${RPMP_TESTS}" = "1" ]; then
                recoverpoint_ingest_tests "mp"
            fi

            if [ "${RPXIO_TESTS}" = "1" ]; then
                recoverpoint_ingest_tests "xio"
            fi
            
            # For now, if ingestion is turned on, complete the tests.  Once everything is stable, ingestion should just be part of normal RP testing.
            return
        fi

        # RP Volume create
        if [ "$RP_TESTS" = "1" ]; then
            secho 'RP volume create...'
            run volume create ${rpvolume} $PROJECT ${rp_src_varray} ${rp_src_vpool} $RP_VOLUME_SIZE --consistencyGroup $RP_CONSISTENCY_GROUP --count=$RP_VOLUME_COUNT
        fi

        if [ "$RPVPLEX_TESTS" = "1" ]; then
            secho 'RP+VPLEX volume create...'
            run volume create ${rpvplexvolume} $PROJECT ${rpvplex_src_varray} ${rpvplex_src_vpool} $RP_VOLUME_SIZE --consistencyGroup $RP_VPLEX_CONSISTENCY_GROUP --count=$RP_VOLUME_COUNT
        fi

        if [ "$RPMP_TESTS" = "1" ]; then
            secho 'MetroPoint volume create...'
            run volume create ${rpmpvolume} $PROJECT ${rpmp_src_varray} ${rpmp_src_vpool} $RP_VOLUME_SIZE --consistencyGroup $RP_METROPOINT_CONSISTENCY_GROUP --count=$RP_VOLUME_COUNT
        fi

        if [ "$RPXIO_TESTS" = "1" ]; then
            secho 'XIO volume create...'
            run volume create ${rpxiovolume} $PROJECT ${rpxio_src_varray} ${rpxio_src_vpool} $RP_VOLUME_SIZE --consistencyGroup $RP_XIO_CONSISTENCY_GROUP --count=$RP_VOLUME_COUNT
        fi

        # Compile a list of target volume names that have been created for RP
        rp_tgt_volumes=()
        for tgt in ${rp_tgt_varrays}
        do
          rp_tgt_volumes=("${rp_tgt_volumes[@]}" "${rpvolume}${volume_name_modifier}-target-${tgt}")
        done
        
        # Compile a list of target volume names that have been created for RP+VPlex
        rp_vplex_tgt_volumes=()
        for tgt in ${rpvplex_tgt_varrays}
        do
          rp_vplex_tgt_volumes=("${rp_vplex_tgt_volumes[@]}" "${rpvplexvolume}${volume_name_modifier}-target-${tgt}")
        done
        
        # Compile a list of target volume names that have been created for MetroPoint
        rpmp_tgt_volumes=()
        for tgt in ${rpmp_tgt_varrays}
        do
          rpmp_tgt_volumes=("${rpmp_tgt_volumes[@]}" "${rpmpvolume}${volume_name_modifier}-target-${tgt}")
        done

        # RP test cases
        # Run these test cases unless specified to only run create/delete tests
        if [ "$RP_RUN_VOLUME_CREATE_ONLY" = "0" ]; then
            if [ "$RP_TESTS" = "1" ]; then
                secho 'RP tests...'

                # Change Virtual Pool Test
                recoverpoint_change_vpool_test ${rpvolume}${volume_name_modifier} ${rp_noprotection_vpool} ${rp_src_vpool} ${RP_CONSISTENCY_GROUP}

                # Add Journal Test
                recoverpoint_add_journal_volume_test ${rp_tgt_varray} ${rp_tgt_varray} ${rp_tgt_vpool} ${RP_CONSISTENCY_GROUP}
                
                # Snapshot/Bookmark Test
                if [ "${RP_CLR}" = "1" ]; then
                    recoverpoint_auto_snapshot_cleanup_test ${rpvolume}${volume_name_modifier} ${rp_tgt_volumes[0]} ${rp_tgt_varrays[0]} ${RP_CONSISTENCY_GROUP} ${rp_tgt_volumes[1]}
                else
                    recoverpoint_auto_snapshot_cleanup_test ${rpvolume}${volume_name_modifier} ${rp_tgt_volumes[0]} ${rp_tgt_varrays[0]} ${RP_CONSISTENCY_GROUP}
                fi

                # Failover Test
                recoverpoint_failover_test ${rpvolume}${volume_name_modifier} ${rp_tgt_volumes[0]}

                # Direct Access Failover Test
                recoverpoint_direct_access_test ${rpvolume}${volume_name_modifier} ${rp_tgt_varrays} ${rp_src_varray} ${rp_src_vpool} ${RP_CONSISTENCY_GROUP} "" 

                # CG Failover/Swap Test
                recoverpoint_cg_failover_test ${RP_CONSISTENCY_GROUP} ${rpvolume}${volume_name_modifier} ${rp_src_varray} ${rp_tgt_volumes} ${rp_tgt_varrays}

                # Change Replication Mode Test
                recoverpoint_vpool_change_replication_mode_test ${rpvolume}${volume_name_modifier} ${rp_src_vpool_sync}
                recoverpoint_vpool_change_replication_mode_test ${rpvolume}${volume_name_modifier} ${rp_src_vpool}

                # RP Export Bookmark Test
                recoverpoint_export_bookmark_tests ${rpvolume} $RECOVERPOINT_VARRAY1 ${rp_tgt_varrays} ${rp_src_vpool} ""
            fi

            # RP+VPLEX test cases
            if [ "$RPVPLEX_TESTS" = "1" ]; then
                secho 'RP+VPLEX tests...'

                # Change Virtual Pool Test
                if [ "$RP_CRR" = "1" ]; then
                    # Upgrade to MetroPoint change vpool test
                    recoverpoint_change_vpool_test ${rpvplexvolume}${volume_name_modifier} ${rpvplex_noprotection_vpool} ${rpvplex_src_vpool} ${RP_VPLEX_CONSISTENCY_GROUP} ${rpvplex_migrate_vpool} rpmp_crr-upgradeMP
                else
                    recoverpoint_change_vpool_test ${rpvplexvolume}${volume_name_modifier} ${rpvplex_noprotection_vpool} ${rpvplex_src_vpool} ${RP_VPLEX_CONSISTENCY_GROUP} ${rpvplex_migrate_vpool}
                fi

                # Add Journal Test
                recoverpoint_add_journal_volume_test ${rpvplex_tgt_varray} ${rpvplex_tgt_varray} ${rpvplex_tgt_vpool} ${RP_VPLEX_CONSISTENCY_GROUP}

                # Snapshot/Bookmark Test
                recoverpoint_auto_snapshot_cleanup_test ${rpvplexvolume}${volume_name_modifier} ${rp_vplex_tgt_volumes[0]} ${rpvplex_tgt_varrays[0]} ${RP_VPLEX_CONSISTENCY_GROUP}

                # Failover Test
                recoverpoint_failover_test ${rpvplexvolume}${volume_name_modifier} ${rp_vplex_tgt_volumes[0]}

                # Direct Access Failover Test
                recoverpoint_direct_access_test ${rpvplexvolume}${volume_name_modifier} ${rpvplex_tgt_varrays} ${rpvplex_src_varray} ${rpvplex_src_vpool} ${RP_VPLEX_CONSISTENCY_GROUP} "vplex"

                # CG Failover/Swap Test
                recoverpoint_cg_failover_test ${RP_VPLEX_CONSISTENCY_GROUP} ${rpvplexvolume}${volume_name_modifier} ${rpvplex_src_varray} ${rp_vplex_tgt_volumes} ${rp_tgt_varrays}

                # Change Replication Mode Test
                secho 'RP change vpool - replication mode, not supported on RP/VPLEX yet'
                # recoverpoint_vpool_change_replication_mode_test ${rpvplexvolume}${volume_name_modifier} ${rpvplex_src_vpool_sync}
                # recoverpoint_vpool_change_replication_mode_test ${rpvplexvolume}${volume_name_modifier} ${rpvplex_src_vpool}

                # RP Export Bookmark Test
                recoverpoint_export_bookmark_tests ${rpvplexvolume} $RECOVERPOINT_VARRAY1 ${rp_tgt_varrays} ${rpvplex_src_vpool} "vplex"
            fi

            # MP test cases
            if [ "$RPMP_TESTS" = "1" ]; then
                secho 'MetroPoint tests...'

                # Change Virtual Pool Test
                recoverpoint_change_vpool_test ${rpmpvolume}${volume_name_modifier} ${rpmp_noprotection_vpool} ${rpmp_src_vpool} ${RP_METROPOINT_CONSISTENCY_GROUP} ${rpmp_migrate_vpool}

                # Add Journal Test
                recoverpoint_add_journal_volume_test ${rpmp_active_tgt_varray} ${rpmp_active_tgt_varray} ${rpmp_tgt_vpool} ${RP_METROPOINT_CONSISTENCY_GROUP}

                # Snapshot/Bookmark Test
                recoverpoint_auto_snapshot_cleanup_test ${rpmpvolume}${volume_name_modifier} ${rpmp_tgt_volumes[0]} ${rpmp_tgt_varrays[0]} ${RP_METROPOINT_CONSISTENCY_GROUP}

                # Failover Test
                recoverpoint_failover_test ${rpmpvolume}${volume_name_modifier} ${rpmp_tgt_volumes[0]}

                # Direct Access Failover Test
                recoverpoint_direct_access_test ${rpmpvolume}${volume_name_modifier} ${rpmp_tgt_varrays} ${rpmp_src_varray} ${rpmp_src_vpool} ${RP_METROPOINT_CONSISTENCY_GROUP} "mp"

                # CG Failover/Swap Test
                recoverpoint_cg_failover_test ${RP_METROPOINT_CONSISTENCY_GROUP} ${rpmpvolume}${volume_name_modifier} ${rpmp_src_varray} ${rpmp_tgt_volumes} ${rpmp_tgt_varrays}

                # Change Replication Mode Test
                secho 'RP change vpool - replication mode, not supported on RP/MP yet'
                # recoverpoint_vpool_change_replication_mode_test ${rpmpvolume}${volume_name_modifier} ${rpmp_src_vpool_sync}
                # recoverpoint_vpool_change_replication_mode_test ${rpmpvolume}${volume_name_modifier} ${rpmp_src_vpool}

                # RP Export Bookmark Test
                recoverpoint_export_bookmark_tests ${rpmpvolume} $RECOVERPOINT_VARRAY1 ${rp_tgt_varrays} ${rpmp_src_vpool} "mp"
            fi

            # XIO test cases
            if [ "$RPXIO_TESTS" = "1" ]; then
                secho 'XIO tests...'

                # Change Virtual Pool Test
                recoverpoint_change_vpool_test ${rpxiovolume}${volume_name_modifier} ${rpxio_noprotection_vpool} ${rpxio_src_vpool} ${RP_XIO_CONSISTENCY_GROUP}

                # Add Journal Test
                recoverpoint_add_journal_volume_test ${rpxio_tgt_varray} ${rpxio_tgt_varray} ${rpxio_tgt_vpool} ${RP_XIO_CONSISTENCY_GROUP}
                
                # Snapshot/Bookmark Test
                if [ "${RP_CLR}" = "1" ]; then
                    recoverpoint_auto_snapshot_cleanup_test ${rpxiovolume}${volume_name_modifier} ${rpxiovolumetarget}${volume_name_modifier} ${rpxio_tgt_varray} ${RP_XIO_CONSISTENCY_GROUP} ${rpxiovolume}-target-${RECOVERPOINT_VARRAY1}
                else
                    recoverpoint_auto_snapshot_cleanup_test ${rpxiovolume}${volume_name_modifier} ${rpxiovolumetarget}${volume_name_modifier} ${rpxio_tgt_varray} ${RP_XIO_CONSISTENCY_GROUP}
                fi

                # Failover Test
                recoverpoint_failover_test ${rpxiovolume}${volume_name_modifier} ${rpxiovolumetarget}

                # CG Failover/Swap Test
                recoverpoint_cg_failover_test ${RP_XIO_CONSISTENCY_GROUP} ${rpxiovolume}${volume_name_modifier} ${rpxio_src_varray} ${rpxio_tgt_volumes} ${rpxio_tgt_varrays}

                # Change Replication Mode Test
                secho 'RP change vpool - replication mode, not supported on RP/XIO yet'
                #recoverpoint_vpool_change_replication_mode_test ${rpxiovolume}${volume_name_modifier} ${rpxio_src_vpool_sync}
                #recoverpoint_vpool_change_replication_mode_test ${rpxiovolume}${volume_name_modifier} ${rpxio_src_vpool}

                # RP Export Bookmark Test
                recoverpoint_export_bookmark_tests ${rpxiovolume} $RECOVERPOINT_VARRAY1 ${rpxio_tgt_varrays} ${rpxio_src_vpool} "xio"
            fi
        fi

        ### End RecoverPoint test cases ###

        if [ "$RP_CLEANUP_VOLUMES" = "1" ]; then
            hecho 'RP cleanup'

            if [ "$RP_TESTS" = "1" ]; then
                secho 'RP volume cleanup...'
                recoverpoint_cleanup_volumes ${rpvolume} $RP_VOLUME_COUNT
            fi

            if [ "$RPVPLEX_TESTS" = "1" ]; then
                secho 'RP+VPLEX volume cleanup...'
                recoverpoint_cleanup_volumes ${rpvplexvolume} $RP_VOLUME_COUNT
            fi

            if [ "$RPMP_TESTS" = "1" ]; then
                secho 'MP volume cleanup...'
                recoverpoint_cleanup_volumes ${rpmpvolume} $RP_VOLUME_COUNT
            fi
            if [ "$RPXIO_TESTS" = "1" ]; then
                secho 'XIO volume cleanup...'
                recoverpoint_cleanup_volumes ${rpxiovolume} $RP_VOLUME_COUNT
            fi

            secho 'CG cleanup...'
            recoverpoint_cleanup_cgs            
        fi
        hecho 'RP tests done'
    fi
}

#
#
#
######################### End of RecoverPoint ############################

#
# filepolicy tests configuration
#
SNAPSHOT_TYPE="file_snapshot"
REPLICATION_TYPE="file_replication"
APPLY_AT_VPOOL="vpool"
APPLY_AT_PROJECT="project"
REPLICATION_CONFIGURATION='false'
SNAPSHOT_EXPIRE_TYPE="days"
SNAPSHOT_EXPIRE_VALUE=3
POLICY_SCHEDULE_FREQUENCY="days"
POLICY_SCHEDULE_REPEAT=2
POLICY_SCHEDULE_TIME="03:00"
LOCAL_REPLICATION_TYPE="LOCAL"
REMOTE_REPLICATION_TYPE="REMOTE"
REPLICATION_COPY_MODE="ASYNC"
APPLY_ON_TARGET_SITE="false"

#
# filepolicy tests
#
filepolicy_tests()
{
    echo "File policy tests started"
    ISI_PROJECT=isiproject
    run project create $ISI_PROJECT
    cos=$1; shift
    fpname=$SNAPSHOT_TYPE$APPLY_AT_VPOOL
    echo $fpname
    
    echo "Creating snapshot file policy $fpname"
    run filepolicy create_pol $fpname $SNAPSHOT_TYPE $APPLY_AT_VPOOL \
                          --snapshotexpiretype $SNAPSHOT_EXPIRE_TYPE \
                          --snapshotexpirevalue $SNAPSHOT_EXPIRE_VALUE \
                          --policyschedulefrequency $POLICY_SCHEDULE_FREQUENCY \
                          --policyschedulerepeat $POLICY_SCHEDULE_REPEAT \
                          --policyscheduletime $POLICY_SCHEDULE_TIME
    
    run filepolicy show $fpname
    
    echo "Assigning filepolicy $fpname to vpool $cos"
    run filepolicy vpool_assign $fpname --apply_on_target_site $APPLY_ON_TARGET_SITE --assign_to_vpools $cos
    
    echo "Unassigning filepolicy $fpname from vpool $cos"
    run filepolicy vpool_unassign $fpname $cos
    
    echo "Deleting the file policy $fpname"
    run filepolicy delete $fpname


    fpname=$REPLICATION_TYPE$APPLY_AT_PROJECT
    echo $fpname
    
    echo "Creating replication file policy $fpname"
    
    run filepolicy create_pol $fpname $REPLICATION_TYPE $APPLY_AT_PROJECT \
                          --replicationtype $REMOTE_REPLICATION_TYPE \
                          --replicationcopymode $REPLICATION_COPY_MODE \
                          --replicationconfiguration $REPLICATION_CONFIGURATION \
                          --policyschedulefrequency $POLICY_SCHEDULE_FREQUENCY \
                          --policyschedulerepeat $POLICY_SCHEDULE_REPEAT \
                          --policyscheduletime $POLICY_SCHEDULE_TIME \
                          --is_access_to_tenants true
    
    sleep 15
    run filepolicy show $fpname
    
    echo "Assigning filepolicy $fpname to project $ISI_PROJECT"
    run filepolicy project_assign $fpname  \
                         --apply_on_target_site true \
                          --project_assign_vpool $cos \
                          --assign_to_projects $ISI_PROJECT \
                          --source_varray $REMOTE_NH \
                          --target_varrays $NH  
                          
    datetime=`date +%m%d%y%H%M%S`
    fsname=FilePolicyTest-$cos-$datetime
    
    echo "Creating filesystem $fsname"
    run fileshare create $fsname $ISI_PROJECT $REMOTE_NH $cos $FS_SIZEMB
    
    sleep 15
    echo "Displaying the filesystem created"
    run fileshare show $ISI_PROJECT/$fsname 
    
    sleep 15
    echo "Deleting the filesystem"
    run fileshare delete $ISI_PROJECT/$fsname
    
    echo "Unassigning filepolicy $fpname from project $PROJECT"
    run filepolicy project_unassign $fpname $ISI_PROJECT
    
    echo "Deleting the file policy $fpname"
    run filepolicy delete $fpname
    
    echo "File Policy Test Completed."

}

hds_cos_setup()
{
    run cos create block $COS_HDS               \
                      --description 'Virtual-Pool-for-HDS' true  \
                      --protocols FC \
                      --numpaths 1              \
                      --provisionType 'Thin'        \

    run cos allow $COS_HDS block $TENANT
}

hds_setup()
{
    hds_setup_once
}

hds_setup_once()
{

    # Discovery the storage systems 
    storageprovider show $HDS_PROVIDER &> /dev/null && return $?
    storageprovider create $HDS_PROVIDER $HDS_PROVIDER_IP $HDS_PROVIDER_PORT $HDS_PROVIDER_USER "$HDS_PROVIDER_PASSWD" $HDS_PROVIDER_INTERFACE_TYPE --usessl false
    storagedevice discover_all
    storagedevice list

    hds_cos_setup
}

#
# hds tests
#
hds_tests()
{
    echo "**** Done hds"
}


#
# add an isilon storage device with a default pool that supports NFS and N+2:1 protection
#
isilon_setup_once()
{
    # do this only once
    discoveredsystem show $ISI_NATIVEGUID &> /dev/null && return $?

    discoveredsystem create $ISI_DEV isilon $ISI_IP 8080 $ISI_USER $ISI_PASSWD --serialno=$ISI_SN
    
    isilon_cos_setup

    storagepool   update $ISI_NATIVEGUID --type file
    storageport   update $ISI_NATIVEGUID IP --tzone $NH/$IP_ZONE

    run cos update file $COS_ISIFILE --storage $ISI_NATIVEGUID
	
}

isilon_replication_setup_once()
{
####ZoneSetup

    run neighborhood create $REMOTE_NH
    if [ "$EXTRA_PARAM" = "search" ] ; then
        neighborhood search $(echo $REMOTE_NH | head -c 2)
        run neighborhood tag $REMOTE_NH $TAG
        neighborhood search $SEARCH_PREFIX --tag true
    fi


    run transportzone create $REM_IP_ZONE $REMOTE_NH --type IP
    if [ "$EXTRA_PARAM" = "search" ] ; then
        transportzone search $(echo $IP_ZONE | head -c 2)
        run transportzone tag $REMOTE_NH/$REM_IP_ZONE $TAG
        transportzone search $SEARCH_PREFIX --tag true
    fi

    run transportzone add $REMOTE_NH/$REM_IP_ZONE $FSEXP1
    run transportzone add $REMOTE_NH/$REM_IP_ZONE $FSEXP2
    run transportzone add $REMOTE_NH/$REM_IP_ZONE $FSEXP3
    
	run neighborhood allow $REMOTE_NH $TENANT
	run neighborhood allow $REMOTE_NH $ROOT_TENANT
	
####device discovery

    discoveredsystem create $REM_ISI_DEV isilon $REM_ISI_IP 8080 $REM_ISI_USER $REM_ISI_PASSWD --serialno=$REM_ISI_SN
    

    storagepool   update $REM_ISI_NATIVEGUID --type file
    storageport   update $REM_ISI_NATIVEGUID IP --tzone $REMOTE_NH/$REM_IP_ZONE



####Cos setup
    echo "setting up remote isilon COS"
    run cos create file $REM_COS_ISIFILE 				\
	--description 'Virtual-Pool-for-Isilon' true 	\
                            --protocols NFS CIFS --max_snapshots 10 \
                            --provisionType 'Thin' \
			    --neighborhoods $REMOTE_NH $NH --schedule_snapshot true --replication_support true --filepolicy_at_project true --filepolicy_at_fs true
    ROOT_TENANT=`tenant root|tail -1`
    run cos allow $REM_COS_ISIFILE file $ROOT_TENANT
    if [ "$EXTRA_PARAM" = "search" ] ; then
        cos search $(echo $REM_COS_ISIFILE | head -c 2) --resource_type file_vpool
        cos tag "$REM_COS_ISIFILE" "file" $TAG
        cos search $SEARCH_PREFIX --tag true --resource_type file_vpool
    fi
    
	run cos update file $REM_COS_ISIFILE --storage $REM_ISI_NATIVEGUID
	
    run cos allow $REM_COS_ISIFILE file $TENANT
	
    trap '_failure $LINENO' ERR

}

isilon_setup()
{
    isilon_setup_once
    run cos allow $COS_ISIFILE file $TENANT
    isilon_replication_setup_once
}

sbsdk_setup()
{
    echo "***** Executing setup for SB SDK sanity tests *****"

    #Define variables
    SBSDK_DRIVER_SYS_NAME=SBSDK_DriverSimulator-${RANDOM}
    SBSDK_DRIVER_SYS_TYPE=driversystem
    SBSDK_DRIVER_SYS_NATIVEGUID=$SBSDK_DRIVER_SYS_TYPE+$SBSDK_DRIVER_SYS_NAME
    SBSDK_DRIVER_SYS_IP=10.20.30.40
    SBSDK_DRIVER_SYS_PORT=8080
    SBSDK_DRIVER_SYS_USER=user
    SBSDK_DRIVER_SYS_PASSWORD=password
    SBSDK_DRIVER_SYS_VPOOL_NAME=SBSDK_VPOOL
    SBSDK_DRIVER_SYS_PWWN10=61:FE:FE:FE:FE:FE:FE:10
    SBSDK_DRIVER_SYS_PWWN11=61:FE:FE:FE:FE:FE:FE:11
    SBSDK_DRIVER_SYS_PWWN12=61:FE:FE:FE:FE:FE:FE:12
    SBSDK_DRIVER_SYS_PWWN13=61:FE:FE:FE:FE:FE:FE:13
    SBSDK_DRIVER_SYS_PWWN14=61:FE:FE:FE:FE:FE:FE:14
    SBSDK_DRIVER_SYS_PWWN15=61:FE:FE:FE:FE:FE:FE:15
    SBSDK_DRIVER_SYS_PWWN16=61:FE:FE:FE:FE:FE:FE:16

    #Discover the SB SDK simulator storage system
    echo "***** Creating SB SDK simulator storage system *****"
    run discoveredsystem create $SBSDK_DRIVER_SYS_NAME $SBSDK_DRIVER_SYS_TYPE  $SBSDK_DRIVER_SYS_IP \
        $SBSDK_DRIVER_SYS_PORT $SBSDK_DRIVER_SYS_USER $SBSDK_DRIVER_SYS_PASSWORD
    run storagedevice discover_all
    #The sleep allows discovery to complete, so that the subsequent commands
    #to add the system ports to the specified network completes successfully.
    sleep 60
    
    #Add storage ports to network
    echo "***** Adding storage ports to network *****"
    run transportzone add ${NH}/${CLUSTER1NET_SIM_NAME} $SBSDK_DRIVER_SYS_PWWN10
    run transportzone add ${NH}/${CLUSTER1NET_SIM_NAME} $SBSDK_DRIVER_SYS_PWWN11
    run transportzone add ${NH}/${CLUSTER1NET_SIM_NAME} $SBSDK_DRIVER_SYS_PWWN12
    run transportzone add ${NH}/${CLUSTER1NET_SIM_NAME} $SBSDK_DRIVER_SYS_PWWN13
    run transportzone add ${NH}/${CLUSTER1NET_SIM_NAME} $SBSDK_DRIVER_SYS_PWWN14
    run transportzone add ${NH}/${CLUSTER1NET_SIM_NAME} $SBSDK_DRIVER_SYS_PWWN15
    run transportzone add ${NH}/${CLUSTER1NET_SIM_NAME} $SBSDK_DRIVER_SYS_PWWN16

    #Define a virtual pool
    echo "***** Creating virtual pool $SBSDK_DRIVER_SYS_VPOOL_NAME *****"
    run cos create block $SBSDK_DRIVER_SYS_VPOOL_NAME true     \
                     --description 'SBSDK-sanity-virtual-pool' \
                     --protocols FC                            \
                     --numpaths 2                              \
                     --provisionType 'Thin'                    \
                     --neighborhoods $NH                       \
                     --max_snapshots 1                         \
                     --max_mirrors 1                           \
                     --expandable false 
    run cos allow $SBSDK_DRIVER_SYS_VPOOL_NAME block $TENANT
    run cos update block $SBSDK_DRIVER_SYS_VPOOL_NAME --storage $SBSDK_DRIVER_SYS_NATIVEGUID
    run cos update block $SBSDK_DRIVER_SYS_VPOOL_NAME --use_matched false

    echo "***** Completed setup for SB SDK sanity tests *****"
}


sbsdk_tests()
{
    echo "***** Executing SB SDK sanity tests *****"

    #Define variables
    VOLUME_NAME=SBSDK_Volume-${RANDOM}
    SNAPSHOT_NAME=${VOLUME_NAME}Snap
    FULLCOPY_NAME=${VOLUME_NAME}FullCopy
    HOST_NAME=$PROJECT
    HOST_FQDN=$HOST_NAME.lss.emc.com
    HOST_INIT_PORT_WWN1=10:00:00:E0:7E:00:00:0F
    HOST_INIT_NODE_WWN1=20:00:00:E0:7E:00:00:0F
    HOST_INIT_PORT_WWN2=10:00:00:90:FA:18:0E:99
    HOST_INIT_NODE_WWN2=20:00:00:90:FA:18:0E:99

    #Create a volume
    echo "***** Creating volume $VOLUME_NAME *****"
    run volume create $VOLUME_NAME $PROJECT $NH $SBSDK_DRIVER_SYS_VPOOL_NAME $BLK_SIZE

    #Create a host
    echo "***** Creating host $HOST_NAME *****"
    run hosts create $HOST_NAME $TENANT Windows $HOST_FQDN --port 8111 --username user --password 'password' --osversion 1.0

    #Create initiators for the host
    echo "***** Creating host initiators *****"
    run initiator create $HOST_NAME FC $HOST_INIT_PORT_WWN1 --node $HOST_INIT_NODE_WWN1
    run initiator create $HOST_NAME FC $HOST_INIT_PORT_WWN2 --node $HOST_INIT_NODE_WWN2

    #Add initiators to network
    echo "***** Adding host initiators to Network *****"
    run transportzone add ${NH}/${CLUSTER1NET_SIM_NAME} $HOST_INIT_PORT_WWN1
    run transportzone add ${NH}/${CLUSTER1NET_SIM_NAME} $HOST_INIT_PORT_WWN2

    #Export the volume
    echo "***** Exporting volume $VOLUME_NAME to host $HOST_NAME in varray $NH *****"
    run export_group create $PROJECT $HOST_NAME-1$HOST_FQDN $NH --volspec "$PROJECT/$VOLUME_NAME" --inits "$HOST_NAME/$HOST_INIT_PORT_WWN1","$HOST_NAME/$HOST_INIT_PORT_WWN2"
 
    #Unexport the volume
    echo "***** Unexporting volume $VOLUME_NAME from host $HOST_NAME *****"
    run export_group delete $PROJECT/$HOST_NAME-1$HOST_FQDN

    #Create a snapshot of the volume
    echo "***** Creating snapshot $SNAPSHOT_NAME of volume $VOLUME_NAME *****"
    run blocksnapshot create $PROJECT/$VOLUME_NAME $SNAPSHOT_NAME

    #Restore the snapshot
    echo "***** Restoring snapshot $SNAPSHOT_NAME *****"
    run blocksnapshot restore $PROJECT/$VOLUME_NAME/$SNAPSHOT_NAME

    #Delete the snapshot
    echo "***** Deleting snapshot $SNAPSHOT_NAME *****"
    run blocksnapshot delete $PROJECT/$VOLUME_NAME/$SNAPSHOT_NAME

    #Create a full copy of the volume
    echo "***** Creating full copy $FULLCOPY_NAME of volume $VOLUME_NAME *****"
    run volume full_copy $FULLCOPY_NAME $PROJECT/$VOLUME_NAME --count=1

    #Restore the full copy
    echo "***** Restoring full copy $FULLCOPY_NAME *****"
    run volume full_copy_restore $PROJECT/$FULLCOPY_NAME

    #Detach the full copy
    echo "***** Detaching full copy $FULLCOPY_NAME *****"
    run volume detach ${PROJECT}/$VOLUME_NAME ${PROJECT}/$FULLCOPY_NAME

    #Delete the full copy
    echo "***** Deleting full copy $FULLCOPY_NAME *****"
    run volume delete $PROJECT/$FULLCOPY_NAME --wait

    #Delete the volume
    echo "***** Deleting volume $VOLUME_NAME *****"
    run volume delete $PROJECT/$VOLUME_NAME --wait

    #Delete the host
    echo "***** Deleting host $HOST_NAME *****"
    run hosts delete $HOST_NAME

    echo "***** Completed SB SDK sanity tests *****"
}

#
# add ECS storage device and storage pool
#
ecs_setup_once()
{
    #discoveredsystem show $ECS_DEV &> /dev/null && return $?
    discoveredsystem create $ECS_DEV ecs $ECS_IP 4443 $ECS_USER $ECS_PASSWD

    storagepool update $ECS_NATIVEGUID --nhadd $NH
    storageport update $ECS_NATIVEGUID IP --addvarrays $NH

    ecs_cos_setup
    
    tenant update_namespace $TENANT  $ECS_NAMESPACE
    project create $PROJECT --tenant $TENANT 
}

ecs_setup()
{
    echo "Performing ecs_setup"
    ecs_setup_once
}

#
# add vnx file device with a storage pool that supports NFS and COS_VNX_PROTECTION protection
# add server_{2,3,4} data mover storage ports to this device, connected to the IP transport zone
#
netapp_setup_once()
{
    #do this only once
    storagedevice show $NETAPPF_NATIVEGUID &> /dev/null && return $?

    discoveredsystem create $NETAPPF_DEV netapp $NETAPPF_IP $NETAPPF_PORT $NETAPPF_USER $NETAPPF_PW --serialno=$NETAPPF_SN
    netapp_cos_setup

    storagepool update $NETAPPF_NATIVEGUID --type file
    
    storageport update $NETAPPF_NATIVEGUID IP --tzone nh/iptz
    run cos update file $COS_NETAPP --storage $NETAPPF_NATIVEGUID

   # netapp_cos_setup
}

netapp_setup()
{
    netapp_setup_once
    run cos allow $COS_NETAPP file $TENANT
}

netappc_setup_once()
{
    #do this only once
    storagedevice show $NETAPPCF_NATIVEGUID &> /dev/null && return $?

    discoveredsystem create $NETAPPCF_DEV netappc $NETAPPCF_IP $NETAPPCF_PORT $NETAPPCF_USER $NETAPPCF_PW --serialno=$NETAPPCF_SN
    netappc_cos_setup

    storagepool update $NETAPPCF_NATIVEGUID --type file

    storageport update $NETAPPCF_NATIVEGUID IP --tzone nh/iptz
    run cos update file $COS_NETAPPC --storage $NETAPPCF_NATIVEGUID

   # netapp_cos_setup
}

netappc_setup()
{
    netappc_setup_once
    run cos allow $COS_NETAPPC file $TENANT
}

#
# add datadomain file device with a storage pool that supports NFS and CIFS
# add server_{2,3,4} data mover storage ports to this device, connected to the IP transport zone
#
datadomainfile_setup_once()
{
    #do this only once

    # Discover the storage systems 
    storageprovider show $DATADOMAINF_DEV &> /dev/null && return $?
    echo "Starting storageprovider create"
    storageprovider create $DATADOMAINF_DEV $DATADOMAINF_DEV_IP $DATADOMAINF_PORT $DATADOMAINF_USER "$DATADOMAINF_PW" $DATADOMAINF_PROVIDER_INTERFACE 
    echo "Storageprovider create done"
    storagedevice discover_all
    storagedevice list
    storageport list $DATADOMAINF_NATIVEGUID
    storagedevice show $DATADOMAINF_NATIVEGUID &> /dev/null && return $?

    datadomainfile_cos_setup

    storagepool update $DATADOMAINF_NATIVEGUID --type file

    storageport update $DATADOMAINF_NATIVEGUID IP --tzone nh/iptz
    run cos update file $COS_DDFILE --storage $DATADOMAINF_NATIVEGUID

}

datadomainfile_setup()
{
    datadomainfile_setup_once
    run cos allow $COS_DDFILE file $TENANT
}

#
# add vnx file device with a storage pool that supports NFS and COS_VNX_PROTECTION protection
# add server_{2,3,4} data mover storage ports to this device, connected to the IP transport zone
#
vnxfile_setup_once()
{

    vnxfile_discovery

    storagepool update $VNXF_NATIVEGUID --type file

    storageport update $VNXF_NATIVEGUID IP --tzone nh/iptz
    run cos update file $COS_VNXFILE --storage $VNXF_NATIVEGUID

   # vnxfile_cos_setup
}

vnxfile_setup()
{
    vnxfile_setup_once
    run cos allow $COS_VNXFILE file $TENANT
}

vnxfile_discovery()
{
    #do this only once
    storagedevice show $VNXF_NATIVEGUID &> /dev/null && return $?

    discoveredsystem create $VNXF_DEV vnxfile $VNXF_IP $VNXF_PORT $VNXF_USER $VNXF_PW \
                         --smisip=$VNXF_SMIS_IP --smisport=$VNXF_SMIS_PORT --smisuser=nasadmin \
                         --smispw=nasadmin --smisssl=true --serialno=$VNXF_SN
    vnxfile_cos_setup
}
<<COMMENT
COP-14502
vnxfile_flex_varray_setup_once()
{
    vnxfile_discovery

    neighborhood create $NH3
    neighborhood show $NH3

    transportzone create2 $IP_ZONE3 IP --endpoints $VNXF_IP_ENDPOINT1,$VNXF_IP_ENDPOINT2,client1.emc.com,client2.emc.com

    # update the varray to add some ports
    storageport update $VNXF_NATIVEGUID IP --name 'spvnx1' --addvarrays $NH3
    storageport update $VNXF_NATIVEGUID IP --name 'spvnx2' --addvarrays $NH3
}

vnxfile_flex_varray_setup()
{
    vnxfile_flex_varray_setup_once
    run cos allow $COS_VNXFILE file $TENANT
}
COMMENT
vnxe_setup_once()
{

    vnxe_discovery

    vnxe_cos_setup

    storagepool update $VNXE_NATIVEGUID --type file

    storageport update $VNXE_NATIVEGUID IP --tzone nh/iptz
    run cos update file $COS_VNXE --storage $VNXE_NATIVEGUID
    
    storagepool update $VNXE_NATIVEGUID --type block --volume_type THIN_AND_THICK
    echo "vnxe block cos update"
    run cos update block $COS_VNXEBLOCK_CG --storage $VNXE_NATIVEGUID
   # run cos update block $COS_VNXEBLOCK_FC --storage $VNXE_NATIVEGUID
    run cos update block $COS_VNXEBLOCK_ISCSI --storage $VNXE_NATIVEGUID
   
}

vnxe_setup()
{
    vnxe_setup_once
    run cos allow $COS_VNXE file $TENANT
}

vnxe_discovery()
{
    #do this only once
    storagedevice show $VNXE_NATIVEGUID &> /dev/null && return $?

    discoveredsystem create $VNXE_DEV vnxe $VNXE_IP $VNXE_PORT $VNXE_USER $VNXE_PW \
                         --serialno=$VNXE_SN

}

ui_setup()
{
    echo "Nothing to do for ui setup"
}

namespace_test() {
    cos=$1; shift
    kpname=$(python -c 'import uuid; print uuid.uuid1()')
#    run namespace sanity --tenant $TENANT $NAMESPACE
}

retentionclass_test() {

    run retentionclass sanity $NAMESPACE "class_"
}

s3_baseurl_setup() {
    create_if_not_present=$1;
    if [ -n "$create_if_not_present"  ]; then
        is_present=$(run baseurl list | (grep 's3.amazonaws.com' || echo ''))
        if [ "$is_present" == '' ]; then
            echo "Default base URL entry not present, hence inserting it"
            run baseurl create "DefaultBaseUrl" "s3.amazonaws.com" false
        fi
    else
        run baseurl create "DefaultBaseUrl" "s3.amazonaws.com" false
    fi
}

s3_bucket_tests() {


    cos=$1; uid=$2; secret=$3; shift
    kpname=bucketsanitytest-$cos-$(python -c 'import uuid; print uuid.uuid1()')
    run bucket sanity $NAMESPACE $kpname --uid $uid --secret=$secret
}

s3_versioning_tests() {
    cos=$1; uid=$2; secret=$3; shift
    kpname=$(python -c 'import uuid; print uuid.uuid1()')
    run bucket create $NAMESPACE $kpname --uid $uid --secret=$secret
    run versioning sanity $NAMESPACE $kpname --uid $uid --secret=$secret
    run bucket delete $NAMESPACE $kpname --uid $uid --secret=$secret
}

s3_multipart_upload_tests() {
    cos=$1; uid=$2; secret=$3; shift
    kpname=$(python -c 'import uuid; print uuid.uuid1()')
    keyname=key-$cos
    run bucket create $NAMESPACE $kpname --uid $uid --secret=$secret
    run s3_multipart_upload.py sanity $NAMESPACE $kpname $keyname --uid $uid --secret=$secret

    # copy part tests
    keysuffix=$(python -c 'import uuid; print uuid.uuid1()')
    srckpname=mpusrcbucket-$keysuffix
    # create source bucket
    run bucket create $NAMESPACE $srckpname --uid $uid --secret=$secret
    # create source object
    srckeyname=mpusrckey-$keysuffix
    run bucketkey create $NAMESPACE $srckpname $srckeyname copy-key-value-$cos --uid $uid --secret=$secret
    # copy source object as part to dest
    run s3_multipart_upload.py sanity_copy $NAMESPACE $kpname $keyname --srcbucket $srckpname --srckey $srckeyname --uid $uid --secret=$secret

    run bucketkey clean $NAMESPACE $srckpname --uid $uid --secret=$secret
    run bucket delete $NAMESPACE $srckpname --uid $uid --secret=$secret

    run bucketkey clean $NAMESPACE $kpname --uid $uid --secret=$secret
    run bucket delete $NAMESPACE $kpname --uid $uid --secret=$secret
}

swift_container_tests() {
    cos=$1; uid=$2; password=$3; shift
    kpname=contsanitytest-$cos-$(python -c 'import uuid; print uuid.uuid1()')
    run swift_container.py sanity $NAMESPACE $kpname --uid $uid --password=$password
}

swift_setup_uid_password() {
    uid=$1; password=$2; shift
    run password.py create $uid $password s3 --groups="admin"
}

swift_cleanup_uid_password() {
    uid=$1; shift
    run password.py remove $uid
}

atmos_subtenant_tests() {
    cos=$1; uid=$2; secret=$3; shift
    run atmossubtenant sanity $NAMESPACE $PROJECT $cos --uid $uid --secret=$secret
}

s3_key_tests(){
    cos=$1; uid=$2; secret=$3; testUid=$4; shift
    kpname=$(python  -c 'import uuid; print uuid.uuid1()')
    run bucket create $NAMESPACE $kpname --uid $uid --secret=$secret
    run bucketkey sanity $NAMESPACE $kpname key-$cos value-$cos  --uid $uid --secret=$secret --testUser=$testUid
    run bucket delete $NAMESPACE $kpname --uid $uid --secret=$secret

    run bucket create $NAMESPACE $kpname --uid $uid --secret=$secret --fileSystemEnabled=true
    run bucketkey filesystemsanity $NAMESPACE $kpname key-$cos value-$cos  --uid $uid --secret=$secret --testUser=$testUid
    run bucket delete $NAMESPACE $kpname --uid $uid --secret=$secret

}

s3_key_copy_tests(){
    cos=$1; uid=$2; secret=$3; shift
    sourcekpname=$(python  -c 'import uuid; print uuid.uuid1()')
    destkpname=$(python  -c 'import uuid; print uuid.uuid1()')
    run bucket create $NAMESPACE $sourcekpname --uid $uid --secret=$secret
    run bucket create $NAMESPACE $destkpname --uid $uid --secret=$secret

    keysuffix=$(python  -c 'import uuid; print uuid.uuid1()')

    # create source object
    sourcekeyname=copy-source-key-$keysuffix
    run bucketkey create $NAMESPACE $sourcekpname $sourcekeyname copy-key-value-$cos --uid $uid --secret=$secret
    # copy source object to dest
    run bucketkey copy $NAMESPACE $sourcekpname $sourcekeyname --destbucket $destkpname --destkey copy-dest-key-$keysuffix --uid $uid --secret=$secret

    # create source object with chars that need to be URL-encoded in copy-source header
    sourcekeyname2=copy,source:key/$keysuffix
    run bucketkey create $NAMESPACE $sourcekpname $sourcekeyname2 copy-key-value-$cos --uid $uid --secret=$secret
    # copy source object to dest
    run bucketkey copy $NAMESPACE $sourcekpname $sourcekeyname2 --destbucket $destkpname --destkey copy-dest-key2-$keysuffix --uid $uid --secret=$secret
    run bucketkey clean $NAMESPACE $sourcekpname --uid $uid --secret=$secret
    run bucketkey clean $NAMESPACE $destkpname --uid $uid --secret=$secret
    run bucket delete $NAMESPACE $sourcekpname --uid $uid --secret=$secret
    run bucket delete $NAMESPACE $destkpname --uid $uid --secret=$secret
}

s3_key_version_tests(){
    cos=$1; uid=$2; secret=$3; shift
    kpname=$(python  -c 'import uuid; print uuid.uuid1()')
    run bucket create $NAMESPACE $kpname --uid $uid --secret=$secret
    run versioning put $NAMESPACE $kpname Enabled --uid $uid --secret=$secret
    run bucketkey sanity $NAMESPACE $kpname key-$cos value-$cos  --uid $uid --secret=$secret --testUser=$testUid
    run versioning put $NAMESPACE $kpname Suspended --uid $uid --secret=$secret
    run bucketkey sanity $NAMESPACE $kpname key-$cos value-$cos  --uid $uid --secret=$secret --testUser=$testUid
    run bucketkey clean_ver $NAMESPACE $kpname --uid $uid --secret=$secret
    run bucket delete $NAMESPACE $kpname --uid $uid --secret=$secret
}

s3_fileaccess_tests() {
    cos=$1; uid=$2; secret=$3; shift

    kpname1="$(python  -c 'import uuid; print uuid.uuid1()')-S3"
    run bucket create $NAMESPACE $kpname1 --uid $uid --secret=$secret
    run fileaccesssanity.py s3 $NAMESPACE $kpname1 $cos $uid $secret

    kpname2="$(python  -c 'import uuid; print uuid.uuid1()')-S3"
    run bucket create $NAMESPACE $kpname2 --uid $uid --secret=$secret
    run fileaccesssanity.py s3 $NAMESPACE $kpname2 $cos $uid $secret True

    fskpname1="$(python  -c 'import uuid; print uuid.uuid1()')-S3"
    run bucket create $NAMESPACE $fskpname1 --uid $uid --secret=$secret --fileSystemEnabled=True
    run fileaccesssanity.py s3 $NAMESPACE $fskpname1 $cos $uid $secret

    fskpname2="$(python  -c 'import uuid; print uuid.uuid1()')-S3"
    run bucket create $NAMESPACE $fskpname2 --uid $uid --secret=$secret --fileSystemEnabled=True
    run fileaccesssanity.py s3 $NAMESPACE $fskpname2 $cos $uid $secret True

    run bucketkey clean $NAMESPACE $kpname1 --uid $uid --secret=$secret
    run bucketkey clean $NAMESPACE $kpname2 --uid $uid --secret=$secret
    run bucketkey clean $NAMESPACE $fskpname1 --uid $uid --secret=$secret
    run bucketkey clean $NAMESPACE $fskpname2 --uid $uid --secret=$secret

    run bucket delete $NAMESPACE $kpname1 --uid $uid --secret=$secret
    run bucket delete $NAMESPACE $kpname2 --uid $uid --secret=$secret
    run bucket delete $NAMESPACE $fskpname1 --uid $uid --secret=$secret
    run bucket delete $NAMESPACE $fskpname2 --uid $uid --secret=$secret
}

swift_object_tests(){
    cos=$1; uid=$2; password=$3; shift
    kpname=$(python  -c 'import uuid; print uuid.uuid1()')
    run swift_container.py create $NAMESPACE $kpname --uid $uid --password=$password
    if [ "$BOURNE_IP" != "127.0.0.1" ]; then
        run swift_object.py sanity $NAMESPACE $kpname key-$cos value-$cos --uid $uid --password=$password
    fi
    run swift_container.py delete $NAMESPACE $kpname --uid $uid --password=$password
}

swift_fileaccess_tests() {
    cos=$1; uid=$2; secret=$3; shift
    kpname="$(python  -c 'import uuid; print uuid.uuid1()')-SWIFT"
    run swift_container.py create $NAMESPACE $kpname --uid $uid --password=$password
    run fileaccesssanity.py swift $NAMESPACE $kpname $cos $uid $password
    run swift_container.py delete $NAMESPACE $kpname --uid $uid --password=$password
}

atmos_object_tests(){
    cos=$1; uid=$2; secret=$3; testUid=$4; testUidSecret=$5; shift
    subtenantstr=`run atmossubtenant create $NAMESPACE $PROJECT $cos --uid $uid --secret=$secret`
    subtenant=${subtenantstr##*subtenant=}    
    run atmoskey sanity $NAMESPACE $PROJECT $subtenant key-$cos value-$cos --uid $uid --secret=$secret --testUid=$testUid --testUidSecret=$testUidSecret
    run atmossubtenant delete $NAMESPACE $subtenant --uid $uid --secret=$secret
}

object_create_secretkey() {
    echo "remove all secret keys before creating one"
    secretkey deleteuser $WS_UID
    secretkey deleteuser $WS_TEST_UID
    object_set_user_scope

    objectuser add $WS_UID $NAMESPACE
    objectuser add $WS_TEST_UID $NAMESPACE

    #object_set_user_scope_test
    WS_SECRET=`secretkeyuser add $WS_UID |tail -1`
    WS_SECRET_TEST_UID=`secretkeyuser add $WS_TEST_UID |tail -1`   
    
    #wait in case datasvc has already cache secret key from previous sanity running
    #echo "wait 3 minutes so that the secret key can be refreshed: secret key $WS_SECRET"
    #sleep 180
}

object_set_user_scope(){

    # disable set user scope  temporarily
    # to be enabled in the next patch
    SCRIPTDIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
    #echo "setting user scope as global"
    #result_global=`python $SCRIPTDIR/userscope set GLOBAL`
    #echo "setting user scope as global done"

    echo "get user scope"
    result=`python $SCRIPTDIR/userscope get`

    echo "get user scope result: $result"

    #if [ "$result_global" == "$result" ]
    #then
    #    echo "User scope is set to GLOBAL"
    #else
    #    echo "Unable to set user space as GLOBAL. Response is:"
    #    echo "$result_global"
    #fi
 
}

object_set_user_scope_test(){
    
    #validate the user scope again by trying to set it as NAMESPACE
    #and it should fail
    echo "setting user scope as namespace"
    result_namespace=`python $SCRIPTDIR/userscope set NAMESPACE`

    echo "get user scope"
    result=`python $SCRIPTDIR/userscope get`

    echo "get user scope result : $result"
   
    #compare the results and it should not be same
    if [ "$result_namespace" != "$result" ]
    then 
        echo "Set user scope as namespace failed as expected"
    else
        echo "Unexpected user scope shouldnt be allowed to change!!!"
    fi

}

object_delete_secretkey() {

    #login with webstorage user so that secret key can be created
    # security login $WS_UID $WS_PASSWORD

    #remove all secret keys
    secretkeyuser delete $WS_UID --secretKey=$WS_SECRET
    objectuser delete $WS_UID
    objectuser delete $WS_TEST_UID

    #relogin with sanity testing user
    security login $SYSADMIN $SYSADMIN_PASSWORD
    if [ "$AUTH" != 'local' ] ; then
        security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
    fi
}



s3_baseurl_tests(){
    run baseurl sanity $NAMESPACE $WS_BUCKET1 --uid $WS_UID --secret=$WS_SECRET
}

SSH(){
    ip=$1
    cmd=$2
    opt=$3
    sshpass -p ${SYSADMIN_PASSWORD} ssh $opt -q -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null root@${ip} ${cmd}
}

SCP(){
    local ip=$1
    shift
    local args=("${@}")
    local len=${#args[@]}
    local src=${args[@]:0:${len}-1}
    local dst=${args[@]:${len}-1}
    sshpass -p ${SYSADMIN_PASSWORD} scp -q -r -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null $src root@$ip:$dst
}

ui_tests()
{
    run platform-ui kickstart
}

#
# add vnx device and use its storage
#
vnxblock_setup_once()
{
    # do this only once
    smisprovider show $VNX_SMIS_DEV &> /dev/null && return $?

    if [ $QUICK -eq 0 ]; then
       run smisprovider create $VNX_SMIS_DEV $VNX_SMIS_IP $VNX_SMIS_PORT $SMIS_USER "$SMIS_PASSWD" $VNX_SMIS_SSL
    else
       run smisprovider create $VNX_SMIS_DEV $SIMULATOR_SMIS_IP 6088 $SMIS_USER "$SMIS_PASSWD" false
    fi

    run storagedevice discover_all --ignore_error

    sleep 15
    vnxblock_cos_setup

    secho "VNX Block storagepools update"
    run storagepool update $VNXB_NATIVEGUID --type block --volume_type THIN_AND_THICK
    run storagepool update $VNXB_NATIVEGUID --type block --volume_type THICK_ONLY

    if [ $QUICK -eq 0 ]; then
       secho "VNX Block storageports update"
       if [ $DISCOVER_SAN -eq 0 ]; then
           run storageport update $VNXB_NATIVEGUID FC --tzone $NH/$FC_ZONE_A --group SP_A
           run storageport update $VNXB_NATIVEGUID FC --tzone $NH/$FC_ZONE_B --group SP_B
       fi
       run storageport update $VNXB_NATIVEGUID IP --tzone nh/iptz            
    fi

    run cos update block $COS_VNXBLOCK --storage $VNXB_NATIVEGUID
    run cos update block $COS_VNXBLOCK_FC --storage $VNXB_NATIVEGUID
    run cos update block $COS_VNXBLOCK_ISCSI --storage $VNXB_NATIVEGUID
    run cos update block $COS_VNXBLOCK_THIN --storage $VNXB_NATIVEGUID
    run cos update block $COS_VNXBLOCK_THICK --storage $VNXB_NATIVEGUID    
}

vnxblock_setup()
{
    secho "VNX Block Setup"
    vnxblock_setup_once
    run cos allow $COS_VNXBLOCK block $TENANT
    run cos allow $COS_VNXBLOCK_FC block $TENANT
    run cos allow $COS_VNXBLOCK_ISCSI block $TENANT
    run cos allow $COS_VNXBLOCK_THIN block $TENANT
    run cos allow $COS_VNXBLOCK_THICK block $TENANT
}


vmaxblock_setup_once()
{
    # do this only once
    smisprovider show $VMAX_SMIS_DEV &> /dev/null && return $?
 
    if [ $QUICK -eq 0 ]; then
       run smisprovider create $VMAX_SMIS_DEV $VMAX_SMIS_IP $VMAX_SMIS_PORT $SMIS_USER "$SMIS_PASSWD" $VMAX_SMIS_SSL
       if [ $VMAX3_COMPRESSION -eq 1 ]; then
       	  run smisprovider create $VMAX3_SMIS_AFA_DEV $VMAX3_SMIS_AFA_IP $VMAX_SMIS_PORT $SMIS_USER "$SMIS_PASSWD" $VMAX_SMIS_SSL       	   
       fi
       else
        if [ "$SIMULATOR_VMAX_SMIS_PORT" = "" ]; then
            # This can be removed in the skywalker release
            echo "Update your sanity.conf to the latest version and rerun, please."
            exit -1
        fi
       run smisprovider create $VMAX_SMIS_DEV $SIMULATOR_SMIS_IP $SIMULATOR_VMAX_SMIS_PORT $SMIS_USER "$SMIS_PASSWD" false
    fi

    run storagedevice discover_all --ignore_error

    vmax_cos_setup
    mirrorblock_cos_setup

    run storagepool update $VMAX_NATIVEGUID --type block --volume_type THIN_ONLY
    run storagepool update $VMAX_NATIVEGUID --type block --volume_type THICK_ONLY
    run storagepool update $VMAX_NATIVEGUID --nhadd $NH --type block
    if [ $VMAX3_COMPRESSION -eq 1 ]; then
        run storagepool update $VMAX3_AFA_NATIVEGUID --nhadd $NH --type block --volume_type THIN_ONLY
    fi
    
    if [ $QUICK -eq 0 ]; then
        echo "VMAX Block storageports update"
        if [ $DISCOVER_SAN -eq 0 ]; then
      
           for porta in ${VMAX_PORTS_A}
           do
	      run storageport update $VMAX_NATIVEGUID FC --tzone $FCTZ_A --group ${porta}
           done

	   for portb in ${VMAX_PORTS_B}
           do
              run storageport update $VMAX_NATIVEGUID FC --tzone $FCTZ_B --group ${portb}
           done
	   for port in ${VMAX3_AFA_PORTS}
           do
	      echo "adding port " $port
              run storageport update $VMAX_AFA_NATIVEGUID FC --tzone $FCTZ_A --group ${port}
           done
       fi
       security login $SYSADMIN $SYSADMIN_PASSWORD
       run storageport update $VMAX_NATIVEGUID IP --tzone nh/iptz
       security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
    fi

    run cos update block $COS_VMAXBLOCK --storage $VMAX_NATIVEGUID
    run cos update block $COS_VMAXBLOCK_FC --storage $VMAX_NATIVEGUID
    run cos update block $COS_VMAXBLOCK_ISCSI --storage $VMAX_NATIVEGUID
    run cos update block $COS_VMAXBLOCK_THIN --storage $VMAX_NATIVEGUID
 
    if [ $VMAX3_COMPRESSION -eq 1 ]; then
	    run cos update block $COS_VMAXBLOCK_V3_COMP_ENABLED --storage $VMAX3_AFA_NATIVEGUID
	    run cos update block $COS_VMAXBLOCK_V3_COMP_NOTENABLED --storage $VMAX3_AFA_NATIVEGUID
    fi
}

vmaxblock_setup()
{
    secho "VMAX Block setup"

    vmaxblock_setup_once
    run cos allow $COS_VMAXBLOCK block $TENANT
    run cos allow $COS_VMAXBLOCK_FC block $TENANT
    run cos allow $COS_VMAXBLOCK_ISCSI block $TENANT
    run cos allow $COS_VMAXBLOCK_THIN block $TENANT
}

vnxblock_flex_varray_setup()
{
    vnxblock_setup

    # do manual assignment of ports
    run transportzone update $FC_ZONE_A --remNeighborhoods $NH
    run transportzone update $FC_ZONE_B --remNeighborhoods $NH
    run transportzone update $IP_ZONE --remNeighborhoods $NH

    run storageport update $VNXB_NATIVEGUID FC --addvarrays $NH --group SP_A
    run storageport update $VNXB_NATIVEGUID FC --addvarrays $NH --group SP_B
    run storageport update $VNXB_NATIVEGUID IP --addvarrays $NH
}


vmaxblock_flex_varray_setup()
{
    vmaxblock_setup

    # remove network assignment and assign ports manually
    run transportzone update $FC_ZONE_A --remNeighborhoods $NH
    run transportzone update $FC_ZONE_B --remNeighborhoods $NH
    run transportzone update $IP_ZONE --remNeighborhoods $NH
    for porta in ${VMAX_PORTS_A}
    do
        run storageport update $VMAX_NATIVEGUID FC --addvarrays $NH --group ${porta}
    done
    for portb in ${VMAX_PORTS_B}
    do
        run storageport update $VMAX_NATIVEGUID FC --addvarrays $NH --group ${portb}
    done
    run storageport update $VMAX_NATIVEGUID IP --addvarrays $NH
}

#
# VMAX and VNX export group tests
#
combined_block_setup()
{
    vnxblock_setup
    vmaxblock_setup
}

# Block Mirror setup
# 
mirrorblock_setup()
{
    vmaxblock_setup
    #vnxblock_setup

    run cos allow $COS_MIRROR block $TENANT
    run cos update block $COS_MIRROR --storage $VMAX_NATIVEGUID

    run cos allow $COS_MIRROR_WITH_OPTIONAL block $TENANT
    run cos update block $COS_MIRROR_WITH_OPTIONAL --storage $VMAX_NATIVEGUID

    run cos allow $COS_MIRROR_WITH_2_MIRRORS block $TENANT
    run cos update block $COS_MIRROR_WITH_2_MIRRORS --storage $VMAX_NATIVEGUID

    run cos allow $COS_MIRROR_BEFORE_CHANGE block $TENANT
    run cos update block $COS_MIRROR_BEFORE_CHANGE --storage $VMAX_NATIVEGUID

    run cos allow $COS_MIRROR_AFTER_CHANGE block $TENANT
    run cos update block $COS_MIRROR_AFTER_CHANGE --storage $VMAX_NATIVEGUID

    #run cos allow $COS_MIRROR_VNX block $TENANT
    #run cos update block $COS_MIRROR_VNX --storage $VNXB_NATIVEGUID    

    run cos allow $COS_VMAX_CG_MIRROR block $TENANT
    run cos update block $COS_VMAX_CG_MIRROR --storage $VMAX_NATIVEGUID
}

#
# syssvc tests
#
syssvc_setup()
{
    echo "syssvc setup, do nothing."
}

security_setup()
{
    echo "Enable root ssh permit"
    syssvc $CONFIG_FILE $BOURNE_IPADDR set_prop system_permit_root_ssh yes
}

all_setup()
{
#    ui_setup
#    webstorage_setup
    isilon_setup
#    vplex_setup
#    vnxfile_setup
    netapp_setup
#    datadomainfile_setup
    vnxblock_setup
    vmaxblock_setup
    mirrorblock_setup
    syssvc_setup
}

#
# Login, Configure SMTP, and add controller and object licenses
#
login_nd_configure_smtp_nd_add_licenses()
{
    security login $SYSADMIN $SYSADMIN_PASSWORD
    root_tenant=`tenant root|tail -1`
    echo "Verifying bulk POST request before license"
    tenant bulk_post "$root_tenant"
    echo "Finished verifying bulk POST request before license"
    echo "Configuring smtp and adding object and controller licenses."
    syssvc $CONFIG_FILE "$BOURNE_IP" setup
    echo "Finished Configuring smtp and adding licenses."
}

#
# setup cos, zone, project and add storage devices to bourne for tests
#
common_setup()
{
    sec_start_ldap_server
    login_nd_configure_smtp_nd_add_licenses

    # By default with automated tests, we don't want to suspend on error or have specific methods stop us.
    run syssvc $CONFIG_FILE $BOURNE_IPADDR set_prop workflow_suspend_on_error false
    run syssvc $CONFIG_FILE $BOURNE_IPADDR set_prop workflow_suspend_on_class_method "none"

    run syssvc $CONFIG_FILE "$BOURNE_IP" set_prop system_proxyuser_encpassword ${SYSADMIN_PASSWORD}
    tenant_setup

    if [ "${RP_INGESTTESTS}" = "0" -o "${RP_INGESTTESTS}" = "" ]; then
            # If we're running physical, DISCOVER_SAN needs to be on for physical configurations.
        if [ $QUICK -eq 0 -a "${SBSDK_QUICK_PARAM}" != "quick" -a "${VPLEX_QUICK_PARAM}" != "quick" -a "${RP_QUICK_PARAM}" != "quick" -a "${XIO_QUICK_PARAM}" != quick ]; then
            secho "Turning on DISCOVER_SAN because we are testing physical"
            DISCOVER_SAN=1
        fi

        # RecoverPoint doesn't need zone setup, it uses its own zone setup
        if [ "${SS}" != "recoverpoint" -a "${SS}" != "srdf" ]; then
          secho "Run zone setup"
          zone_setup
          run transportzone add $NH/$IP_ZONE $BLK_CLIENT_iSCSI
        fi
    fi

    if [ "${SS}" != "srdf" ]; then
        project_setup
        projectid=$(project query $PROJECT)
        secho "Project id of $PROJECT is $projectid."

        if [ "${SS}" != "recoverpoint" ]; then
            ROOT_TENANT=`tenant root|tail -1`
            if [ "${RP_INGESTTESTS}" = "0" ]; then
                secho "Setup ACLs on neighborhood for $TENANT"
                run neighborhood allow $NH $TENANT
                run neighborhood allow $NH2 $TENANT
                run neighborhood allow $NH $ROOT_TENANT
                run neighborhood allow $NH2 $ROOT_TENANT
                secho "Setup hosts and clusters for $TENANT"
                host_setup
            fi
        fi
    fi
}

blocksnapshot_setup()
{
    vnxblock_setup
    vmaxblock_setup
}

blocksnapshot_single_vnx()
{
    snaptest_vol="${VNX_VOLUME}-snaptest"
    run volume create ${snaptest_vol} $PROJECT $NH $COS_VNXBLOCK 1280000000

    # VMAX snap tests
    vnx_snap1_label=vnx_snap1-${HOSTNAME}-${RANDOM}
    vnx_snap2_label=vnx_snap2-${HOSTNAME}-${RANDOM}
    run blocksnapshot create $PROJECT/${snaptest_vol} $vnx_snap1_label
    if [ "$EXTRA_PARAM" = "search" ] ; then
        blocksnapshot search $(echo $snaptest_vol| head -c 2)
        blocksnapshot search $(echo $snaptest_vol | head -c 2) --project $projectid

        blocksnapshot tag $PROJECT/${snaptest_vol}/${vnx_snap1_label} $TAG
        blocksnapshot search $SEARCH_PREFIX --scope $TENANT --tag true
    fi

    run blocksnapshot create $PROJECT/${snaptest_vol} $vnx_snap2_label
    run blocksnapshot list $PROJECT/${snaptest_vol}
    run blocksnapshot show $PROJECT/${snaptest_vol}/${vnx_snap1_label}
    run blocksnapshot show $PROJECT/${snaptest_vol}/${vnx_snap2_label}
    run blocksnapshot restore $PROJECT/${snaptest_vol}/${vnx_snap2_label}
    run blocksnapshot delete $PROJECT/${snaptest_vol}/${vnx_snap1_label}
    run blocksnapshot delete $PROJECT/${snaptest_vol}/${vnx_snap2_label}
    run volume delete $PROJECT/${snaptest_vol} --wait

    # ----------------------------------
    # Run tests with activate operations
    # ----------------------------------
    activate_vol="${snaptest_vol}-ac"
    run volume create ${activate_vol} $PROJECT $NH $COS_VNXBLOCK 1280000000

    # Snapshot without create_inactive operation specified ==> Use default
    snap1_label=snap1-${HOSTNAME}-${RANDOM}
    run blocksnapshot create ${PROJECT}/${activate_vol} $snap1_label
    run blocksnapshot show ${PROJECT}/${activate_vol}/${snap1_label}
    run blocksnapshot activate ${PROJECT}/${activate_vol}/${snap1_label}
    run blocksnapshot delete ${PROJECT}/${activate_vol}/${snap1_label}

    # Snapshot with create_inactive=true
    inactive_snap_label=inactive-snap-${HOSTNAME}-${RANDOM}
    run blocksnapshot create ${PROJECT}/${activate_vol} $inactive_snap_label --create_inactive=true
    run blocksnapshot show ${PROJECT}/${activate_vol}/${inactive_snap_label}
    run blocksnapshot activate ${PROJECT}/${activate_vol}/${inactive_snap_label}
    run blocksnapshot delete ${PROJECT}/${activate_vol}/${inactive_snap_label}

    # Snapshot with create_inactive=false
    active_snap_label=active-snap-${HOSTNAME}-${RANDOM}
    run blocksnapshot create ${PROJECT}/${activate_vol} $active_snap_label --create_inactive=false
    run blocksnapshot show ${PROJECT}/${activate_vol}/${active_snap_label}
    run blocksnapshot activate ${PROJECT}/${activate_vol}/${active_snap_label}
    run blocksnapshot restore ${PROJECT}/${activate_vol}/${active_snap_label}
    run blocksnapshot delete ${PROJECT}/${activate_vol}/${active_snap_label}

    # Cleanup
    run volume delete ${PROJECT}/${activate_vol} --wait
}

blocksnapshot_single_vmax()
{
    snaptest_vol="${VMAX_VOLUME}-snaptest"

    run volume create ${snaptest_vol} $PROJECT $NH $COS_VMAXBLOCK 1280000000 --thinVolume true

    # VMAX snap tests
    vmx_snap1_label=vmx_snap1-${HOSTNAME}-${RANDOM}
    vmx_snap2_label=vmx_snap2-${HOSTNAME}-${RANDOM}
    run blocksnapshot create $PROJECT/${snaptest_vol} $vmx_snap1_label
    run blocksnapshot create $PROJECT/${snaptest_vol} $vmx_snap2_label
    run blocksnapshot list $PROJECT/${snaptest_vol}
    run blocksnapshot show $PROJECT/${snaptest_vol}/${vmx_snap1_label}
    run blocksnapshot show $PROJECT/${snaptest_vol}/${vmx_snap2_label}
    run blocksnapshot restore $PROJECT/${snaptest_vol}/${vmx_snap2_label}
    run blocksnapshot delete $PROJECT/${snaptest_vol}/${vmx_snap1_label}
    run blocksnapshot delete $PROJECT/${snaptest_vol}/${vmx_snap2_label}

    system_type=`volume show $PROJECT/${snaptest_vol} | grep system_type | awk -F\" '{print $4}'`
    # delete VMAX3 snapshot session
    if [ "$system_type" = "vmax3" ] ; then
        echo "Delete snapshot sessions for volume ${snaptest_vol}"
        run snapshotsession delete $PROJECT/${snaptest_vol}/${vmx_snap1_label}
        run snapshotsession delete $PROJECT/${snaptest_vol}/${vmx_snap2_label}
    fi

    run volume delete $PROJECT/${snaptest_vol} --wait

    # ----------------------------------
    # Run tests with activate operations
    # ----------------------------------
    activate_vol="${snaptest_vol}-ac"
    run volume create ${activate_vol} $PROJECT $NH $COS_VMAXBLOCK 1280000000 --thinVolume true

    # Snapshot without create_inactive operation specified ==> Use default
    snap1_label=snap1-${HOSTNAME}-${RANDOM}
    run blocksnapshot create ${PROJECT}/${activate_vol} $snap1_label
    run blocksnapshot show ${PROJECT}/${activate_vol}/${snap1_label}
    run blocksnapshot activate ${PROJECT}/${activate_vol}/${snap1_label}
    run blocksnapshot delete ${PROJECT}/${activate_vol}/${snap1_label}

    # Snapshot with create_inactive=true
	# ---------------------------------------------------------------------------------------------
	# Creating inactive snapshot is not applicable for V3 array. hence commenting out this section.
	# ---------------------------------------------------------------------------------------------
    # inactive_snap_label=inactive-snap-${HOSTNAME}-${RANDOM}
    # run blocksnapshot create ${PROJECT}/${activate_vol} $inactive_snap_label --create_inactive=true
    # run blocksnapshot show ${PROJECT}/${activate_vol}/${inactive_snap_label}
    # run blocksnapshot activate ${PROJECT}/${activate_vol}/${inactive_snap_label}
    # run blocksnapshot delete ${PROJECT}/${activate_vol}/${inactive_snap_label}
	# ---------------------------------------------------------------------------------------------

    # Snapshot with create_inactive=false
    active_snap_label=active-snap-${HOSTNAME}-${RANDOM}
    run blocksnapshot create ${PROJECT}/${activate_vol} $active_snap_label --create_inactive=false
    run blocksnapshot show ${PROJECT}/${activate_vol}/${active_snap_label}
    run blocksnapshot activate ${PROJECT}/${activate_vol}/${active_snap_label}
    run blocksnapshot restore ${PROJECT}/${activate_vol}/${active_snap_label}
    run blocksnapshot delete ${PROJECT}/${activate_vol}/${active_snap_label}

    # Cleanup
    system_type=`volume show $PROJECT/${activate_vol} | grep system_type | awk -F\" '{print $4}'`
    if [ "$system_type" = "vmax3" ] ; then
        echo "Delete snapshot sessions for volume ${activate_vol}"
        run snapshotsession delete $PROJECT/${activate_vol}/${snap1_label}
        run snapshotsession delete $PROJECT/${activate_vol}/${active_snap_label}
    fi

    run volume delete ${PROJECT}/${activate_vol} --wait
}

blocksnapshot_consistency_group_vnx()
{
    snaptest_vol="${VNX_VOLUME}-cg"
    consistency_group=`openssl passwd "$RANDOM" | tr -cd [:alnum:] | cut -c1-8`

    # Create consistency group
    run blockconsistencygroup create $PROJECT $consistency_group

    # Create volumes
    run volume create ${snaptest_vol}1 $PROJECT $NH $COS_VNXBLOCK 1280000000 --consistencyGroup consistency_group
    run volume create ${snaptest_vol}2 $PROJECT $NH $COS_VNXBLOCK 1280000000 --consistencyGroup consistency_group
    run volume create ${snaptest_vol}3 $PROJECT $NH $COS_VNXBLOCK 1280000000 --consistencyGroup consistency_group
    # Create snaps
    snap1_label=snap1-${HOSTNAME}-${RANDOM}
    snap2_label=snap2-${HOSTNAME}-${RANDOM}
    snap3_label=snap3-${HOSTNAME}-${RANDOM}
    run blocksnapshot create $PROJECT/${snaptest_vol}1 $snap1_label
    run blocksnapshot create $PROJECT/${snaptest_vol}1 $snap2_label
    run blocksnapshot create $PROJECT/${snaptest_vol}1 $snap3_label
    # Show it
    run blocksnapshot list $PROJECT/${snaptest_vol}1
    run blocksnapshot list $PROJECT/${snaptest_vol}2
    run blocksnapshot list $PROJECT/${snaptest_vol}3
    # Restore
    run blocksnapshot restore $PROJECT/${snaptest_vol}2/${snap2_label}
    # Clean up - delete one snap, will delete all in snapset
    run blocksnapshot delete $PROJECT/${snaptest_vol}1/${snap1_label}
    run blocksnapshot delete $PROJECT/${snaptest_vol}1/${snap2_label}
    run blocksnapshot delete $PROJECT/${snaptest_vol}1/${snap3_label}
    # Delete
    run volume delete $PROJECT/${snaptest_vol}1 --wait
    run volume delete $PROJECT/${snaptest_vol}2 --wait
    run volume delete $PROJECT/${snaptest_vol}3 --wait
    run blockconsistencygroup delete $consistency_group

    # --------------------------------------------
    # Consistency Group snap tests with activation
    # --------------------------------------------
    consistency_group=`openssl passwd "$RANDOM" | tr -cd [:alnum:] | cut -c1-8`

    # Create consistency group
    run blockconsistencygroup create $PROJECT $consistency_group

    volumename="${VNX_VOLUME}-CG-VOL-TO-SNAP"
    cg_vol1=${volumename}1
    cg_vol2=${volumename}2
    run volume create ${cg_vol1} $PROJECT $NH $COS_VNXBLOCK 1280000000 --consistencyGroup consistency_group
    run volume create ${cg_vol2} $PROJECT $NH $COS_VNXBLOCK 1280000000 --consistencyGroup consistency_group

    # Create CG snap with create_inactive=true
    inactive_snap_label=inactive-snap--${HOSTNAME}-${RANDOM}
    run blocksnapshot create $PROJECT/${cg_vol1} $inactive_snap_label --create_inactive=true
    # This requires an OPT to be fixed for it work
    # run blocksnapshot activate $PROJECT/${cg_vol1}/${inactive_snap_label}
    # run blocksnapshot restore $PROJECT/${cg_vol1}/${inactive_snap_label}
    run blocksnapshot show $PROJECT/${cg_vol1}/${inactive_snap_label}

    # Create CG snap with create_inactive=false
    active_snap_label=active-snap-${HOSTNAME}-${RANDOM}
    run blocksnapshot create $PROJECT/${cg_vol1} $active_snap_label --create_inactive=false
    run blocksnapshot show $PROJECT/${cg_vol1}/${active_snap_label}
    run blocksnapshot activate $PROJECT/${cg_vol1}/${active_snap_label}
    run blocksnapshot restore $PROJECT/${cg_vol1}/${active_snap_label}

    # Create CG snap without specifying create_inactive value
    snap_label=snap-${HOSTNAME}-${RANDOM}
    run blocksnapshot create $PROJECT/${cg_vol1} $snap_label
    run blocksnapshot show $PROJECT/${cg_vol1}/${snap_label}
    run blocksnapshot activate $PROJECT/${cg_vol1}/${snap_label}
    run blocksnapshot restore $PROJECT/${cg_vol1}/${snap_label}
    run blocksnapshot delete $PROJECT/${cg_vol1}/${snap_label}

    # Clean up
    run blocksnapshot delete $PROJECT/${cg_vol1}/${inactive_snap_label}
    run blocksnapshot delete $PROJECT/${cg_vol1}/${active_snap_label}
    run volume delete $PROJECT/${cg_vol1} --wait
    run volume delete $PROJECT/${cg_vol2} --wait
    run blockconsistencygroup delete $consistency_group
}

blocksnapshot_consistency_group_vmax()
{
    snaptest_vol="${VMAX_VOLUME}-cg"
    consistency_group=`openssl passwd "$RANDOM" | tr -cd [:alnum:] | cut -c1-8`
	
    # Create volumes
    run volume create ${snaptest_vol}1 $PROJECT $NH $COS_VMAXBLOCK_FC 1280000000 --consistencyGroup consistency_group
    run volume create ${snaptest_vol}2 $PROJECT $NH $COS_VMAXBLOCK_FC 1280000000 --consistencyGroup consistency_group
    # Create snaps
    snap1_label=snap1-${HOSTNAME}-${RANDOM}
    snap2_label=snap2-${HOSTNAME}-${RANDOM}
    run blocksnapshot create $PROJECT/${snaptest_vol}1 $snap1_label 
    run blocksnapshot create $PROJECT/${snaptest_vol}1 $snap2_label
    # Show it
    run blocksnapshot list $PROJECT/${snaptest_vol}1
    run blocksnapshot list $PROJECT/${snaptest_vol}2
    # Restore
    # run blocksnapshot restore $PROJECT/${snaptest_vol}2/${snap1_label}
    # Clean up
    run blocksnapshot delete $PROJECT/${snaptest_vol}1/${snap1_label}
    run blocksnapshot delete $PROJECT/${snaptest_vol}1/${snap2_label}
    # Delete
    run volume delete $PROJECT/${snaptest_vol}1 --wait
    run volume delete $PROJECT/${snaptest_vol}2 --wait

    # --------------------------------------------
    # Consistency Group snap tests with activation
    # --------------------------------------------
    volumename="${VMAX_VOLUME}-CG-VOL-TO-SNAP"
    cg_vol1=${volumename}1
    cg_vol2=${volumename}2
    consistency_group=`openssl passwd "$RANDOM" | tr -cd [:alnum:] | cut -c1-8`
    
    run volume create ${cg_vol1} $PROJECT $NH $COS_VMAXBLOCK 1280000000 --consistencyGroup consistency_group
    run volume create ${cg_vol2} $PROJECT $NH $COS_VMAXBLOCK 1280000000 --consistencyGroup consistency_group

    # Create CG snap with create_inactive=true
    nactive_snap_label=inactive-snap--${HOSTNAME}-${RANDOM}
    run blocksnapshot create $PROJECT/${cg_vol1} $inactive_snap_label --create_inactive=true
    # This requires an OPT to be fixed for it work
    # run blocksnapshot activate $PROJECT/${cg_vol1}/${inactive_snap_label}
    # run blocksnapshot restore $PROJECT/${cg_vol1}/${inactive_snap_label}
    run blocksnapshot show $PROJECT/${cg_vol1}/${inactive_snap_label}

    # Create CG snap with create_inactive=false
    active_snap_label=active-snap-${HOSTNAME}-${RANDOM}
    run blocksnapshot create $PROJECT/${cg_vol1} $active_snap_label --create_inactive=false
    run blocksnapshot show $PROJECT/${cg_vol1}/${active_snap_label}
    run blocksnapshot activate $PROJECT/${cg_vol1}/${active_snap_label}
    run blocksnapshot restore $PROJECT/${cg_vol1}/${active_snap_label}

    # Create CG snap without specifying create_inactive value
    #snap_label=snap-${HOSTNAME}-${RANDOM}
    #run blocksnapshot create $PROJECT/${cg_vol1} $snap_label
    #run blocksnapshot show $PROJECT/${cg_vol1}/${snap_label}
    #run blocksnapshot activate $PROJECT/${cg_vol1}/${snap_label}
    #run blocksnapshot restore $PROJECT/${cg_vol1}/${snap_label}
    #run blocksnapshot delete $PROJECT/${cg_vol1}/${snap_label}

    # Clean up
    run blocksnapshot delete $PROJECT/${cg_vol1}/${inactive_snap_label}
    run blocksnapshot delete $PROJECT/${cg_vol1}/${active_snap_label}
    run volume delete $PROJECT/${cg_vol1} --wait
    run volume delete $PROJECT/${cg_vol2} --wait
}

blocksnapshot_tests()
{
    blocksnapshot_single_vnx
    blocksnapshot_single_vmax

#    blocksnapshot_consistency_group_vnx
#    blocksnapshot_consistency_group_vmax
}


#
# fileshare tests
#
file_tests()
{
    echo "File tests started"
    cos=$1; shift
    perms=${@}
    datetime=`date +%m%d%y%H%M%S`
    fsname=fs-$cos-$macaddr-$datetime
    fsname=$(echo $fsname | sed s/-/_/g)
    fsname=$(echo $fsname | sed s/:/_/g)
    echo $fsname
    
	if [ $cos = $COS_VNXFILE ] ; then
	    run vnas list
	fi
    
    run fileshare create $fsname $PROJECT $NH $cos $FS_SIZEMB
    if [ "$EXTRA_PARAM" = "search" ] ; then
        run fileshare search --name $(echo $fsname | head -c 2)
        run fileshare search --name $(echo $fsname | head -c 2) --project $projectid

        run fileshare tag ${PROJECT}/${fsname} $TAG
        run fileshare search --tag $SEARCH_PREFIX 
    fi

    run fileshare show $PROJECT/$fsname
    run fileshare expand $PROJECT/$fsname $FS_EXPAND_SIZE
    run fileshare show $PROJECT/$fsname
    if [ $cos = $COS_ISIFILE ] ; then
        run snapshot create $PROJECT/$fsname $fsname-$datetime
        if [ "$EXTRA_PARAM" = "search" ] ; then
            snapshot search $(echo $fsname| head -c 2)
            snapshot search $(echo $fsname| head -c 2) --project $projectid

            snapshot tag $PROJECT/$fsname-$datetime $PROJECT/${fsname} $TAG
            snapshot search $SEARCH_PREFIX --scope $TENANT --tag true
        fi
        run snapshot create $PROJECT/$fsname $fsname-$datetime-2
        run bulkapi filesnapshots $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
    fi

    for p in $perms
    do
        case $p in
        ro)
            exp_args="$FSEXP_RO_EPS --perm ro --comments TESTCOMMENTS"
            exp_upd_args="--readonlyhosts $FSEXP1"
            snap_exp_args="$FSEXP_RO_EPS --perm ro"
            snap_share_args="--perm read"
            ;;
        rw)
            exp_args="$FSEXP_RW_EPS --perm rw --comments TESTCOMMENTS"
            exp_upd_args="--readwritehosts $FSEXP2"
            snap_exp_args="$FSEXP_RW_EPS --perm ro"
            snap_share_args="--perm read"
            ;;
        root)
            exp_args="$FSEXP_ROOT_EPS --perm root --comments TESTCOMMENTS"
            exp_upd_args="--roothosts $FSEXP3"
            snap_exp_args="$FSEXP_ROOT_EPS --perm ro"
            snap_share_args="--perm read"
            ;;
        default)
            exp_args="$FSEXP_DEFAULT_EPS --comments TESTCOMMENTS"
            exp_upd_args="--readwritehosts $FSEXP4"
            snap_exp_args="$FSEXP_DEFAULT_EPS"
            snap_share_args="--perm read"
            ;;
        esac
        echo "exp args = $exp_args"
        run fileshare export $PROJECT/$fsname $exp_args
        run fileshare show $PROJECT/$fsname

        echo "exp upd args = $exp_upd_args"
        run fileshare export_update $PROJECT/$fsname $exp_upd_args --operation modify --securityflavor sys 

        if [ $cos != $COS_VNXE ] ; then
            let "FS_EXPAND_SIZE = $FS_SIZE + $FS_EXPAND_SIZE"
            run fileshare expand $PROJECT/$fsname $FS_EXPAND_SIZE
        fi

        if [ $cos = $COS_VNXFILE ] ; then
            run snapshot create $PROJECT/$fsname $fsname-$datetime
            run snapshot export $PROJECT/$fsname-$datetime $PROJECT/$fsname $snap_exp_args
            run snapshot show $PROJECT/$fsname-$datetime $PROJECT/$fsname
            run snapshot restore $PROJECT/$fsname-$datetime $PROJECT/$fsname
            run snapshot unexport $PROJECT/$fsname-$datetime $PROJECT/$fsname
            run snapshot delete $PROJECT/$fsname-$datetime $PROJECT/$fsname
            run snapshot create $PROJECT/$fsname $fsname-$datetime-2
            run snapshot export $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $SNAPEXP_DEFAULT_EPS
            run snapshot show $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
            run snapshot unexport $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
            run snapshot delete $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
            run snapshot create $PROJECT/$fsname $fsname-$datetime-3
            run snapshot export $PROJECT/$fsname-$datetime-3 $PROJECT/$fsname $SNAPEXP_SECKRP_EPS
            run snapshot show $PROJECT/$fsname-$datetime-3 $PROJECT/$fsname
            run snapshot unexport $PROJECT/$fsname-$datetime-3 $PROJECT/$fsname
            run snapshot show $PROJECT/$fsname-$datetime-3 $PROJECT/$fsname
            run snapshot delete $PROJECT/$fsname-$datetime-3 $PROJECT/$fsname
        fi
        run fileshare unexport $PROJECT/$fsname

    	if [ $cos = $COS_NETAPP ] ; then
            run snapshot create $PROJECT/$fsname $fsname-$datetime-2
            run snapshot export $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $snap_exp_args
            run snapshot show $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
            run snapshot restore $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
            run snapshot unexport $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
            run snapshot share $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPF_SMBSNAPSHARE1 --description 'New_SNAPSHOT_SMB_Share1' --perm 'read'
	        run snapshot share $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPF_SMBSNAPSHARE2 --description 'New_SNAPSHOT_SMB_Share2' --perm 'read'
            echo "Snapshot Share ACL testing for NetApp7 Started===========>>>"
			run snapshot share_acl $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPF_SMBSNAPSHARE1 --user 'Everyone' --permission 'read' --operation add
            run snapshot share_acl_show $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPF_SMBSNAPSHARE1
	        run snapshot share_acl $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPF_SMBSNAPSHARE1 --user 'Everyone' --permission 'read' --operation modify
            run snapshot share_acl $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPF_SMBSNAPSHARE1 --user 'Everyone' --permission 'read' --operation delete
	        run snapshot share_acl $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPF_SMBSNAPSHARE2 --user 'Everyone' --permission 'read' --operation add
    	    run snapshot share_acl_delete $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname  $NETAPPF_SMBSNAPSHARE2
			echo "Snapshot Share ACL testing for NetApp7 Finished===========<<<"
            run snapshot unshare $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPF_SMBSNAPSHARE1 
	        run snapshot unshare $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPF_SMBSNAPSHARE2 
            run snapshot delete $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
        fi

        if [ $cos = $COS_NETAPPC ] ; then
            run snapshot create $PROJECT/$fsname $fsname-$datetime-2
            run snapshot show $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname	    
           run snapshot share $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPCF_SMBSNAPSHARE1 --description 'New_SNAPSHOT_SMB_Share1' --perm 'read'
	        run snapshot share $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPCF_SMBSNAPSHARE2 --description 'New_SNAPSHOT_SMB_Share2' --perm 'read'
			echo "Snapshot Share ACL testing for NetApp Cluster Started===========>>>"
            run snapshot share_acl $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPCF_SMBSNAPSHARE1 --user 'Everyone' --permission 'read' --operation add
            run snapshot share_acl_show $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPCF_SMBSNAPSHARE1
	        run snapshot share_acl $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPCF_SMBSNAPSHARE1 --user 'Everyone' --permission 'read' --operation modify
            run snapshot share_acl $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPCF_SMBSNAPSHARE1 --user 'Everyone' --permission 'read' --operation delete
	        run snapshot share_acl $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPCF_SMBSNAPSHARE2 --user 'Everyone' --permission 'read' --operation add
    	    run snapshot share_acl_delete $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname  $NETAPPCF_SMBSNAPSHARE2
			echo "Snapshot Share ACL testing for NetApp Cluster done===========<<<"
            run snapshot unshare $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPCF_SMBSNAPSHARE1 
	        run snapshot unshare $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPCF_SMBSNAPSHARE2
		    run snapshot delete $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
        fi
        
        if [ $cos = $COS_VNXE ] ; then
            run snapshot create $PROJECT/$fsname $fsname-$datetime-2
            run snapshot restore $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
            run snapshot export $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $snap_exp_args
            run snapshot show $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
            run snapshot unexport $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
            run snapshot delete $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
        fi

        if [ $cos = $COS_UNITY ] ; then
            run snapshot create $PROJECT/$fsname $fsname-$datetime-2
            # Commenting the below test since the restore operation does rename the snapshot name to timestamp name
            # This will in turn fail the file system delete test case.
            # run snapshot restore $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
            run snapshot export $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $snap_exp_args
            run snapshot show $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
            run snapshot unexport $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
            run snapshot delete $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
        fi

        
    done

    # Run only for Isilon (temporary until vnx implements smb)
    if [ $cos = $COS_ISIFILE ] ; then

        run fileshare share $PROJECT/$fsname  $ISI_SMBFILESHARE1 --description 'New_SMB_Share'
        run fileshare share $PROJECT/$fsname  $ISI_SMBFILESHARE2 --description 'New_SMB_Share2'
		echo "File Share ACL testing for ISILON Started"
        run fileshare share_acl $PROJECT/$fsname  $ISI_SMBFILESHARE1 --user 'Everyone' --permission 'change' --operation add
	    run fileshare share_acl_show $PROJECT/$fsname  $ISI_SMBFILESHARE1
	    run fileshare share_acl $PROJECT/$fsname  $ISI_SMBFILESHARE1 --user 'Everyone' --permission 'read' --operation modify
	    run fileshare share_acl $PROJECT/$fsname  $ISI_SMBFILESHARE1 --user 'Everyone' --permission 'read' --operation delete
	    run fileshare share_acl $PROJECT/$fsname  $ISI_SMBFILESHARE2 --user 'Everyone' --permission 'read' --operation add
    	run fileshare share_acl_delete $PROJECT/$fsname  $ISI_SMBFILESHARE2
		echo "File Share ACL testing for ISILON done"
        run fileshare show $PROJECT/$fsname

	    # Deleting the FileSystem Shares.
        run fileshare unshare $PROJECT/$fsname $ISI_SMBFILESHARE1
        run fileshare unshare $PROJECT/$fsname $ISI_SMBFILESHARE2
    fi

    # Run only for Data Domain
    if [ $cos = $COS_DDFILE ] ; then
        echo "DD SMB share tests started...create 2 fileshares"
        run fileshare share $PROJECT/$fsname  $DATADOMAINF_SMBFILESHARE1 --description 'New_SMB_Share'
        run fileshare share $PROJECT/$fsname  $DATADOMAINF_SMBFILESHARE2 --description 'New_SMB_Share2'
        echo "DD SMB share tests started...create 2 fileshares done"
        run fileshare show $PROJECT/$fsname

        # Deleting the FileSystem Shares.
        echo "Delete 1st fileshare..."
        run fileshare unshare $PROJECT/$fsname $DATADOMAINF_SMBFILESHARE1
        echo "Delete 1st fileshare done...deleting 2nd fileshare"
        run fileshare unshare $PROJECT/$fsname $DATADOMAINF_SMBFILESHARE2
        echo "Delete fileshares done"

    fi
    
    # Run only for Netapp 
    if [ $cos = $COS_NETAPP ] ; then
        run fileshare share $PROJECT/$fsname  $NETAPPF_SMBFILESHARE1 --description 'New_SMB_Share'
		run fileshare share $PROJECT/$fsname  $NETAPPF_SMBFILESHARE2 --description 'New_SMB_Share2'
        run fileshare show $PROJECT/$fsname
		echo "File Share ACL testing for NetApp7 Started"
		run fileshare share_acl $PROJECT/$fsname  $NETAPPF_SMBFILESHARE1 --user 'Everyone' --permission 'change' --operation add
	    run fileshare share_acl_show $PROJECT/$fsname  $NETAPPF_SMBFILESHARE1
	    run fileshare share_acl $PROJECT/$fsname  $NETAPPF_SMBFILESHARE1 --user 'Everyone' --permission 'read' --operation modify
	    run fileshare share_acl $PROJECT/$fsname  $NETAPPF_SMBFILESHARE1 --user 'Everyone' --permission 'read' --operation delete
	    run fileshare share_acl $PROJECT/$fsname  $NETAPPF_SMBFILESHARE2 --user 'Everyone' --permission 'read' --operation add
    	run fileshare share_acl_delete $PROJECT/$fsname  $NETAPPF_SMBFILESHARE2
		echo "File Share ACL testing for NetApp7 Finished"
        run fileshare unshare $PROJECT/$fsname $NETAPPF_SMBFILESHARE1
		run fileshare unshare $PROJECT/$fsname $NETAPPF_SMBFILESHARE2
		
    fi

    if [ $cos = $COS_NETAPPC ] ; then
        run fileshare share $PROJECT/$fsname  $NETAPPCF_SMBFILESHARE1 --description 'New_SMB_Share'
		run fileshare share $PROJECT/$fsname  $NETAPPCF_SMBFILESHARE2 --description 'New_SMB_Share2'
        run fileshare show $PROJECT/$fsname
		echo "File Share ACL testing for NetApp Cluster Started"
		run fileshare share_acl $PROJECT/$fsname  $NETAPPCF_SMBFILESHARE1 --user 'Everyone' --permission 'change' --operation add
	    run fileshare share_acl_show $PROJECT/$fsname  $NETAPPCF_SMBFILESHARE1
	    run fileshare share_acl $PROJECT/$fsname  $NETAPPCF_SMBFILESHARE1 --user 'Everyone' --permission 'read' --operation modify
	    run fileshare share_acl $PROJECT/$fsname  $NETAPPCF_SMBFILESHARE1 --user 'Everyone' --permission 'read' --operation delete
	    run fileshare share_acl $PROJECT/$fsname  $NETAPPCF_SMBFILESHARE2 --user 'Everyone' --permission 'read' --operation add
    	run fileshare share_acl_delete $PROJECT/$fsname  $NETAPPCF_SMBFILESHARE2
		echo "File Share ACL testing for NetApp Cluster Finished"
        run fileshare unshare $PROJECT/$fsname $NETAPPCF_SMBFILESHARE1
		run fileshare unshare $PROJECT/$fsname $NETAPPCF_SMBFILESHARE2
    fi
    
    if [ $cos = $COS_VNXFILE ] ; then
        fscifs=filesysCifs-$datetime
        run fileshare create $fscifs $PROJECT $NH $cos $FS_SIZEMB
        run fileshare share $PROJECT/$fscifs  $VNXF_SMBFILESHARE1 --description 'New_SMB_Share'
        run fileshare show $PROJECT/$fscifs
        run fileshare unshare $PROJECT/$fscifs $VNXF_SMBFILESHARE1
        run fileshare delete $PROJECT/$fscifs
    fi
    
    if [ $cos = $COS_VNXE ] ; then
        run fileshare share $PROJECT/$fsname  $VNXE_SMBFILESHARE1 --description 'New_SMB_Share'
        run fileshare show $PROJECT/$fsname
        run fileshare unshare $PROJECT/$fsname $VNXE_SMBFILESHARE1 
    fi

   if [ $cos = $COS_UNITY ] ; then
        fscifs=filesysCifs-$datetime
        run fileshare create $fscifs $PROJECT $NH $COS_UNITY_CIFS $FS_SIZEMB
        run fileshare share $PROJECT/$fscifs  $UNITY_SMBFILESHARE1 --description 'New_SMB_Share'
        run fileshare show $PROJECT/$fscifs
        run fileshare unshare $PROJECT/$fscifs $UNITY_SMBFILESHARE1 
        run fileshare delete $PROJECT/$fscifs
    fi

    
    if [ $cos = $COS_ISIFILE ] ; then
        run snapshot delete $PROJECT/$fsname-$datetime $PROJECT/$fsname
        run snapshot delete $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
    fi
   
    # create some fileshares for bulk get test
     if [ $cos != $COS_VNXE ] ; then
	    fs1=sanityFs1-$datetime
	    fs2=sanityFs2-$datetime
	    run fileshare create ${fs1} $PROJECT $NH $cos $FS_SIZE
	    run fileshare create ${fs2} $PROJECT $NH $cos $FS_SIZE
	    run fileshare bulkget
	    run fileshare delete $PROJECT/${fs1}
	    run fileshare delete $PROJECT/${fs2}
    fi

    run fileshare delete $PROJECT/$fsname

    if [ $cos = $COS_VNXFILE ] ; then
        fsForceDelete="deleteTestVNX"
        echo "--Delete tests for VNXFile--"
        run fileshare create $fsForceDelete $PROJECT $NH $cos $FS_SIZE
        run fileshare export $PROJECT/$fsForceDelete $exp_args
        run snapshot create $PROJECT/$fsForceDelete $fsForceDelete-$datetime
        run snapshot create $PROJECT/$fsForceDelete $fsForceDelete-$datetime-2
        run snapshot export $PROJECT/$fsForceDelete-$datetime-2 $PROJECT/$fsForceDelete $SNAPEXP_DEFAULT_EPS
        # force delete is not supported from release 3.6.2
        # so, delete configuration on snapshot and snapshots explicitly
        run snapshot unexport $PROJECT/$fsForceDelete-$datetime-2 $PROJECT/$fsForceDelete
        run snapshot delete $PROJECT/$fsForceDelete-$datetime $PROJECT/$fsForceDelete 
        run snapshot delete $PROJECT/$fsForceDelete-$datetime-2 $PROJECT/$fsForceDelete 
        run fileshare unexport $PROJECT/$fsForceDelete
        run fileshare delete $PROJECT/$fsForceDelete
        
        # duplicate label Test - Should fail
        # run fileshare create $fsForceDelete $PROJECT $NH $cos $FS_SIZE
        # run fileshare create $fsForceDelete $PROJECT $NH $cos $FS_SIZE

    fi


    if [ $cos = $COS_NETAPP ] ; then
        fsForceDelete="deleteTestNETAPP"
        echo "--Delete tests for NETAPP--"
        run fileshare create $fsForceDelete $PROJECT $NH $cos $FS_SIZE
        run snapshot create $PROJECT/$fsForceDelete $fsForceDelete-$datetime
        run snapshot create $PROJECT/$fsForceDelete $fsForceDelete-$datetime-2
        # force delete is not supported from release 3.6.2
        # so, delete configuration on snapshot and snapshots explicitly
        run snapshot delete $PROJECT/$fsForceDelete-$datetime $PROJECT/$fsForceDelete 
        run snapshot delete $PROJECT/$fsForceDelete-$datetime-2 $PROJECT/$fsForceDelete 
        run fileshare delete $PROJECT/$fsForceDelete
        # duplicate label Test - Should fail
        # run fileshare create $fsForceDelete $PROJECT $NH $cos $FS_SIZE
        # run fileshare create $fsForceDelete $PROJECT $NH $cos $FS_SIZE

    fi

    if [ $cos = $COS_NETAPPC ] ; then
        fsForceDelete="deleteTestNETAPPC"
        echo "--Delete tests for NETAPPC--"
        run fileshare create $fsForceDelete $PROJECT $NH $cos $FS_SIZE
        run snapshot create $PROJECT/$fsForceDelete $fsForceDelete-$datetime
        run snapshot create $PROJECT/$fsForceDelete $fsForceDelete-$datetime-2
        # force delete is not supported from release 3.6.2
        # so, delete configuration on snapshot and snapshots explicitly
        run snapshot delete $PROJECT/$fsForceDelete-$datetime $PROJECT/$fsForceDelete 
        run snapshot delete $PROJECT/$fsForceDelete-$datetime-2 $PROJECT/$fsForceDelete 
        run fileshare delete $PROJECT/$fsForceDelete
        # duplicate label Test - Should fail
        # run fileshare create $fsForceDelete $PROJECT $NH $cos $FS_SIZE
        # run fileshare create $fsForceDelete $PROJECT $NH $cos $FS_SIZE

    fi

    if [ $cos = $COS_ISIFILE ] ; then
        fsForceDelete="deleteTestISILON"
        echo "--delete tests for ISILON--"
        run fileshare create $fsForceDelete $PROJECT $NH $cos $FS_SIZE
        run snapshot create $PROJECT/$fsForceDelete $fsForceDelete-$datetime
        run snapshot create $PROJECT/$fsForceDelete $fsForceDelete-$datetime-2
        # force delete is not supported from release 3.6.2
        # so, delete configuration on snapshot and snapshots explicitly
        run snapshot delete $PROJECT/$fsForceDelete-$datetime $PROJECT/$fsForceDelete 
        run snapshot delete $PROJECT/$fsForceDelete-$datetime-2 $PROJECT/$fsForceDelete 
        run fileshare delete $PROJECT/$fsForceDelete
	# duplicate label Test - Should fail
	# run fileshare create $fsForceDelete $PROJECT $NH $cos $FS_SIZE
	# run fileshare create $fsForceDelete $PROJECT $NH $cos $FS_SIZE
    fi

    # Force Delete test for Data Domain
    if [ $cos = $COS_DDFILE ] ; then
        fsForceDelete="deleteTestDATADOMAIN"
        #File systems (mtrees) are not deleted from DDMC, just marked inactive
        #until the GC runs.  To avoid conflict with the previous run, we attach
        #a timestamp to FS name.
        fsForceDeleteTimed=$fsForceDelete-$datetime
        echo "--Delete tests for DATADOMAIN--"
        run fileshare create $fsForceDeleteTimed $PROJECT $NH $cos $FS_SIZE
        run fileshare delete $PROJECT/$fsForceDeleteTimed
    fi

    # File System Quota directory ops.
    if [ $cos = $COS_NETAPP ] ; then
        echo "--Quota directory operations for NETAPP--"
        fsQuotaDir="FileSystemQuotaDirTest"--$datetime
        quotaDir1="NetAppQuotaDirTest1"
        quotaDir2="NetAppQuotaDirTest1234" 
        run fileshare create $fsQuotaDir $PROJECT $NH $cos $FS_SIZE
        run fileshare create_quota_dir $PROJECT/$fsQuotaDir $quotaDir2 --sec "unix" --size $FS_SIZE --oplock true
        run fileshare show_quota_dir $PROJECT/$fsQuotaDir $quotaDir2
        run fileshare update_quota_dir $PROJECT/$fsQuotaDir $quotaDir2 --sec "ntfs" 
        run fileshare show_quota_dir $PROJECT/$fsQuotaDir $quotaDir2 
        run fileshare delete_quota_dir $PROJECT/$fsQuotaDir $quotaDir2
        run fileshare delete $PROJECT/$fsQuotaDir
    fi
    
    # File System Quota directory ops.
    if [ $cos = $COS_NETAPPC ] ; then
        echo "--Quota directory operations for NETAPP Cluster--"
        fsQuotaDir="FileSystemQuotaDirTest"--$datetime
        quotaDir1="NetAppQuotaDirTest1"
        quotaDir2="NetAppQuotaDirTest1234" 
        run fileshare create $fsQuotaDir $PROJECT $NH $cos $FS_SIZE
        run fileshare create_quota_dir $PROJECT/$fsQuotaDir $quotaDir2 --sec "unix" --size $FS_SIZE --oplock true
        run fileshare show_quota_dir $PROJECT/$fsQuotaDir $quotaDir2
        run fileshare update_quota_dir $PROJECT/$fsQuotaDir $quotaDir2 --sec "ntfs" 
        run fileshare show_quota_dir $PROJECT/$fsQuotaDir $quotaDir2 
        run fileshare delete_quota_dir $PROJECT/$fsQuotaDir $quotaDir2
        run fileshare delete $PROJECT/$fsQuotaDir
    fi
    
    # File System Quota directory ops.
    if [ $cos = $COS_ISIFILE ] ; then
        echo "--Quota directory operations for Isilon--"
        fsQuotaDir="FileSystemQuotaDirTest"--$datetime
        quotaDir1="IsilonQuotaDirTest1"
        quotaDir2="IsilonQuotaDirTest2" 
        run fileshare create $fsQuotaDir $PROJECT $NH $cos $FS_SIZE
        run fileshare create_quota_dir $PROJECT/$fsQuotaDir $quotaDir2 --sec "unix" --size $FS_SIZE --oplock true
        run fileshare show_quota_dir $PROJECT/$fsQuotaDir $quotaDir2
        run fileshare update_quota_dir $PROJECT/$fsQuotaDir $quotaDir2 --sec "ntfs" 
        run fileshare show_quota_dir $PROJECT/$fsQuotaDir $quotaDir2 
        run fileshare delete_quota_dir $PROJECT/$fsQuotaDir $quotaDir2
        run fileshare delete $PROJECT/$fsQuotaDir
    fi
    
    # File System Quota directory ops.
    if [ $cos = $COS_VNXFILE ] ; then
        echo "--Quota directory operations for VNXFile--"
        fsQuotaDir="FileSystemQuotaDirTest"--$datetime
        quotaDir1="VNXQuotaDir1"
        quotaDir2="VNXQuotaDir2"
        quotafs_exp_args="$FSEXP_SHARED_VARRAY_RW_EPS --perm rw  --comments TESTCOMMENTS"
        run fileshare create $fsQuotaDir $PROJECT $NH $cos $FS_SIZE
        run fileshare export $PROJECT/$fsQuotaDir $quotafs_exp_args
        run fileshare create_quota_dir $PROJECT/$fsQuotaDir $quotaDir2 --sec "unix" --size $FS_SIZE --oplock true
        run fileshare show_quota_dir $PROJECT/$fsQuotaDir $quotaDir2
        run fileshare update_quota_dir $PROJECT/$fsQuotaDir $quotaDir2 --sec "ntfs"
        run fileshare show_quota_dir $PROJECT/$fsQuotaDir $quotaDir2
        run fileshare delete_quota_dir $PROJECT/$fsQuotaDir $quotaDir2
        run fileshare unexport $PROJECT/$fsQuotaDir
        run fileshare delete $PROJECT/$fsQuotaDir
    fi

}

quick_file_tests()
{
    cos=$1; shift
    perms=${@}
    datetime=`date +%m%d%y%H%M%S`
    fsname=fs-$cos-$macaddr-$datetime
    fsname=$(echo $fsname | sed s/-/_/g)
    fsname=$(echo $fsname | sed s/:/_/g)
    echo $fsname

    run fileshare create $fsname $PROJECT $NH $cos $FS_SIZE
    if [ $cos = $COS_ISIFILE ] ; then
        run snapshot create $PROJECT/$fsname $fsname-$datetime
        run snapshot create $PROJECT/$fsname $fsname-$datetime-2
    fi

    if [ $cos = $COS_DDFILE ] ; then
        echo "DD snapshot create"
        run snapshot create $PROJECT/$fsname $fsname-$datetime
        run snapshot create $PROJECT/$fsname $fsname-$datetime-2
    fi

    for p in $perms
    do
        case $p in
        ro)
            exp_args="$FSEXP_RO_EPS --perm ro"
            snap_share_args="--perm read"
            ;;
        rw)
            exp_args="$FSEXP_RW_EPS --perm rw"
            snap_share_args="--perm read"
            ;;
        root)
            exp_args="$FSEXP_ROOT_EPS --perm root --rootuser nobody"
            snap_share_args="--perm read"
            ;;
        default)
            exp_args="$FSEXP_DEFAULT_EPS"
            snap_share_args="--perm read"
            ;;
        esac

        run fileshare export $PROJECT/$fsname   $exp_args
        run fileshare show $PROJECT/$fsname
        if [ $cos = $COS_VNXFILE ] ; then
            run snapshot create $PROJECT/$fsname $fsname-$datetime
            run snapshot export $PROJECT/$fsname-$datetime $PROJECT/$fsname $exp_args
            run snapshot show $PROJECT/$fsname-$datetime $PROJECT/$fsname
            run snapshot restore $PROJECT/$fsname-$datetime $PROJECT/$fsname
            run snapshot unexport $PROJECT/$fsname-$datetime $PROJECT/$fsname 
            run snapshot delete $PROJECT/$fsname-$datetime $PROJECT/$fsname
        fi
        run fileshare unexport $PROJECT/$fsname

       # if [ $cos = $COS_ISIFILE ] ; then
       #     run snapshot export $PROJECT/$fsname-$datetime $PROJECT/$fsname $exp_args
       #     run snapshot show $PROJECT/$fsname-$datetime $PROJECT/$fsname
       #     run snapshot unexport $PROJECT/$fsname-$datetime $PROJECT/$fsname 
       # fi
    done

    # Run only for Isilon (temporary until vnx implements smb)
    if [ $cos = $COS_ISIFILE ] ; then
        run fileshare share $PROJECT/$fsname  $ISI_SMBFILESHARE1 --description 'New_SMB_Share'
        run fileshare show $PROJECT/$fsname
        run fileshare unshare $PROJECT/$fsname $ISI_SMBFILESHARE1 
        #
        run snapshot share $PROJECT/$fsname-$datetime $PROJECT/$fsname  $ISI_SMBSNAPSHARE1  --description 'New_SMB_Share_For_Snapshot' $snap_share_args
        run snapshot share_list $PROJECT/$fsname-$datetime $PROJECT/$fsname
        run snapshot show $PROJECT/$fsname-$datetime $PROJECT/$fsname
        run snapshot unshare $PROJECT/$fsname-$datetime $PROJECT/$fsname $ISI_SMBSNAPSHARE1 
        run snapshot share_list $PROJECT/$fsname-$datetime $PROJECT/$fsname
        run snapshot show $PROJECT/$fsname-$datetime $PROJECT/$fsname
    fi

    if [ $cos = $COS_ISIFILE ] ; then
        run snapshot delete $PROJECT/$fsname-$datetime $PROJECT/$fsname
        run snapshot delete $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
    fi
    # Data Domain share tests for both fileshare and snapshot
    if [ $cos = $COS_DDFILE ] ; then
        echo "DD tests for fileshare and snapshot"
        run fileshare share $PROJECT/$fsname  $DATADOMAINF_SMBFILESHARE1 --description 'New_SMB_Share'
        run fileshare show $PROJECT/$fsname
        run fileshare unshare $PROJECT/$fsname $DATADOMAINF_SMBFILESHARE1
    fi

    if [ $cos = $COS_DDFILE ] ; then
        echo "DD snapshot delete 2"
        run snapshot delete $PROJECT/$fsname-$datetime $PROJECT/$fsname
        run snapshot delete $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
    fi 

    run fileshare delete $PROJECT/$fsname
}

#
# do fileshare tests using isilon specific cos to exercise Isilon tests
#
isilon_tests()
{
    file_tests $COS_ISIFILE default ro rw root

    if [ "$AUTH" != 'local' && "$AUTH" != 'ipv6' ] ; then
        bulkapi tenants $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        bulkapi projects $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        bulkapi neighborhoods $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        # hosts, clusters and vcenters will need to ran through a setup/create process before beeing called
        #bulkapi hosts $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        #bulkapi clusters $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        #bulkapi vcenters $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1

        bulkapi storage-systems $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        bulkapi storage-ports $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        bulkapi storage-pools $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        bulkapi transport-zones $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        bulkapi filecos $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        bulkapi fileshares $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1 $NH $COS_ISIFILE $FS_SIZE
    fi
    
    filepolicy_tests $REM_COS_ISIFILE

}

ecs_tests()
{
    echo "Performing ecs_tests"
    bucket create $ECS_BUCKET $PROJECT $NH $COS_ECS $ECS_SOFT_QUOTA $ECS_HARD_QUOTA $ECS_BUCKET_OWNER
    #bucket delete bkt_id  #currently create itself deletes bucket
}


#
# do fileshare tests using isilon specific cos to exercise Isilon tests
#
vnxfile_tests()
{
    file_tests $COS_VNXFILE default
}

#
# do fileshare tests using manually assigned ports to varray
#
<<COMMENT
vnxfile_flex_varray_tests () {
    cos=$COS_VNXFILE
    fsname=fs-$cos-$macaddr
    exp_args="$FSEXP_SHARED_VARRAY_RW_EPS --perm rw"
    NH=$NH3
    FSEXP_RW_EPS=$FSEXP_SHARED_VARRAY_RW_EPS
    file_tests $COS_VNXFILE rw
}
COMMENT
netapp_tests()
{
    file_tests $COS_NETAPP default
}

netappc_tests()
{
    file_tests $COS_NETAPPC default
}

vnxe_tests()
{
	FS_SIZEMB=$FS_VNXE_SIZE;
	FS_SIZE=$FS_VNXE_SIZE;
	FS_EXPAND_SIZE=$FS_VNXE_EXPAND_SIZE;
	file_tests $COS_VNXE default
	
	echo "vnxe block tests begin"
	vnxe_block_tests
}

vnxe_block_tests() {
	vol1=vnxe1-${RANDOM};
	vol2=vnxe-cg-${RANDOM};
    host=$PROJECT.lss.emc.com
    hostLbl=$PROJECT
	iqn1=iqn.1998-01.com.vmware:lgly6193-7ae20d76
	consistency_group=cg-${RANDOM}
	snap1_label=snap1-${RANDOM}
	snap2_label=snap2-${RANDOM}
	eg=eg-${RANDOM}
    
    run transportzone add $NH/$IP_ZONE $iqn1
    run volume create $vol1 $PROJECT $NH $COS_VNXEBLOCK_ISCSI $BLK_SIZE
    run volume expand $PROJECT/$vol1 $BLK_SIZE_EXPAND

    run hosts create $hostLbl $TENANT Windows $host --port 8111 --username user --password 'password' --osversion 1.0
    run initiator create $hostLbl iSCSI $iqn1

    run export_group create $PROJECT $eg $NH --type Host --volspec $PROJECT/$vol1 --hosts $hostLbl
	
    run blocksnapshot create $PROJECT/$vol1 $snap1_label
    run blocksnapshot list $PROJECT/$vol1
    run blocksnapshot restore $PROJECT/$vol1/${snap1_label}
    run blocksnapshot delete $PROJECT/$vol1/${snap1_label}
    run export_group delete $PROJECT/$eg
    run volume delete $PROJECT/$vol1 --wait
    run hosts delete $hostLbl
    
    echo "vnxe consistency group tests"
    
    run blockconsistencygroup create $PROJECT $consistency_group
    run volume create $vol2 $PROJECT $NH $COS_VNXEBLOCK_CG 1280000000 --consistencyGroup $consistency_group
    run blocksnapshot create $PROJECT/$vol2 $snap2_label
    run blocksnapshot delete $PROJECT/$vol2/${snap2_label}
    run volume delete $PROJECT/$vol2 --wait
    run blockconsistencygroup delete $consistency_group
    echo "**** Done vnxe"
}

datadomainfile_tests()
{
    echo "DD file tests started"
    file_tests $COS_DDFILE default
    echo "DD file tests completed succesfully"
}

#
# do vmax block tests using network assignment to varray
#
vmaxblock_tests()
{
   #--------------------------------------------------------------
    echo "Starting to execute VMAX block provisioning tests..."

    block_tests $VMAXEXPORT_GROUP $VMAXEXPORT_GROUP_HOST $VMAX_VOLUME $COS_VMAXBLOCK $VMAX_VOLUME $COS_VMAXBLOCK
    # meta_volume_block_tests $VMAXEXPORT_GROUP $VMAXEXPORT_GROUP_HOST $VMAX_META_VOLUME $COS_VMAXBLOCK $VMAX_META_VOLUME $COS_VMAXBLOCK
    echo "Sleeping for 60 seconds..."
    sleep 60
    volume_expand_test $VMAX_VOLUME $COS_VMAXBLOCK_THIN   # work only for concatenate, stripe tbd

    if [ "${AUTH}" != "local" -a "${AUTH}" != "ipv6" ]
    then
        bulkapi tenants $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        bulkapi projects $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        bulkapi neighborhoods $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        # hosts, clusters and vcenters will need to ran through a setup/create process before beeing called
        #bulkapi hosts $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        #bulkapi clusters $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        #bulkapi vcenters $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1

        bulkapi smis-providers $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        bulkapi storage-systems $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        bulkapi storage-ports $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        bulkapi storage-pools $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        bulkapi transport-zones $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        bulkapi blockcos $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
    fi

    # ---------------------------------------------------------
    # Thick/Thin volume testing
    # ---------------------------------------------------------
    # The following tests depend on the VMAX array type.
    # Some arrays only support thin pools, so the thick test
    # cases would not work against them.
    TkOnTk=vmax-thick-on-thick-${seed}
    TnOnTn=vmax-thin-on-thin-${seed}

#    run volume create $TkOnTk $PROJECT $NH $COS_VMAXBLOCK_THICK $BLK_SIZE
#    run volume create $TnOnTn $PROJECT $NH $COS_VMAXBLOCK_THIN $BLK_SIZE --thinVolume true

#    run volume delete $PROJECT/${TkOnTk} --wait &
#    run volume delete $PROJECT/${TnOnTn} --wait &
#    wait

   # VMAX3 compression tests
   #
   if [ $VMAX3_COMPRESSION -eq 1 ]; then
   echo "Executing VMAX3 Compression tests"
      vmaxblock_compression_tests
   fi
}

vmaxblock_compression_tests()
{
    block_tests $VMAXEXPORT_GROUP $VMAXEXPORT_GROUP_HOST $VMAX_VOLUME $COS_VMAXBLOCK_V3_COMP_ENABLED $VMAX_VOLUME $COS_VMAXBLOCK_V3_COMP_NOTENABLED

   comptest_vol="${VMAX_VOLUME}-test-vpool-change-c2noc"

    echo "Creating compressed volume" 
    run volume create ${comptest_vol} $PROJECT $NH $COS_VMAXBLOCK_V3_COMP_ENABLED 1073741825 --thinVolume true

    echo "Change volumes' virtual pool to uncompressed vpool"
    run volume change_cos $PROJECT/${comptest_vol} $COS_VMAXBLOCK_V3_COMP_NOTENABLED

    echo "delete volume"
    run volume delete $PROJECT/${comptest_vol} --wait

    comptest_vol="${VMAX_VOLUME}-test-vpool-change-noc2c"

    echo "Creating non-compressed volume"
    run volume create ${comptest_vol} $PROJECT $NH $COS_VMAXBLOCK_V3_COMP_NOTENABLED 1073741825 --thinVolume true

    echo "Change volumes' virtual pool to compressed vpool"
    run volume change_cos $PROJECT/${comptest_vol} $COS_VMAXBLOCK_V3_COMP_ENABLED

    echo "delete volume"
    run volume delete $PROJECT/${comptest_vol} --wait

    echo "Done VMAX3 compression tests"
}

#
# do vmax block tests using using manual port assignments tp varray
#

# Before proceeding with ingestion, we need to substitute volumes & host details here.
ingestblock_setup() {
    secho "Ingest Block Tests Setup"
    vmaxblock_setup
}

ingestblock_tests() {
    secho "VMAX Ingest Block Tests BEGIN"
	vmaxblock_non_cg_ingest_tests
    secho "VMAX Ingest Block Tests END"
}

vmaxblock_non_cg_ingest_tests(){
	secho "VMAX Ingest Block Tests BEGIN"
	# Labels
    
    VMAX_RANDOM=${RANDOM}
    VMAX_INGEST_CG=vmaxingest${VMAX_RANDOM}
    VMAX_INGEST_VOL1=vmaxingest1${VMAX_RANDOM}
    VMAX_INGEST_VOL2=vmaxingest2${VMAX_RANDOM}

    run volume create ${VMAX_INGEST_VOL1} ${PROJECT} ${NH} ${COS_VMAXBLOCK_FC} 1GB
    run volume create ${VMAX_INGEST_VOL2} ${PROJECT} ${NH} ${COS_VMAXBLOCK_FC} 1GB

    # Delete the volume
    run volume delete ${PROJECT}/${VMAX_INGEST_VOL1} --vipronly
    run volume delete ${PROJECT}/${VMAX_INGEST_VOL2} --vipronly

    secho "VMAX ingest discovery"

    # Discover unmanaged volumes on the array
    run storagedevice discover_namespace $VMAX_NATIVEGUID 'UNMANAGED_VOLUMES'

    run unmanagedvolume ingest_unexport ${NH} ${COS_VMAXBLOCK_FC} $PROJECT --volspec "${VMAX_INGEST_VOL1}"
    run unmanagedvolume ingest_unexport ${NH} ${COS_VMAXBLOCK_FC} $PROJECT --volspec "${VMAX_INGEST_VOL2}"

    # Verify the volumes got created; fail script if this fails.
    run volume show ${PROJECT}/${VMAX_INGEST_VOL1}
    run volume show ${PROJECT}/${VMAX_INGEST_VOL2}
    
    run volume delete ${PROJECT}/${VMAX_INGEST_VOL1} --wait 
    run volume delete ${PROJECT}/${VMAX_INGEST_VOL2} --wait

    secho "VMAX Ingest Block Tests END"
}

vmaxblock_flex_varray_tests() {
    # run the usual tests
    vmaxblock_tests

    # restore network assignment and remove manual port assignment
    transportzone update $FC_ZONE_A --addNeighborhoods $NH
    transportzone update $FC_ZONE_B --addNeighborhoods $NH
    transportzone update $IP_ZONE --addNeighborhoods $NH

    for porta in ${VMAX_PORTS_A}
    do
        storageport update $VMAX_NATIVEGUID FC --tzone $FCTZ_A --group ${porta}
    done
    for portb in ${VMAX_PORTS_B}
    do
        storageport update $VMAX_NATIVEGUID FC --tzone $FCTZ_B --group ${portb}
    done
    storageport update $VMAX_NATIVEGUID IP --tzone nh/iptz
}

#
# do vnx block tests using network assignment to varray
#
vnxblock_tests()
{
    block_tests $VNXEXPORT_GROUP $VNXEXPORT_GROUP_HOST $VNX_VOLUME $COS_VNXBLOCK $VNX_VOLUME $COS_VNXBLOCK
    volume_expand_test $VNX_VOLUME $COS_VNXBLOCK_THIN

    # Thick/Thin volume testing
    TkOnTn=vnx-thick-on-thin-${seed}
    TkOnTk=vnx-thick-on-thick-${seed}
    TnOnTn=vnx-thin-on-thin-${seed}

    run volume create $TkOnTn $PROJECT $NH $COS_VNXBLOCK $BLK_SIZE
    run volume create $TkOnTk $PROJECT $NH $COS_VNXBLOCK_THICK $BLK_SIZE
    run volume create $TnOnTn $PROJECT $NH $COS_VNXBLOCK $BLK_SIZE --thinVolume true

    run volume delete $PROJECT/${TkOnTn} --wait &
    run volume delete $PROJECT/${TkOnTk} --wait &
    run volume delete $PROJECT/${TnOnTn} --wait &
    wait
}

#
# do vnx block tests using using manual port assignments tp varray
#
vnxblock_flex_varray_tests()
{
    vnxblock_tests

    # undo manual assignment of ports
    transportzone update $FC_ZONE_A --addNeighborhoods $NH
    transportzone update $FC_ZONE_B --addNeighborhoods $NH
    transportzone update $IP_ZONE --addNeighborhoods $NH

    storageport update $VNXB_NATIVEGUID FC --rmvarrays $NH --group SP_A
    storageport update $VNXB_NATIVEGUID FC --rmvarrays $NH --group SP_B
    storageport update $VNXB_NATIVEGUID IP --rmvarrays $NH
}

#
# do block tests that spans arrays
#
combined_block_tests()
{
    block_tests $BLOCKEXPORT_GROUP $VMAX_VNXEXPORT_GROUP_HOST $VNX_VOLUME $COS_VNXBLOCK $VMAX_VOLUME $COS_VMAXBLOCK
}

block_tests()
{
    export_name=$1
    export_host=$2
    v1=${3}1
    cos1=$4
    v2=${5}2
    cos2=$6

    run volume create ${v1} $PROJECT $NH $cos1 $BLK_SIZE --thinVolume true
    if [ "$EXTRA_PARAM" = "search" ] ; then
        run volume search --name $(echo ${v1}| head -c 2)
        run volume search --name $(echo ${v1}| head -c 2) --project $projectid

        run volume tag $PROJECT/"$v1" $TAG
        run volume search --tag $SEARCH_PREFIX 
    fi

    run volume create ${v2} $PROJECT $NH $cos2 $BLK_SIZE --thinVolume true

#    export_test_1 ${export_name}1 ${export_host}1 ${v1} ${v2}
#    export_test_2 ${export_name}2 ${export_host}2 ${v1} ${v2}
#    export_test_3 ${export_name}3 ${export_host}3 ${v1} ${v2}
#    export_test_4 ${export_name}4 ${export_host}4 ${v1} ${v2}
     export_initiator ${export_name}1 ${v1} ${v2}
     export_host ${export_name}2 ${v1} ${v2}
#    export_cluster ${export_name}3 ${v1} ${v2}

    run volume bulkget
    
    run volume delete $PROJECT/${v1} --wait
    run volume delete $PROJECT/${v2} --wait
}


meta_volume_block_tests()
{
#    size=1099511627776    # 1TB
    size=322122547200      # 300GB
    export_name=$1
    export_host=$2
    v1=${3}1
    cos1=$4
    v2=${5}2
    cos2=$6

    run volume create ${v1} $PROJECT $NH $cos1 $size --thinVolume true
    run volume create ${v2} $PROJECT $NH $cos2 $size --thinVolume true
    sleep 15 # sleep to make sure that volume binding completed

#    export_test_1 ${export_name}1 ${export_host}1 ${v1} ${v2}
#    export_test_2 ${export_name}2 ${export_host}2 ${v1} ${v2}
#    export_test_3 ${export_name}3 ${export_host}3 ${v1} ${v2}
#    export_test_4 ${export_name}4 ${export_host}4 ${v1} ${v2}
    
    run volume delete $PROJECT --project --wait
}

quick_block_tests()
{
    export_name=$1
    export_host=$2
    v=$3
    cos1=$4

    v1=${3}-1
    v2=${3}-2

    run volume create $v $PROJECT $NH $cos1 $BLK_SIZE --thinVolume true --count=2
    export_initiator_quick ${export_name}Q ${v1} ${v2}
    export_host ${export_name}H ${v1} ${v2}
    run volume delete $PROJECT --project --wait
}

volume_expand_test()
{
    ev=$1-${seed}
    cos=$2

    run volume create ${ev} $PROJECT $NH $cos $BLK_SIZE
    run volume expand $PROJECT/$ev $BLK_SIZE_EXPAND
    run volume show $PROJECT/$ev
    run volume expand $PROJECT/$ev $BLK_SIZE_EXPAND_2
    run volume show $PROJECT/$ev
    run volume expand $PROJECT/$ev $BLK_SIZE_EXPAND_3
    run volume show $PROJECT/$ev

    run volume delete $PROJECT/${ev} --wait
}

# =======================================================================================
# Export Group 1
# =======================================================================================
export_test_1()
{
    expname=$1
    host=$2
    vol1=$PROJECT/$3
    vol2=$PROJECT/$4

    proj=$PROJECT
    exp=$proj/$expname
    NWWN=$BLK_CLIENT_FC_NODE
    PWWN1=`pwwn A1`
    PWWN2=`pwwn A2`

    run export_group create $proj $expname $NH
    run export_group add_volume $exp "$vol1+1"
    run export_group add_volume $exp "$vol2+2"
    run export_group show $exp
    run export_group remove_volume $exp "$vol1"
    run export_group show $exp
    run export_group add_volume $exp "$vol1+1"

    run export_group add_initiator $exp "FC+$NWWN+$PWWN1+$host"
    run export_group show $exp

    run export_group add_initiator $exp "FC+$NWWN+$PWWN2+$host"
    run export_group remove_initiator $exp "FC+$PWWN1"
    run export_group show $exp

    run export_group add_initiator $exp "FC+$NWWN+$PWWN1+$host"
    run export_group remove_initiator $exp "FC+$PWWN2"
    run export_group show $exp

    run export_group remove_volume $exp "$vol1"
    run export_group show $exp

    if [ "$BOURNE_SECURITY_DISABLED" != '1' ] ; then
        run bulkapi exportgroups $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
    fi

    run export_group delete $exp
    run export_group show $exp
}

# =======================================================================================
# Export Exclusive Type
# =======================================================================================
export_initiator()
{   
    snap1_label=snap1-${HOSTNAME}-${RANDOM}
    vol1=$PROJECT/$2
    vol2=$PROJECT/$3
    snap1=${vol1}/${snap1_label}
    proj=$PROJECT
    tenant=$TENANT
    c=1
    h=1
    expname=$1
    hostname=$hostbase$tenant$c$h
    echo $vol1 $vol2 $proj $tenant $hostname $expname
    
    exp=$proj/$expname
    nwwn=`nwwn $i$j`
    k=`wwnIdx $c $h`
    pwwn1=`pwwn A$k`
    pwwn2=`pwwn B$k`
    pwwn3=`pwwn C$k`
    pwwn4=`pwwn D$k`

    run blocksnapshot create $vol1 ${snap1_label}

    if [ "$BOURNE_SECURITY_DISABLED" != '1' ] ; then
        run bulkapi blocksnapshots $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
    fi

    run export_group create $proj $expname $NH --volspec "$vol1,$snap1" --inits "$hostname/$pwwn1"
    run export_group show $exp
    run export_group update $exp --addVolspec "$vol2" --remVols $vol1
    run export_group show $exp
    run export_group update $exp --addInits "$hostname/$pwwn3"
    run export_group show $exp
    run export_group update $exp --remInits "$hostname/$pwwn1"
    run export_group show $exp
    run export_group update $exp --remVols $vol2,$snap1
    # export group will be deleted by clean up process in ExportUpdateCompleter

    if [ "$BOURNE_SECURITY_DISABLED" != '1' ] ; then
        run bulkapi exportgroups $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
    fi

    run blocksnapshot delete $snap1
    # TODO Only for vmax3
    if ! [ ${SS} = "vnxblock" -o ${SS} = "vnxblock_flex_varray" ]; then
        run snapshotsession delete $snap1
    fi
}

# =======================================================================================
# Export Initiator Type (Quick)
# =======================================================================================
export_initiator_quick()
{
    snap1_label=snap1-${HOSTNAME}-${RANDOM}
    vol1=$PROJECT/$2
    vol2=$PROJECT/$3
    snap1=${vol1}/${snap1_label}
    proj=$PROJECT
    tenant=$TENANT
    c=1
    h=1
    expname=$1
    hostname=$hostbase$tenant$c$h
    echo $vol1 $vol2 $proj $tenant $hostname $expname

    exp=$proj/$expname
    nwwn=`nwwn $i$j`
    k=`wwnIdx $c $h`
    pwwn1=`pwwn A$k`
    pwwn2=`pwwn B$k`
    pwwn3=`pwwn C$k`
    pwwn4=`pwwn D$k`

    run blocksnapshot create $vol1 ${snap1_label}

    if [ "$BOURNE_SECURITY_DISABLED" != '1' ] ; then
        run bulkapi blocksnapshots $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
    fi

    run export_group create $proj $expname $NH --volspec "$vol1+1,$snap1+2" --inits "$hostname/$pwwn1"
    run export_group show $exp

    if [ "$BOURNE_SECURITY_DISABLED" != '1' ] ; then
        run bulkapi exportgroups $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
    fi

    run export_group delete $exp
    run blocksnapshot delete $snap1
    secho "Export initiator quick complete"
}

# =======================================================================================
# Export Host Type
# =======================================================================================
export_host()
{
    vol1=$PROJECT/$2
    vol2=$PROJECT/$3
    proj=$PROJECT
    tenant=$TENANT
    expname=$1
    exp=$proj/$expname
    
    c=1
    h=1
    hostname1=$hostbase$tenant$c$h
    k=`wwnIdx $c $h`
    pwwn11=`pwwn A$k`
    pwwn12=`pwwn B$k`
    pwwn13=`pwwn C$k`
    pwwn14=`pwwn D$k`
    
    h=2
    hostname2=$hostbase$tenant$c$h
    k=`wwnIdx $c $h`
    pwwn21=`pwwn A$k`
    pwwn22=`pwwn B$k`
    pwwn23=`pwwn C$k`
    pwwn24=`pwwn D$k`

    run export_group create $proj $expname $NH --type Host --volspec "$vol1+1" --hosts "$hostname1"
    run export_group show $exp
    run export_group update $exp --addVolspec "$vol2+2" --remVols $vol1
    run export_group show $exp
    # FIXME Not supported for VMAX3.  See COP-22496 for details.
    #run export_group update $exp --addHosts "$hostname2" --remHosts "$hostname1"
    run export_group show $exp
    run export_group update $exp --remInits "$hostname2/$pwwn21,$hostname2/$pwwn23"
    run export_group show $exp
    run export_group update $exp --remVols $vol2
    # export group will be deleted by clean up process in ExportUpdateCompleter

    if [ "$BOURNE_SECURITY_DISABLED" != '1' ] ; then
        run bulkapi exportgroups $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
    fi
}

# =======================================================================================
# Export Cluster Type
# =======================================================================================
export_cluster()
{
    vol1=$PROJECT/$2
    vol2=$PROJECT/$3
    proj=$PROJECT
    tenant=$TENANT
    expname=$1
    exp=$proj/$expname
    
    cluster1=${proj}Cluster1
    cluster2=${proj}Cluster2

    c=1
    h=1
    hostname=$hostbase$tenant$c$h
    k=`k $c $h`
    pwwn11=`pwwn A$k`
    pwwn12=`pwwn B$k`
    pwwn13=`pwwn C$k`
    pwwn14=`pwwn D$k`
    
    c=2
    h=2
    hostname2=$hostbase$tenant$c$h
    k=`k $c $h`
    pwwn21=`pwwn A$k`
    pwwn22=`pwwn B$k`
    pwwn23=`pwwn C$k`
    pwwn24=`pwwn D$k`

    run export_group create $proj $expname $NH --type Cluster --volspec "$vol1+1" --clusters "$cluster1"
    run export_group show $exp
    run export_group update $exp --addVolspec "$vol2+1" --remVols $vol1
    run export_group show $exp
    run export_group update $exp --addClusters "$cluster2" --remClusters "$cluster1"
    run export_group show $exp
    run export_group update $exp --remHosts "$hostname2"
    run export_group show $exp
    run export_group update $exp --addHosts "$hostname2"
    run export_group show $exp
    run export_group update $exp --remInits "$hostname2/$pwwn21,$hostname2/$pwwn23"
    run export_group show $exp
    run export_group update $exp --remVols $vol2
    run export_group show $exp

    if [ "$BOURNE_SECURITY_DISABLED" != '1' ] ; then
        run bulkapi exportgroups $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
    fi

    run export_group delete $exp
    run export_group show $exp
}

# =======================================================================================
# - create empty export group
# - add initiators, try to add duplicate initiators
# - add/remove volumes with initiator list non-empty
# =======================================================================================
export_test_2()
{
    expname=$1
    host=$2
    vol1=$PROJECT/$3
    vol2=$PROJECT/$4

    proj=$PROJECT
    exp=$proj/$expname
    NWWN=$BLK_CLIENT_FC_NODE
    PWWN1=`pwwn A3`
    PWWN2=`pwwn A4`

    run export_group create $proj $expname $NH
    run export_group show $exp

    run export_group add_initiator $exp "FC+$NWWN+$PWWN1+$host"
    run export_group add_initiator $exp "FC+$NWWN+$PWWN2+$host"
    run export_group show $exp

    run export_group add_volume $exp "$vol1+1"
    run export_group show $exp

    run export_group add_volume $exp "$vol2+2"
    run export_group show $exp

    run export_group remove_volume $exp "$vol1"
    run export_group show $exp

    run export_group remove_volume $exp "$vol2"
    run export_group show $exp

    run export_group add_volume $exp "$vol2+2"
    run export_group show $exp

    run export_group delete $exp
    run export_group show $exp
}

#
# create block export
#
export_test_3()
{
    expname=$1
    host=$2
    vol1=$PROJECT/$3
    vol2=$PROJECT/$4

    proj=$PROJECT
    exp=$proj/$expname
    NWWN=$BLK_CLIENT_FC_NODE
    PWWN1=`pwwn A1`
    PWWN2=`pwwn A2`

    run export_group create $proj $expname $NH --volspec "$vol1+1,$vol2+2" --initspec "FC+$NWWN+$PWWN1+$host,FC+$NWWN+$PWWN2+$host,iSCSI++$BLK_CLIENT_iSCSI+$host"
    run export_group show $exp

    run export_group remove_volume $exp $vol2
    run export_group show $exp

    run export_group remove_initiator $exp "FC+$PWWN1"
    run export_group show $exp

    run export_group delete $exp
    run export_group show $exp

    expname=${expname}2
    exp=$proj/$expname
    run export_group create $proj $expname $NH --volspec "$vol1,$vol2" --initspec "FC+$NWWN+$PWWN2+$host,iSCSI++$BLK_CLIENT_iSCSI+$host"
    run export_group remove_volume $exp $vol2
    run export_group show $exp

    run export_group add_initiator $exp "FC+$NWWN+$PWWN1+$host"
    run export_group delete $exp
    run export_group show $exp
}

#
# Test blocksnapshot and volume exports
#
export_test_4()
{
    expname=$1
    host=$2
    vol1=$PROJECT/$3
    vol2=$PROJECT/$4
    snap1_label=snap1-${HOSTNAME}-${RANDOM}
    snap1="${vol1}/${snap1_label}"

    proj=$PROJECT
    exp=$proj/$expname
    NWWN=$BLK_CLIENT_FC_NODE
    PWWN1=`pwwn A1`
    PWWN2=`pwwn A2`
    
    
    run blocksnapshot create $vol1 $snap1_label 

    if [ "$BOURNE_SECURITY_DISABLED" != '1' ] ; then
        run bulkapi blocksnapshots $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
    fi

    run export_group create $proj $expname $NH --volspec "$vol1,$vol2,$snap1" --initspec "FC+$NWWN+$PWWN2+$host"
    run export_group show $exp
    run volume exports $vol2
    run export_group remove_volume $exp $vol2
    run volume exports $vol2
    run export_group show $exp
    run blocksnapshot exports $snap1
    run export_group remove_volume $exp $snap1
    run blocksnapshot exports $snap1
    run export_group add_volume $exp $snap1
    run blocksnapshot exports $snap1

    run export_group add_initiator $exp "FC+$NWWN+$PWWN1+$host"
    run export_group delete $exp
    run export_group show $exp

    run blocksnapshot delete $snap1
}

syssvc_tests()
{
    syssvc $CONFIG_FILE "$BOURNE_IP"
}

security_tests()
{
    if [ "$AUTH" = 'local' ] ; then
        echo 'no security tests for local security'
        return
    fi

    # done in setupe: security login $SYSADMIN $SYSADMIN_PASSWORD
    security test_firewall
    security test_proxy_token
    security test_formlogin $SYSADMIN $SYSADMIN_PASSWORD
    security test_vulnerability $SYSADMIN $SYSADMIN_PASSWORD
    security login $SYSADMIN $SYSADMIN_PASSWORD
    security test_logout $SYSADMIN
    security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
    
    security update_authn_provider $LOCAL_LDAP_AUTHN_PROVIDER_NEWNAME
    balance run security add_tenant_role subject_id $LOCAL_LDAP_TENANTADMIN_USERNAME TENANT_ADMIN
    balance run security add_tenant_role subject_id $LOCAL_LDAP_TENANTADMIN_USERNAME TENANT_APPROVER
    balance run security login $LOCAL_LDAP_TENANTADMIN_USERNAME $LOCAL_LDAP_TENANTADMIN_PASSWORD
    balance run security add_tenant_role group $LOCAL_LDAP_TENANT_PROJECT_ADMINS_GROUP PROJECT_ADMIN
    balance run security login $LOCAL_LDAP_PROJECT_ADMIN_USERNAME $LOCAL_LDAP_PROJECT_ADMIN_PASSWORD
    balance run project create $PROJECT.securitytest
    balance run security login $LOCAL_LDAP_MAXGROUPSUSER_USERNAME $LOCAL_LDAP_MAXGROUPSUSER_PASSWORD
    for(( i=0; i<${#BOURNE_IP_ARRAY[@]};i++ ));
    do
        run security verify_user $LOCAL_LDAP_MAXGROUPSUSER_USERNAME --ip=${BOURNE_IP_ARRAY[$i]}
    done

    #run security logout
    #Test the ldaps provider
    balance run security login $SYSADMIN $SYSADMIN_PASSWORD
    balance security add_authn_provider $LOCAL_LDAP_AUTHN_MODE $LOCAL_SECURE_LDAP_AUTHN_URLS $LOCAL_SECURE_LDAP_AUTHN_MANAGER_DN $LOCAL_SECURE_LDAP_AUTHN_MANAGER_PWD $LOCAL_SECURE_LDAP_AUTHN_SEARCH_BASE $LOCAL_LDAP_AUTHN_SEARCH_FILTER $LOCAL_LDAP_AUTHN_AUTHN_GROUP_ATTR "$LOCAL_SECURE_LDAP_AUTHN_NAME" $LOCAL_SECURE_LDAP_AUTHN_DOMAINS "$LOCAL_SECURE_LDAP_AUTHN_WHITELIST" $LOCAL_LDAP_AUTHN_SEARCH_SCOPE --group_object_classes "$LOCAL_LDAP_AUTHN_GROUP_OBJECT_CLASSES" --group_member_attributes "$LOCAL_LDAP_AUTHN_GROUP_MEMBER_ATTRIBUTES"
    balance tenant add_attribute $LOCAL_SECURE_LDAP_AUTHN_DOMAINS $LOCAL_SECURE_LDAP_TENANT_ATTRIBUTE_KEY $LOCAL_SECURE_LDAP_TENANT_ATTRIBUTE_VALUE
    for(( i=0; i<${#BOURNE_IP_ARRAY[@]};i++ ));
    do
        run security login $LOCAL_SECURE_LDAP_USER_USERNAME $LOCAL_SECURE_LDAP_USER_PASSWORD --ip=${BOURNE_IP_ARRAY[$i]} 
    done
    
    #Test the LDAP provider
    balance run security login $SYSADMIN $SYSADMIN_PASSWORD
    balance tenant add_attribute $LOCAL_LDAP_AUTHN_DOMAINS $LOCAL_LDAP_TENANT_ATTRIBUTE_KEY $LOCAL_LDAP_TENANT_ATTRIBUTE_ROOT_SUBTENANT1_VALUE
        
    balance run security login $SYSADMIN $SYSADMIN_PASSWORD
    for(( i=0; i<${#BOURNE_IP_ARRAY[@]};i++ ));
    do
        run security verify_user $SYSADMIN --ip=${BOURNE_IP_ARRAY[$i]}
    done
    
    balance run security add_tenant_role subject_id $LOCAL_LDAP_USER_USERNAME_1 TENANT_ADMIN
    balance run security add_tenant_role subject_id $LOCAL_LDAP_USER_USERNAME_2 PROJECT_ADMIN
    
    # end of proxy token test
    
    # Leave the security test suite logged in as the super user
    balance tenant add_attribute $LOCAL_LDAP_AUTHN_DOMAINS $LOCAL_LDAP_TENANT_ATTRIBUTE_KEY $LOCAL_LDAP_TENANT_ATTRIBUTE_ROOT_TENANT_VALUE
    balance run security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD 
    security test_tenant_access_permissions $LOCAL_SECURE_LDAP_USER_USERNAME $LOCAL_SECURE_LDAP_USER_PASSWORD
    
    # test the domain name with spaces at the beginning and at the end CQ 603992
    # Tenant update with Whitespace before/after domain name and Group Name returns 400 before this change
    security login $SYSADMIN $SYSADMIN_PASSWORD
    security test_tenant_domain_update $SYSADMIN $SYSADMIN_PASSWORD $(toLower ${LOCAL_SECURE_LDAP_AUTHN_DOMAINS}) $LOCAL_SECURE_LDAP_TENANT_ATTRIBUTE_KEY $LOCAL_SECURE_LDAP_TENANT_ATTRIBUTE_VALUE
    
    # test adding and removing a group with spaces before and after the group's name CQ 603992
    tenant add_group $LOCAL_LDAP_AUTHN_DOMAINS "$LOCAL_LDAP_VIPR_USER_GROUP"
    tenant remove_group $LOCAL_LDAP_AUTHN_DOMAINS "$LOCAL_LDAP_VIPR_USER_GROUP"
    
    # test adding role with subject ID with spaces at the beginning and at the end and removing it for roles
    security add_tenant_role subject_id $LOCAL_SECURE_LDAP_USER_USERNAME_WITH_SPACES TENANT_ADMIN
    security remove_tenant_role subject_id $LOCAL_SECURE_LDAP_USER_USERNAME_WITH_SPACES TENANT_ADMIN
    
    # test the sequence: create a subtenant (TENANT_ID), get the URI of that tenant, 
    # deactivate the tenant and then try to create that tenant again. In the reply we should have the tenant ID like:
    # duplicated in another tenant (urn:storageos:TenantOrg:TENANT_ID)
    # The argument "true" means that we expect the tenant ID be reported in the error
    security test_tenant_duplicate_message $LOCAL_LDAP_AUTHN_DOMAINS "TEST_SUBTENANT_$$" "true"
    
    # The argument "false" means that we do not expect the tenant ID to be reported in the error
    security test_tenant_duplicate_message $LOCAL_LDAP_AUTHN_DOMAINS "TEST_SUBTENANT_$$" "false"
    
    # test that the password change with the same value returns the error 400
    security login $SYSADMIN $SYSADMIN_PASSWORD
    security test_password_change $SVCUSER $SYSADMIN_PASSWORD

    #Test the ad provider
	if [ "$TEST_AD_PROVIDER" = 'yes' ] ; then
		balance run security login $SYSADMIN $SYSADMIN_PASSWORD
		balance security add_authn_provider $AD_AUTHN_MODE $AD_AUTHN_URLS $AD_AUTHN_MANAGER_DN $AD_AUTHN_MANAGER_PWD $AD_AUTHN_SEARCH_BASE $AD_AUTHN_SEARCH_FILTER $AD_AUTHN_AUTHN_GROUP_ATTR "$AD_AUTHN_NAME" $AD_AUTHN_DOMAINS "$AD_AUTHN_WHITELIST" $AD_AUTHN_SEARCH_SCOPE 
		balance tenant add_attribute $AD_AUTHN_DOMAINS $AD_TENANT_ATTRIBUTE_KEY $AD_TENANT_ATTRIBUTE_VALUE
		for(( i=0; i<${#BOURNE_IP_ARRAY[@]};i++ ));
		do
			run security login $AD_USER_USERNAME $AD_USER_PASSWORD --ip=${BOURNE_IP_ARRAY[$i]} 
		done
	fi

	# verify java support ssl3 and don't have keysize restriction
	verify_java_security_file $BOURNE_IPADDR
}

verify_java_security_file() {
    VIP=$1
    OPT=""
    if [[ $VIP == \[* ]];
    then
        tmp=${VIP#*[}
        VIP=${tmp%]*}
        OPT="-6"
    fi
    echo "VIP=${VIP} OPT=${OPT}"

    # make sure line jdk.tls.disabledAlgorithms=SSLv3 was comented out
    cmd="find /usr/lib64 -name java.security -exec grep jdk.tls.disabledAlgorithms {} \; | grep SSLv3 | grep -v '#' > /dev/null || echo ok"
    echo "CMD=${cmd}"
    result=`SSH ${VIP} "$cmd" ${OPT}`
    if [[ $result != "ok" ]];
    then
        echo "***"
        echo "*** FAILED! vipr java.security disabled ssl3"
        echo "***"
        exit 1
    fi

    # make sure line jdk.certpath.disabledAlgorithms don't contains 'keySize' restriction
    cmd="find /usr/lib64 -name java.security -exec grep jdk.certpath.disabledAlgorithms {} \; | grep keySize | grep -v '#' > /dev/null || echo ok"
    echo "CMD=${cmd}"
    result=`SSH ${VIP} "$cmd" ${OPT}`
    if [[ $result != "ok" ]];
    then
        echo "***"
        echo "*** FAILED! vipr java.security contains keySize restriction"
        echo "***"
        exit 1
    fi
}

##### full_copy tests 
#######################

VMAX3_FC_SMIS_DEV=VMAX3_FC_SMIS_DEV
VMAX3_FC_PORTS_A="FA-1D FA-3D"
VMAX3_FC_PORTS_B="FA-2D FA-4D"
COS_VMAX3BLOCK_FC=COS_VMAX3BLOCK_FC

vmax3_fc_setup_once()
{
    secho "VMAX3 FC Setup"

    smisprovider show $VMAX3_FC_SMIS_DEV &> /dev/null && return $?

    run smisprovider create $VMAX3_FC_SMIS_DEV $VMAX3_FC_SMIS_IP $VMAX3_FC_SMIS_PORT $SMIS_USER "$SMIS_PASSWD" $VMAX3_FC_SMIS_SSL

    run storagedevice discover_all --ignore_error
    secho "Waiting 5 minutes after storage discovery"
    sleep 300

    run storagepool update $VMAX3_FC_NATIVEGUID --type block --volume_type THIN_ONLY
    run storagepool update $VMAX3_FC_NATIVEGUID --type block --volume_type THICK_ONLY
    run storagepool update $VMAX3_FC_NATIVEGUID --nhadd $NH --type block

    if [ $QUICK -eq 0 ]; then     
        echo "vmax3 fc storageports update"
        if [ $DISCOVER_SAN -eq 0 ]; then
           for porta in ${VMAX3_FC_PORTS_A}
           do
	      run storageport update $VMAX3_FC_NATIVEGUID FC --tzone $FCTZ_A --group ${porta}
           done
	    for portb in ${VMAX3_FC_PORTS_B}
           do
              run storageport update $VMAX3_FC_NATIVEGUID FC --tzone $FCTZ_B --group ${portb}
           done
       fi
       run storageport update $VMAX3_FC_NATIVEGUID IP --tzone nh/iptz
    fi

    run cos create block $COS_VMAX3BLOCK_FC false \
			 --description 'Virtual-Pool for VMAX3 block FC' \
                      --protocols FC 			\
                      --numpaths 2 \
                      --max_snapshots 10 \
	               --system_type vmax \
                      --provisionType 'Thin' \
			 --neighborhoods $NH


    run cos update block $COS_VMAX3BLOCK_FC --storage $VMAX3_FC_NATIVEGUID
    run cos allow $COS_VMAX3BLOCK_FC block $TENANT
}

vmax_fc_setup()
{
    secho "VMAX FC Setup"
    vmaxblock_setup
    # This vpool is used for the standard vmax tests and we
    # want to make sure the vmax3 is not used for those tests,
    # so we make sure to use only assigned pools, which are
    # those for the standard vmax array.
    run cos update block $COS_VMAXBLOCK_FC --use_matched false
    # vmax3_fc_setup_once
}

full_copy_setup()
{
    secho "Tenant is $TENANT"
    secho "Project is $PROJECT"
    vmax_fc_setup
    vnxblock_setup
}

full_copy_single_volume_vnx()
{
    echo "Finished vnx full-copy"

    full_copy_source="${FULL_COPY_VOLUME}-source${RANDOM}"
    full_copy_clone="${FULL_COPY_VOLUME}-clone${RANDOM}"

    echo "Creating source volume for VNX"
    run volume create ${full_copy_source} $PROJECT $NH $COS_VNXBLOCK_FC 1073741825 --thinVolume true

    echo "Creating 1 full copy"
    # TODO sanity_utils does not yet handle the count arg.  If you change this, perform manual cleanup of +1 copies
    run volume full_copy ${full_copy_clone} $PROJECT/${full_copy_source} --count=1

    echo "Listing all full copies for source volume"
    run volume full_copy_list $PROJECT/${full_copy_source}

    echo "Resynchronizing full copy from source"
    run volume full_copy_resync $PROJECT/${full_copy_clone}

    echo "Restoring source from full copy"
    run volume full_copy_restore $PROJECT/${full_copy_clone}

    echo "Detaching full copy..."
    run volume detach ${PROJECT}/${full_copy_source} ${PROJECT}/${full_copy_clone}

    echo "Deactivating full copy volume"
    run volume delete $PROJECT/${full_copy_clone} --wait

    echo "Deactivating source volume"
    run volume delete $PROJECT/${full_copy_source} --wait
    
    echo "Finished vnx full-copy"
}

full_copy_single_volume_vmax()
{
    echo "Started vmax full-copy"

    full_copy_source="${FULL_COPY_VOLUME}-source${RANDOM}"
    full_copy_clone="${FULL_COPY_VOLUME}-clone${RANDOM}"

    echo "Creating source volume"
    run volume create ${full_copy_source} $PROJECT $NH $COS_VMAXBLOCK_FC 1073741825 --thinVolume true

    echo "Creating 1 full copy"
    # TODO sanity_utils does not yet handle the count arg.  If you change this, perform manual cleanup of +1 copies
    run volume full_copy ${full_copy_clone} $PROJECT/${full_copy_source} --count=1

    echo "Listing all full copies for source volume"
    run volume full_copy_list $PROJECT/${full_copy_source}

    echo "Resynchronizing full copy from source"
    run volume full_copy_resync $PROJECT/${full_copy_clone}

    echo "Restoring source from full copy"
    run volume full_copy_restore $PROJECT/${full_copy_clone}

    echo "Detaching full copy..."
    run volume detach ${PROJECT}/${full_copy_source} ${PROJECT}/${full_copy_clone}

    echo "Deactivating full copy volume"
    run volume delete $PROJECT/${full_copy_clone} --wait

    echo "Deactivating source volume"
    run volume delete $PROJECT/${full_copy_source} --wait

    echo "Finished vmax full-copy"
}

full_copy_single_volume_vmax_inactive()
{
    echo "Started vmax inactive full-copy"

    full_copy_source="${FULL_COPY_VOLUME}-source${RANDOM}"
    full_copy_clone="${FULL_COPY_VOLUME}-clone${RANDOM}"

    echo "Creating source volume"
    run volume create ${full_copy_source} $PROJECT $NH $COS_VMAXBLOCK_FC 1073741825 --thinVolume true

    echo "Creating 1 full copy"
    # TODO sanity_utils does not yet handle the count arg.  If you change this, perform manual cleanup of +1 copies
    run volume full_copy ${full_copy_clone} $PROJECT/${full_copy_source} --count=1 --create_inactive true

    echo "Checking synchronization progress"
    run volume full_copy_check_progress ${PROJECT}/${full_copy_source} ${PROJECT}/${full_copy_clone}

    echo "Activating full copy..."
    sleep 10
    run volume activate ${PROJECT}/${full_copy_source} ${PROJECT}/${full_copy_clone}

    echo "Checking synchronization progress"
    run volume full_copy_check_progress ${PROJECT}/${full_copy_source} ${PROJECT}/${full_copy_clone}
    echo "Checking synchronization progress"
    run volume full_copy_check_progress ${PROJECT}/${full_copy_source} ${PROJECT}/${full_copy_clone}
    echo "Checking synchronization progress"
    run volume full_copy_check_progress ${PROJECT}/${full_copy_source} ${PROJECT}/${full_copy_clone}

    echo "Detaching full copy..."
    sleep 10
    run volume detach ${PROJECT}/${full_copy_source} ${PROJECT}/${full_copy_clone}

    echo "Listing all full copies for source volume"
    run volume full_copy_list $PROJECT/${full_copy_source}

    echo "Deactivating source volume"
    run volume delete $PROJECT/${full_copy_source} --wait

    echo "Deactivating full copy volume"
    run volume delete $PROJECT/${full_copy_clone} --wait

    echo "Finished vmax inactive full-copy"
}

full_copy_single_volume_vmax3()
{
    echo "Started vmax3 full-copy"

    full_copy_source="${FULL_COPY_VOLUME}-source${RANDOM}"
    full_copy_clone="${FULL_COPY_VOLUME}-clone${RANDOM}"

    echo "Creating source volume"
    run volume create ${full_copy_source} $PROJECT $NH $COS_VMAX3BLOCK_FC 1073741825 --thinVolume true

    echo "Creating 1 full copy"
    # TODO sanity_utils does not yet handle the count arg.  If you change this, perform manual cleanup of +1 copies
    run volume full_copy ${full_copy_clone} $PROJECT/${full_copy_source} --count=1

    echo "Listing all full copies for source volume"
    run volume full_copy_list $PROJECT/${full_copy_source}

    echo "Resynchronizing full copy from source"
    run volume full_copy_resync $PROJECT/${full_copy_clone}

    echo "Restoring source from full copy"
    run volume full_copy_restore $PROJECT/${full_copy_clone}

    echo "Detaching full copy..."
    run volume detach ${PROJECT}/${full_copy_source} ${PROJECT}/${full_copy_clone}

    echo "Deactivating full copy volume"
    run volume delete $PROJECT/${full_copy_clone} --wait

    echo "Deactivating source volume"
    run volume delete $PROJECT/${full_copy_source} --wait

    echo "Finished vmax3 full-copy"
}

full_copy_tests()
{
    echo "Started full-copy tests"
    full_copy_single_volume_vmax_inactive
    full_copy_single_volume_vmax
    #full_copy_single_volume_vmax3
    full_copy_single_volume_vnx
    echo "Finished full-copy tests"
}

##### end of full_copy tests

##### blockmirror tests 
#######################


blockmirror_setup()
{
    secho "Tenant is $TENANT"
    secho "Project is: $PROJECT"
    mirrorblock_setup
}

blockmirror_single_mirror()
{
    mirrortest_vol="${MIRROR_VOLUME}-single"

    echo "Creating source volume"
    run volume create ${mirrortest_vol} $PROJECT $NH $COS_MIRROR 1073741825 --thinVolume true

    echo "Attaching a single mirror"
    run blockmirror attach $PROJECT/${mirrortest_vol} "foo" 1 

    echo "Listing active mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}
	
	echo "Pause a sigle mirror" 
	run blockmirror pause $PROJECT/${mirrortest_vol}
	
	echo "Deactivating a sigle mirror" 
	run blockmirror deactivate $PROJECT/${mirrortest_vol} "foo"
    echo "Deleting all volumes in project"
    run volume delete $PROJECT --project --wait
}

blockmirror_attach_mirror_with_optional()
{
    mirrortest_vol="${MIRROR_VOLUME}-mirrorcos"

    echo "Creating source volume"
    run volume create ${mirrortest_vol} $PROJECT $NH $COS_MIRROR_WITH_OPTIONAL 1073741825 --thinVolume true

    echo "Attaching a mirror"
    run blockmirror attach $PROJECT/${mirrortest_vol} "foo" 1 

    echo "Listing active mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}
	
	echo "Pause a active mirror" 
	run blockmirror pause $PROJECT/${mirrortest_vol}
	
	echo "Deactivating a active mirror" 
	run blockmirror deactivate $PROJECT/${mirrortest_vol} "foo"
    
    echo "Deleting all volumes in project"
    run volume delete $PROJECT --project --wait
}

blockmirror_attach_2mirrors()
{
    mirrortest_vol="${MIRROR_VOLUME}-test-attach2"

    echo "Creating source volume"
    run volume create ${mirrortest_vol} $PROJECT $NH $COS_MIRROR_WITH_2_MIRRORS 1073741825 --thinVolume true

    echo "Attaching two mirrors"
    run blockmirror attach $PROJECT/${mirrortest_vol} "foo" 2 

    echo "Listing active mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}
	
	echo "Pause a active mirror" 
	run blockmirror pause $PROJECT/${mirrortest_vol}
	
	echo "Deactivating a active mirror" 
	run blockmirror deactivate $PROJECT/${mirrortest_vol} "foo-1"
	run blockmirror deactivate $PROJECT/${mirrortest_vol} "foo-2"
    
    echo "Deleting all volumes in project"
    run volume delete $PROJECT --project --wait
}

blockmirror_detach_mirror_all()
{
    mirrortest_vol="${MIRROR_VOLUME}-test-detach"

    echo "Creating source volume"
    run volume create ${mirrortest_vol} $PROJECT $NH $COS_MIRROR_WITH_2_MIRRORS 1073741825 --thinVolume true

    echo "Listing active mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}

    echo "Attaching first mirror"
    run blockmirror attach $PROJECT/${mirrortest_vol} "foo" 1 

    # COP-23516 
    #echo "Attaching second mirror"
    #run blockmirror attach $PROJECT/${mirrortest_vol} "bar" 1 

    echo "Listing active mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}
	
	echo "Pause a active mirror" 
	run blockmirror pause $PROJECT/${mirrortest_vol}

    echo "Detaching all mirrors"
    run blockmirror detach $PROJECT/${mirrortest_vol}
	
    echo "Deleting all volumes in project"
    run volume delete $PROJECT --project --wait
}

blockmirror_pause_resume_all()
{
    mirrortest_vol="${MIRROR_VOLUME}-test-pause-resume-all"

    echo "Creating source volume"
    run volume create ${mirrortest_vol} $PROJECT $NH $COS_MIRROR 1073741825 --thinVolume true 

    echo "Attaching first mirror"
    run blockmirror attach $PROJECT/${mirrortest_vol} "foo" 1 

    echo "Pausing all mirrors"
    run blockmirror pause $PROJECT/${mirrortest_vol}

    echo "Listing paused mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}

    echo "Resuming all mirrors"
    run blockmirror resume $PROJECT/${mirrortest_vol}

    echo "Listing resumed mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}
	
	echo "Pause a active mirror" 
	run blockmirror pause $PROJECT/${mirrortest_vol}

    echo "Detaching all mirrors"
    run blockmirror detach $PROJECT/${mirrortest_vol}
	
    echo "Deleting all volumes in project"
    run volume delete $PROJECT --project --wait
}

blockmirror_vpool_change()
{
    mirrortest_vol="${MIRROR_VOLUME}-test-vpool-change"

    echo "Creating source volume with no mirrors explicitly"
    run volume create ${mirrortest_vol} $PROJECT $NH $COS_MIRROR_BEFORE_CHANGE 1073741825 --thinVolume true 

    echo "Change virtual pool to one that has 1 explicit maximum mirror"
    run volume change_cos $PROJECT/${mirrortest_vol} $COS_MIRROR_AFTER_CHANGE

    echo "Attaching a mirror"
    run blockmirror attach $PROJECT/${mirrortest_vol} "foo" 1 

    echo "Listing mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}
	
	echo "Pause a active mirror" 
	run blockmirror pause $PROJECT/${mirrortest_vol}

    echo "Detaching active mirror"
    run blockmirror detach $PROJECT/${mirrortest_vol}
	
	echo "Deleting all volumes in project"
    run volume delete $PROJECT --project --wait
}

blockmirror_vnx()
{
    mirrortest_vol="${MIRROR_VOLUME_VNX}-test-vnx"

    echo "Creating source volume on VNX"
    run volume create ${mirrortest_vol} $PROJECT $NH $COS_MIRROR_VNX 1073741825 --thinVolume true 

    echo "Attaching a mirror"
    run blockmirror attach $PROJECT/${mirrortest_vol} "foo" 1 

    echo "Pausing all mirrors"
    run blockmirror pause $PROJECT/${mirrortest_vol}

    echo "Listing paused mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}

    echo "Resuming all mirrors"
    run blockmirror resume $PROJECT/${mirrortest_vol}

    echo "Listing resumed mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}

    echo "Deactivating source volume and all mirrors"
    run volume delete $PROJECT/${mirrortest_vol} --wait
}

blockmirror_group_mirrors()
{
    mirrortest_vol="${MIRROR_VOLUME}-group"
    consistency_group=`openssl passwd "$RANDOM" | tr -cd [:alnum:] | cut -c1-8`
    group_mirror="sanitytest"
    run blockconsistencygroup create $PROJECT $consistency_group

    # Create source volumes
    echo "Creating source volumes"
    run volume create ${mirrortest_vol}1 $PROJECT $NH $COS_VMAX_CG_MIRROR 1073741825 --thinVolume true --consistencyGroup $consistency_group
    run volume create ${mirrortest_vol}2 $PROJECT $NH $COS_VMAX_CG_MIRROR 1073741825 --thinVolume true --consistencyGroup $consistency_group

    run blockconsistencygroup show $consistency_group
    run volume list $PROJECT

    echo "Attaching group mirrors"
    run blockmirror attach $PROJECT/${mirrortest_vol}1 $group_mirror 1

    echo "Listing active mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}1
    run blockmirror list $PROJECT/${mirrortest_vol}2

    echo "Pausing group mirrors"
    run blockmirror pause $PROJECT/${mirrortest_vol}1

    echo "Listing group mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}1
    run blockmirror list $PROJECT/${mirrortest_vol}2

    echo "Resuming group mirrors"
    run blockmirror resume $PROJECT/${mirrortest_vol}1

    echo "Listing group mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}1
    run blockmirror list $PROJECT/${mirrortest_vol}2
	
	echo "Pausing group mirrors"
    run blockmirror pause $PROJECT/${mirrortest_vol}1

    echo "Deactivating group mirrors"
    run blockmirror deactivate $PROJECT/${mirrortest_vol}1 ${mirrortest_vol}1-${group_mirror}
	
    echo "Listing group mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}1
    run blockmirror list $PROJECT/${mirrortest_vol}2

    echo "Attaching group mirrors"
    run blockmirror attach $PROJECT/${mirrortest_vol}1 ${group_mirror}1 1

    echo "Listing group mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}1
    run blockmirror list $PROJECT/${mirrortest_vol}2

    echo "Detaching group mirrors"
    run blockmirror detach $PROJECT/${mirrortest_vol}1
	

    echo "Listing group mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}1
    run blockmirror list $PROJECT/${mirrortest_vol}2

    echo "Deleting all volumes in project"
    run volume delete $PROJECT --project --wait
}

blockmirror_tests()
{
    # quick_file_tests $COS_VNXFILE default
    # quick_block_tests $VMAXEXPORT_GROUP $VMAXEXPORT_GROUP_HOST $VMAX_VOLUME $COS_VMAXBLOCK
    # quick_block_tests $VNXEXPORT_GROUP $VNXEXPORT_GROUP_HOST $VNX_VOLUME $COS_VNXBLOCK
    blockmirror_single_mirror
    blockmirror_attach_mirror_with_optional
    # blockmirror_attach_2mirrors
    blockmirror_detach_mirror_all
    blockmirror_pause_resume_all
    blockmirror_vpool_change

    #blockmirror_vnx
    blockmirror_group_mirrors
}

#### end of blockmirror test section

##### errorhandling tests 
#################################

errorhandling_setup()
{
    masa=`date +%s | cut -c5-10`
    mainvalue=value"$masa"
    mainkey=key"$masa"
    MAINERRORHANDLING=mainerrorhandling"$masa"
    PROJECTERRORHANDLING=prjcterrorhandling"$masa"
}

errorhandling_tests()
{
    ## CoS ErrorHandling tests
    cos errorhandling file $COS_VNXFILE 				\
	--description 'Virtual-Pool-for-VNX-file' false 	\
                         --protocols NFS CIFS --provisionType 'Thin'
}

#### end of errorhandling test section

##### blockconsistencygroup tests
#################################

blockconsistencygroup_setup()
{
    # Run setup
    init_setup

    # Create MultiVolumeConsistency CoS
    consistencygroup_block_cos_setup

    # Create a Second Project
    date=`date +%s | cut -c5-10`
    PROJECT_GROUP_OTHER=cgProject"$date"
    project create $PROJECT_GROUP_OTHER --tenant $TENANT

    # Create Consistency Group
    blockconsistencygroup_create_test

    # Print environment
    secho "Tenant is $TENANT"
    secho "Project is $PROJECT"
    secho "Project Other is $PROJECT_GROUP_OTHER"
    secho "SMIS IP is $VNX_SMIS_IP"
    secho "VNX CG CoS is $VNX_COS_GROUP"
    secho "VMAX CG CoS is $VMAX_COS_GROUP"
    secho "CoS without MultiVolumeConsistency $COS_GROUP_INVALID"
    secho "Consistency Group is $CONSISTENCY_GROUP"
    secho "Consistency Group Snapshot is $CONSISTENCY_GROUP_SNAPSHOT"
}

blockconsistencygroup_create_test()
{
    ### create blockconsistencygroup
    echo "Creating consistency group"
    run blockconsistencygroup create $PROJECT $CONSISTENCY_GROUP
}

blockconsistencygroup_show_test()
{
    echo "Getting consistency group"
    run blockconsistencygroup show $CONSISTENCY_GROUP
}

blockconsistencygroup_bulk_test()
{
    echo "Getting bulk data for consistency groups"
    run blockconsistencygroup bulk
}

blockconsistencygroup_delete_test()
{
    ### delete blockconsistencygroup
    echo "Deleting consistency group"
    run blockconsistencygroup delete $CONSISTENCY_GROUP
}

blockconsistencygroup_add_volume_test()
{
    ### Create Volume
    echo "Adding volume to consistency group"
    run volume create  volume-${CONSISTENCY_GROUP} $PROJECT $NH $VNX_COS_GROUP 1280000000 --consistencyGroup $CONSISTENCY_GROUP

    ### Check that volume is inside the group
    echo "Checking volume is part of consistency group"
    run blockconsistencygroup check_volume  $PROJECT volume-${CONSISTENCY_GROUP} $CONSISTENCY_GROUP --expected

    ### Check that consistencygroup cannot be deleted at this point
    echo "Checking consistency group cannot be deleted with active volumes"
    run blockconsistencygroup delete_with_volumes $CONSISTENCY_GROUP
}

blockconsistencygroup_add_volume_invalid_project(){
    echo "Checking Volume creation fails when the consistency group project and the volume project don't match"

    # Create Volume in a different project: this should fail
    blockconsistencygroup check_volume_error volume-${CONSISTENCY_GROUP} $PROJECT_GROUP_OTHER $NH $VNX_COS_GROUP 1280000000 'Objects should all be in the project $projid' --consistencyGroup $CONSISTENCY_GROUP --consistencyGroupProject $PROJECT
}

blockconsistencygroup_add_volume_invalid_CoS(){
    echo "Checking volume creation fails when consistency group is provided but MultiVolumeConsistency attribute in VirtualPool is false"

    # Create Volume in a different project: this should fail
    blockconsistencygroup check_volume_error volume-${CONSISTENCY_GROUP} $PROJECT $NH $COS_GROUP_INVALID 1280000000 'Consistency group $cgid was provided but multi_volume_consistency attribute in the virtual pool $cosid is false' --consistencyGroup $CONSISTENCY_GROUP
}

blockconsistencygroup_add_volume_no_consistency_group(){
    echo "Checking volume creation fails when MultiVolumeConsistency attribute in VirtualPool is true but consistency group is not provided"

    # Create Volume in a different project: this should fail
    blockconsistencygroup check_volume_error volume-${CONSISTENCY_GROUP} $PROJECT $NH $VNX_COS_GROUP 1280000000 'Required parameter consistencyGroup was missing or empty' --servicecode 1005
}

blockconsistencygroup_create_snapshot()
{
    ### create blockconsistencygroup snapshot
    echo "Creating consistency group snapshot"
    run blockconsistencygroup create_snapshot $1 $2 --createInactive 
}

blockconsistencygroup_create_active_snapshot ()
{
    ### create blockconsistencygroup snapshot
    echo "Creating consistency group snapshot"
    run blockconsistencygroup create_snapshot $1 $2

}

blockconsistencygroup_activate_snapshot()
{
    ### activate blockconsistencygroup snapshot
    echo "Activating consistency group snapshot"
    run blockconsistencygroup activate_snapshot $1 $2
}

blockconsistencygroup_deactivate_snapshot()
{
    ### deactivate blockconsistencygroup snapshot
    echo "Deactivating consistency group snapshot"
    run blockconsistencygroup deactivate_snapshot $1 $2
}

blockconsistencygroup_restore_snapshot()
{
    ### restore blockconsistencygroup snapshot
    echo "Restoring consistency group snapshot"
    run blockconsistencygroup restore_snapshot $1 $2
}

blockconsistencygroup_expand_snapshot()
{
    ### expand snapshot
    echo "Expanding the consistency group snapshot"
    run blocksnapshot expand $1 $2
}

blockconsistencygroup_show_snapshot()
{
    ### show blockconsistencygroup snapshot
    echo "Showing consistency group snapshot"
    run blockconsistencygroup show_snapshot $1 $2
}

blockconsistencygroup_list_snapshot()
{
    ### list blockconsistencygroup snapshot
    echo "Listing consistency group snapshot"
    run blockconsistencygroup list_snapshots $1 $2
}

blockconsistencygroup_cleanup(){
    echo "Cleaning up after running blockconsistencygroup test"

    ### Delete volume from consistency group
    echo "Deleting volume from consistency group"
    run volume delete $PROJECT/volume-${CONSISTENCY_GROUP} --wait

    # Remove Invalid CoS
    cos delete $COS_GROUP_INVALID block

    # Delete the project
    project delete $PROJECT_GROUP_OTHER

    # Remove Consistency Group
    blockconsistencygroup_delete_test
}

blockconsistencygroup_setup_snapshot()
{
    echo "Creating setup for Consistency Group Snapshot tests"

    # Creating Consisstency Group for the snapshot
    run blockconsistencygroup create $PROJECT $1

    # Adding a Volume to the Consistency Group
    run volume create  volume-$1 $PROJECT $NH $2 1280000000 --consistencyGroup $1
}

blockconsistencygroup_snapshot_tests()
{    
    cg_name=$1
    cg_cos=$2
    cg_snapshot=$3
	
	
    
    echo "Running Consistency Group Snapshot tests"
    echo "Consistency Group: $cg_name"
    echo "CoS: $cg_cos"
    echo "CG Snapshot: $cg_snapshot" 
    
    blockconsistencygroup_setup_snapshot $cg_name $cg_cos
	blockconsistencygroup_create_active_snapshot $cg_name $cg_snapshot
	blockconsistencygroup_restore_snapshot $cg_name $cg_snapshot
    blockconsistencygroup_show_snapshot $cg_name $cg_snapshot
    blockconsistencygroup_list_snapshot $cg_name $cg_snapshot
    blockconsistencygroup_deactivate_snapshot $cg_name $cg_snapshot   
}

blockconsistencygroup_update_tests()
{
    cg1=$1
    cg1_vpool=$2

    cg2=$3
    cg2_vpool=$4
    vnx_vol=vnx-vol${RANDOM}
    vmax_vol=vmax-vol${RANDOM}

    run volume create ${vnx_vol} $PROJECT $NH cosvnxb 1GB
    run volume create ${vmax_vol} $PROJECT $NH cosvmaxb 1GB

    blockconsistencygroup create $PROJECT $cg1
    blockconsistencygroup create $PROJECT $cg2

    secho "Trying to update CG that is not on array"
    blockconsistencygroup update $cg1 --add $PROJECT/${vnx_vol}
    blockconsistencygroup update $cg2 --add $PROJECT/${vmax_vol}

    secho "Creating volumes and adding them to the CG"
    run volume create ${vnx_vol}-cg $PROJECT $NH $cg1_vpool 1GB --count=2 --consistencyGroup=$cg1
    run volume create ${vmax_vol}-cg $PROJECT $NH $cg2_vpool 1GB --count=2 --consistencyGroup=$cg2

    blockconsistencygroup show $cg1
    blockconsistencygroup show $cg2

    secho "Creating volumes to update"
    run volume create ${vnx_vol}-update $PROJECT $NH $cg1_vpool 1GB --count=3
    run volume create ${vmax_vol}-update $PROJECT $NH $cg2_vpool 1GB --count=3

    secho "Adding volumes to CG"
    run blockconsistencygroup update $cg1 --add $PROJECT/${vnx_vol}-update-1,$PROJECT/${vnx_vol}-update-2,$PROJECT/${vnx_vol}-update-3
    run blockconsistencygroup update $cg2 --add $PROJECT/${vmax_vol}-update-1,$PROJECT/${vmax_vol}-update-2,$PROJECT/${vmax_vol}-update-3

    secho "Remove volumes from CG"
    run blockconsistencygroup update $cg1 --remove $PROJECT/${vnx_vol}-update-1
    run blockconsistencygroup update $cg2 --remove $PROJECT/${vmax_vol}-update-1

    secho "Add and Remove volumes from CG"
    run blockconsistencygroup update $cg1 --remove $PROJECT/${vnx_vol}-update-2 --add $PROJECT/${vnx_vol}-update-3
    run blockconsistencygroup update $cg2 --remove $PROJECT/${vmax_vol}-update-2 --add $PROJECT/${vmax_vol}-update-3

    secho "Cleaning up after CG update tests"
    volume delete $PROJECT --project --wait
    run blockconsistencygroup delete $cg1
    run blockconsistencygroup delete $cg2
}

blockconsistencygroup_tests()
{
    blockconsistencygroup_show_test
    blockconsistencygroup_add_volume_test
    blockconsistencygroup_add_volume_invalid_project
    blockconsistencygroup_add_volume_invalid_CoS
#    blockconsistencygroup_add_volume_no_consistency_group
    blockconsistencygroup_snapshot_tests vnx-$CONSISTENCY_GROUP $VNX_COS_GROUP vnx-$CONSISTENCY_GROUP_SNAPSHOT 
    blockconsistencygroup_snapshot_tests vmax-$CONSISTENCY_GROUP $VMAX_COS_GROUP vmax-$CONSISTENCY_GROUP_SNAPSHOT
    blockconsistencygroup_bulk_test
    blockconsistencygroup_cleanup
    blockconsistencygroup_update_tests vnx-update $VNX_COS_GROUP vmax-update $VMAX_COS_GROUP
}

#### end of blockconsistencygroup test section

init_setup()
{
#    vnxfile_setup
    vnxblock_setup
    vmaxblock_setup
}

# ------------------------------------------------------------------------------------
# The 'init' operation is way to set up Bourne with all the configuration, but not run
# any tests. This would be useful for setting up for running manual test cases.
# ------------------------------------------------------------------------------------
init_tests()
{
    secho "Tenant is $TENANT"
    secho "Project is $PROJECT"
}

quick_setup()
{
    secho "quick setup"
#    vnxfile_setup
    VNXB_NATIVEGUID=$SIMULATOR_VNX_NATIVEGUID
    VMAX_NATIVEGUID=$SIMULATOR_VMAX_NATIVEGUID
    vnxblock_setup
#    datadomainfile_setup
    vmaxblock_setup
    errorhandling_setup
    sbsdk_setup
}

quick_tests()
{
#    quick_file_tests $COS_NETAPP default
#    quick_file_tests $COS_VNXFILE default
    # TODO Add isilon simulator WJEIV
    quick_block_tests $VNXEXPORT_GROUP $VNXEXPORT_GROUP_HOST $VNX_VOLUME $COS_VNXBLOCK
    quick_block_tests $VMAXEXPORT_GROUP $VMAXEXPORT_GROUP_HOST $VMAX_VOLUME $COS_VMAXBLOCK
    #quick_file_tests $COS_DDFILE default
    errorhandling_tests
    sbsdk_tests
}

all_tests()
{
#    ui_tests
#    webstorage_tests
    isilon_tests
#    vplex_tests
#    vnxfile_tests
#    datadomainfile_tests
    vnxblock_tests
    vmaxblock_tests
    blocksnapshot_tests
    blockmirror_tests
    blockconsistencygroup_tests
    syssvc_tests
    security_tests
    errorhandling_tests	
    ingestblock_tests
}

# query auditlogs happened in specific timeslot and return them in desired language
audit_setup()
{
    echo "Nothing to do for audit setup"
}

audit_tests()
{
    language="en_US"
    if [ -z ${EXTRA_PARAM} ]; then
        timeslot=`date -u +%Y-%m-%dT%H`
    else 
        timeslot=${EXTRA_PARAM}
    fi
    audit query $timeslot $language
}

# query monitorlogs happened in specific timeslot and return them in desired language
monitor_setup()
{
    echo "Nothing to do for monitor setup"
}

monitor_tests()
{
    language="en_US"
    if [ -z ${EXTRA_PARAM} ]; then
        timeslot=`date -u +%Y-%m-%dT%H`
    else
        timeslot=${EXTRA_PARAM}
    fi
    monitor query $timeslot $language
}

dr_setup()
{
    if [ -z "$DR_SITE_B_IP" ] ; then
        echo -e "\nFail: Usage of dr sanity test is like:\n ./sanity 255.254.253.site1 dr 255.254.253.site2\nOr multi site configuration can be tested:\n ./sanity 255.254.253.site1 dr 255.254.253.site2 255.254.253.site3"
        exit 1
    fi

    vdc_common_setup

    #setting pipefail option to pipe exit codes
    set -o pipefail

    DR_SITE_A_IP=$BOURNE_IPADDR
    security login $SYSADMIN $SYSADMIN_PASSWORD 10 600
    LOCAL_LDAP_GROUPUSER_USERNAME='ldapvipruser1@viprsanity.com'

}

# method to wait for all sites that are given as arguments to be stable
# each one wait 600 seconds and check every 10 seconds
wait_for_site_stable()
{
    local old_bourne_ipaddr=$BOURNE_IPADDR
    for ip in "$@"
    do
        echo "Waiting for $ip to be stable ..."
        BOURNE_IPADDR=$ip
        security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
        dr waitforstable --ip "${ip}" 10 600
    done
    BOURNE_IPADDR=$old_bourne_ipaddr
}

dr_tests()
{
    if [ -z "$DR_SITE_C_IP" ]; then
        dr_basic_tests
    else
        dr_multisite_tests
    fi
}

dr_basic_tests()
{
    projectid=$(project query $PROJECT)
    echo "Project id of $PROJECT is $projectid."
    
    active_site=$(dr find state ACTIVE|tail -1) || { echo "get dr active site failed."; echo $active_site; exit 1; }
    echo "active uuid is $active_site"
    
    echo "---ADD STANDBY SITE---"
    echo "Adding new standby into current vipr system"
    standby_site=$(dr add $DR_SITE_B_NAME "$DR_SITE_B_DESCRIPTION" "${DR_SITE_B_IP}" $SYSADMIN $SYSADMIN_PASSWORD|tail -1) || { echo "Add dr site failed."; echo $standby_site; exit 1; }
    echo "Adding dr done"
    echo "stanby uuid is $standby_site"    

    echo "---GET SITES---"
    dr list

    echo "---GET SITE $standby_site---"
    dr get $standby_site

    echo "---VERIFY SITE $standby_site---"
    name=$(dr get $standby_site name|tail -1)
    description=$(dr get $standby_site description|tail -1)
    vip=$(dr get $standby_site vip_endpoint|tail -1)

    if [ "$vip" != "$DR_SITE_B_IP" -a "[$vip]" != "$DR_SITE_B_IP" ] ; then
        echo -e "Fail: site vip \"$vip\" isn't equal to \"$DR_SITE_B_IP\""
        exit 1
    fi

    if [ "$name" != "$DR_SITE_B_NAME" ] ; then
        echo -e "Fail: site name \"$name\" isn't equal to \"$DR_SITE_B_NAME\""
        exit 1
    fi

    if [ "$description" != "$DR_SITE_B_DESCRIPTION" ] ; then
        echo -e "Fail: site description \"$description\" isn't equal to \"$DR_SITE_B_DESCRIPTION\""
        exit 1
    fi

    
    echo "---WAIT FOR SYNC $standby_site---"
    dr waitforstate $standby_site STANDBY_SYNCED 30 1200
    echo "Syncing dr done"

    echo "---WAIT 1m FOR $standby_site TO BOOTSTRAP---"
    sleep 60

    # login to standby with auth
    echo "---VERIFY PROJECT SYNCED $standby_site---"
    BOURNE_IPADDR=$DR_SITE_B_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    
    echo "---WAIT FOR STABLE $standby_site---"
    dr waitforstable 30 1200

    echo "---VERIFY SITE DETAILS $standby_site---"
    state=$(dr details $standby_site clusterState|tail -1)
    latency=$(dr details $standby_site networkLatencyInMs|tail -1)

    if [ "$state" != "STABLE" ] ; then
        echo -e "Fail: site clusterState \"$state\" isn't equal to STABLE"
        exit 1
    fi

    if (( $(echo "$latency < 0" | bc -l) )) ; then
        echo -e "Fail: site latency \"$latency\" isn't calculated correctly, should be 0 or greater"
        exit 1
    fi

    # verify project
    project show $projectid

    # verify sites in standby
    echo "---VERIFY sites from standby $standby_site---"
    dr get $standby_site
    dr get $active_site
    dr checkstandby "${DR_SITE_A_IP}"

    # wait for both sites stable so able to pause standby
    wait_for_site_stable "${DR_SITE_A_IP}" "${DR_SITE_B_IP}"

    # pause standby
    echo "---PAUSE SITE $standby_site---"
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr pause $standby_site || { echo "pause dr standby site failed."; exit 1; }

    echo "---WAIT 30s FOR $active_site TO STABILIZE---"
    sleep 30

    echo "---WAIT FOR PAUSE $standby_site---"
    dr waitforstate $standby_site STANDBY_PAUSED 30 1200
    echo "Pausing dr done"

    echo "---CREATE A NEW PROJECT ON ACTIVE---"
    PROJECT_NEW=${PROJECT}_new
    project create ${PROJECT_NEW} --tenant $TENANT
    projectid_new=$(project query ${PROJECT_NEW})
    echo "Project id of ${PROJECT_NEW} is ${projectid_new}."

    echo "---RESUME SITE $standby_site---"
    wait_for_site_stable "${DR_SITE_A_IP}" "${DR_SITE_B_IP}"
    BOURNE_IPADDR="${DR_SITE_A_IP}"
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr resume $standby_site || { echo "resume dr standby site failed."; exit 1; }

    echo "---WAIT FOR SYNC $standby_site---"
    dr waitforstate $standby_site STANDBY_SYNCED 30 1200
    echo "Syncing dr done"

    echo "---VERIFY NEW PROJECT FROM $standby_site---"
    BOURNE_IPADDR=$DR_SITE_B_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    echo "wait for $standby_site to become stable"
    dr waitforstable 30 1200
    project show ${projectid_new}

    echo "---PREPARING STANDBY_DEGRADED TEST---"
    syssvc $CONFIG_FILE "$DR_SITE_A_IP" set_prop system_permit_root_ssh yes
    dr waitforstable 10 600
    tune_dr_config ${DR_SITE_A_IP} degrade_standby_threshold_millis 0
    stop_vdc_services "$DR_SITE_B_IP"
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600

    echo "---WAIT FOR $standby_site TO BECOME DEGRADED---"
    dr waitforstate $standby_site STANDBY_DEGRADED 30 1200

    echo "---PREPARING STANDBY REJOIN TEST---"
    start_vdc_services "$DR_SITE_B_IP"

    echo "---WAIT FOR RESYNC $standby_site---"
    dr waitforstate $standby_site STANDBY_SYNCED 30 1200

    # set degrade_standby_threshold_millis to default value: 15 minutes
    tune_dr_config ${DR_SITE_A_IP} degrade_standby_threshold_millis 900000

    # wait both active and standby to be stable, ready to do switchover
    echo "---WAIT BOTH ACTIVE AND STANBY TO BE STABLE, READY FOR SWITCHOVER---"
    # there's no point in updating the property on both active and standby since they are shared
    BOURNE_IPADDR=$DR_SITE_B_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitforstable 10 600

    # switchover to standby
    echo "---SWITCHOVER FROM TO $active_site TO $standby_site---"
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitfornetworkhealth $standby_site GOOD 30 1200
    dr switchover $standby_site || { echo "swichover dr standby site failed."; exit 1; }

    echo "---WAIT ACTIVE AND STANDBY TO BE STABLE AFTER SWITCHOVER---"
    sleep 120
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitforstable 10 600
    BOURNE_IPADDR=$DR_SITE_B_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitforstable 10 600

    # verify switchover status
    echo "---WAIT FOR SWITCHOVER FROM $active_site TO $standby_site DONE---"
    dr waitforstate $standby_site ACTIVE 10 1200 && echo "standby has been switched into active"
    dr waitforstate $active_site STANDBY_SYNCED 10 1200 && echo "active has been switched into standby"

    # NOTE: active site and standby site has been switched, site A is standby and site B is active now

    # failover test part
    echo "Stopping active site $DR_SITE_B_NAME services to prepare for failover test ..."
    stop_vdc_services "$DR_SITE_B_IP"
    echo "Sleep 1m to wait standby site (VIP: ${DR_SITE_A_IP}) ZK switch to participant"
    sleep 60
    echo "Wait for site ${DR_SITE_A_NAME} to be stable ..."
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 800
    dr waitforstable --ip "${DR_SITE_A_IP}" 10 600

    echo "sleep 2m until active site disappears"
    sleep 120

    echo "Failover to promote site $DR_SITE_A_NAME as active ..."
    dr failover --ip "${DR_SITE_A_IP}" $active_site || { echo "failover to site $DR_SITE_A_NAME failed."; exit 1; }
    echo "Sleep 1m to wait failover ..."
    sleep 60
    dr waitforstable --ip "${DR_SITE_A_IP}"  10 800

    echo "Wait for site ${DR_SITE_A_NAME} to be stable ..."
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitforstable --ip "${DR_SITE_A_IP}" 10 600

    echo "Wait site state to become ACTIVE after failover ..."
    dr waitforstate $active_site ACTIVE 30 600 --ip "${DR_SITE_A_IP}"

    # failback test part
    echo "Starting old active site $DR_SITE_B_NAME services ..."
    start_vdc_services "$DR_SITE_B_IP"
    echo "Sleep 300s to wait old site to become ACTIVE_DEGRADED ..."
    sleep 300
    BOURNE_IPADDR=$DR_SITE_B_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitforstable --ip "${DR_SITE_B_IP}" 10 600
    dr waitforstate $standby_site ACTIVE_DEGRADED 30 600 --ip "${DR_SITE_B_IP}"

    # resume ACTIVE_DEGRADED site to prepare next standby-removing test
    echo "Waiting $DR_SITE_A_NAME and $DR_SITE_B_NAME to be stable ..."
    wait_for_site_stable "${DR_SITE_A_IP}" "${DR_SITE_B_IP}"
    echo "---RESUME OLD ACTIVE SITE ${DR_SITE_B_NAME} (ACTIVE_DEGRADED) AFTER FAILOVER---"
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr resume --ip "${DR_SITE_A_IP}" $standby_site || { echo "resume dr standby site failed."; exit 1; }
    echo "Resuming done"
    echo "Resuming site uuid is $standby_site"

    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    echo "Wait standby resuming done ..."
    dr waitforstate $standby_site STANDBY_SYNCED 30 1200

    # NOTE: after switchover and failover (then resume old active), site A is active and site B is standby (STANDBY_SYNCED state)

    # delete standby
    echo "---DELETE STANDBY $standby_site---"
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr delete $standby_site

    echo "---WAIT FOR REMOVED $standby_site---should throw exceptions---"
    dr waitforstate $standby_site STANDBY_REMOVING 30 600 -n|| echo "standby removed"
    echo "Removing dr done"

    # verify deleted
    echo "---VERIFY STANDBY DELETED $standby_site---should throw exceptions---"
    dr list $DR_SITE_B_NAME || echo "standby removed from site list"
    dr get $standby_site || echo "standby uuid removed from active"
    
    # verify active still works
    TESTPROJECT=dr_project_$$
    echo "---CREATE PROJECT $TESTPROJECT ON ACTIVE $active_site---"
    project show $TESTPROJECT &> /dev/null && return $?
    project create $TESTPROJECT --tenant $TENANT
    test_projectid=$(project query $TESTPROJECT)
    echo "Project id of $TESTPROJECT is $test_projectid."

    #disabling pipefail option to pipe exit codes
    set +o pipefail
}

time_start(){
    startvar=starttime_$1
    export "$startvar"=$(date +"%s")
}
time_finish(){
    endvar=endtime_$1
    startvar=starttime_$1
    declare "$endvar"=$(date +"%s")
    runtime=$(expr "${!endvar}" - "${!startvar}")
    echo "$1=${runtime}s" >> $PERFORMANCE_FILE

    #Record difference from last run if there was one
    if [ ! -z ${!1} ] ; then
        difference=$(expr "${runtime}" - "${!1%?}")
        echo "$1_diff=${difference}s" >> $PERFORMANCE_FILE
    fi
}


dr_multisite_tests()
{
    if [ -z $PERFORMANCE_FILE ] ; then
        export PERFORMANCE_FILE=dr_performance_results
    fi
    if [ -e $PERFORMANCE_FILE ] ; then
        source $PERFORMANCE_FILE
        truncate -s 0 $PERFORMANCE_FILE
    fi
    #Time DR operations if DR_RECORD_TIME
    projectid=$(project query $PROJECT)
    echo "Project id of $PROJECT is $projectid."

    active_site=$(dr find state ACTIVE|tail -1) || { echo "get dr active site failed."; echo $active_site; exit 1; }
    echo "active uuid is $active_site"

    echo "Change system property to allow root ssh login ..."
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    syssvc $CONFIG_FILE "$DR_SITE_A_IP" set_prop system_permit_root_ssh yes
    dr waitforstable 10 600
    
    echo "---ADD STANDBY SITE B---"
    time_start add_standby_b
    echo "Adding new standby into current vipr system"
    standby_site_b=$(dr add $DR_SITE_B_NAME "$DR_SITE_B_DESCRIPTION" "${DR_SITE_B_IP}" $SYSADMIN $SYSADMIN_PASSWORD|tail -1) || { echo "Add dr site failed."; echo $standby_site_b; exit 1; }
    echo "Adding dr done"
    echo "stanby uuid is $standby_site_b"    

    echo "---GET SITES---"
    dr list

    echo "---GET SITE $standby_site_b---"
    dr get $standby_site_b

    echo "---VERIFY SITE $standby_site_b---"
    name=$(dr get $standby_site_b name|tail -1)
    description=$(dr get $standby_site_b description|tail -1)
    vip=$(dr get $standby_site_b vip_endpoint|tail -1)

    if [ "$vip" != "$DR_SITE_B_IP" -a "[$vip]" != "$DR_SITE_B_IP"  ] ; then
        echo -e "Fail: site vip \"$vip\" isn't equal to \"$DR_SITE_B_IP\""
        exit 1
    fi

    if [ "$name" != "$DR_SITE_B_NAME" ] ; then
        echo -e "Fail: site name \"$name\" isn't equal to \"$DR_SITE_B_NAME\""
        exit 1
    fi

    if [ "$description" != "$DR_SITE_B_DESCRIPTION" ] ; then
        echo -e "Fail: site description \"$description\" isn't equal to \"$DR_SITE_B_DESCRIPTION\""
        exit 1
    fi

    
    echo "---WAIT FOR SYNC $standby_site_b---"
    dr waitforstate $standby_site_b STANDBY_SYNCED 30 1200
    echo "Syncing dr done"
    time_finish add_standby_b

    echo "---WAIT 1m FOR $standby_site_b TO BOOTSTRAP---"
    sleep 60

    # login to standby with auth
    echo "---LOGIN TO STANDBY WITH SYNCED AUTH $standby_site_b---"
    BOURNE_IPADDR=$DR_SITE_B_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600

    echo "---WAIT FOR STABLE $standby_site_b---"
    dr waitforstable 30 1200

    echo "---VERIFY SITE DETAILS $standby_site_b---"
    state=$(dr details $standby_site_b clusterState|tail -1)
    latency=$(dr details $standby_site_b networkLatencyInMs|tail -1)

    if [ "$state" != "STABLE" ] ; then
        echo -e "Fail: site clusterState \"$state\" isn't equal to STABLE"
        exit 1
    fi

    if (( $(echo "$latency < 0" | bc -l) )) ; then
        echo -e "Fail: site latency \"$latency\" isn't calculated correctly, should be 0 or greater"
        exit 1
    fi

    # verify project
    echo "---VERIFY PROJECT SYNCED $standby_site_b---"
    project show $projectid

    # verify sites in standby
    echo "---VERIFY sites from standby $standby_site_b---"
    dr get $standby_site_b
    dr get $active_site

    dr waitforstable 10 600

    echo "---ADD STANDBY SITE C---"
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600

    time_start add_standby_c
    echo "Adding new standby into current vipr system"
    standby_site_c=$(dr add $DR_SITE_C_NAME "$DR_SITE_C_DESCRIPTION" "${DR_SITE_C_IP}" $SYSADMIN $SYSADMIN_PASSWORD|tail -1) || { echo "Add dr site c failed."; echo $standby_site_c; exit 1; }
    echo "Adding dr site c done"
    echo "stanby uuid is $standby_site_c"

    echo "---GET SITES---"
    dr list

    echo "---GET SITE $standby_site_c---"
    dr get $standby_site_c

    echo "---VERIFY SITE $standby_site_c---"
    name=$(dr get $standby_site_c name|tail -1)
    description=$(dr get $standby_site_c description|tail -1)
    vip=$(dr get $standby_site_c vip_endpoint|tail -1)

    if [ "$vip" != "$DR_SITE_C_IP" -a "[$vip]" != "$DR_SITE_C_IP" ] ; then
        echo -e "Fail: site vip \"$vip\" isn't equal to \"$DR_SITE_C_IP\""
        exit 1
    fi

    if [ "$name" != "$DR_SITE_C_NAME" ] ; then
        echo -e "Fail: site name \"$name\" isn't equal to \"$DR_SITE_C_NAME\""
        exit 1
    fi

    if [ "$description" != "$DR_SITE_C_DESCRIPTION" ] ; then
        echo -e "Fail: site description \"$description\" isn't equal to \"$DR_SITE_C_DESCRIPTION\""
        exit 1
    fi


    echo "---WAIT FOR SYNC $standby_site_c---"
    dr waitforstate $standby_site_c STANDBY_SYNCED 30 1200
    echo "Syncing dr done"
    time_finish add_standby_c

    echo "---WAIT 1m FOR $standby_site_c TO BOOTSTRAP---"
    sleep 60

    # login to standby with auth
    echo "---LOGIN TO STANDBY WITH SYNCED AUTH $standby_site_c---"
    BOURNE_IPADDR=$DR_SITE_C_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600

    echo "---WAIT FOR STABLE $standby_site_c---"
    dr waitforstable 30 1200

    echo "---VERIFY SITE DETAILS $standby_site_c---"
    state=$(dr details $standby_site_c clusterState|tail -1)
    latency=$(dr details $standby_site_c networkLatencyInMs|tail -1)

    if [ "$state" != "STABLE" ] ; then
        echo -e "Fail: site clusterState \"$state\" isn't equal to STABLE"
        exit 1
    fi

    if (( $(echo "$latency < 0" | bc -l) )) ; then
        echo -e "Fail: site latency \"$latency\" isn't calculated correctly, should be 0 or greater"
        exit 1
    fi

    # verify project
    echo "---VERIFY PROJECT SYNCED $standby_site_c---"
    project show $projectid

    # verify sites in standby
    echo "---VERIFY sites from standby $standby_site_c---"
    dr get $standby_site_b
    dr get $standby_site_c
    dr get $active_site

    # login to standby with auth
    echo "---LOGIN TO STANDBY WITH SYNCED AUTH $standby_site_b---"
    BOURNE_IPADDR=$DR_SITE_B_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600

    # verify sites in standby
    echo "---VERIFY sites from standby $standby_site_b---"
    dr get $standby_site_b
    dr get $standby_site_c
    dr get $active_site

    wait_for_site_stable "${DR_SITE_A_IP}" "${DR_SITE_B_IP}" "${DR_SITE_C_IP}"

    # pause standby b
    echo "---PAUSE SITE $standby_site_b---"
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600

    time_start pause_standby_b
    dr pause $standby_site_b || { echo "pause dr standby site b failed."; exit 1; }

    echo "---WAIT 30s FOR $active_site_b TO STABILIZE---"
    sleep 30

    echo "---WAIT FOR PAUSE $standby_site_b---"
    dr waitforstate $standby_site_b STANDBY_PAUSED 30 1200
    echo "Pausing dr done"
    time_finish pause_standby_b

    wait_for_site_stable "${DR_SITE_A_IP}" "${DR_SITE_B_IP}" "${DR_SITE_C_IP}"

    # COP-23445: We should check if Standby C is SYNCED before initiating a pause command.
    dr waitforstate $standby_site_c STANDBY_SYNCED 30 1200
    # pause standby c
    echo "---PAUSE SITE $standby_site_c---"
    time_start pause_standby_c
    dr pause $standby_site_c || { echo "pause dr standby site c failed."; exit 1; }

    echo "---WAIT 30s FOR $active_site_c TO STABILIZE---"
    sleep 30

    echo "---WAIT FOR PAUSE $standby_site_c---"
    dr waitforstate $standby_site_c STANDBY_PAUSED 30 1200
    echo "Pausing dr done"
    time_finish pause_standby_c

    echo "---CREATE A NEW PROJECT ON ACTIVE---"
    PROJECT_NEW=${PROJECT}_new
    project create ${PROJECT_NEW} --tenant $TENANT
    projectid_new=$(project query ${PROJECT_NEW})
    echo "Project id of ${PROJECT_NEW} is ${projectid_new}."

    echo "---RESUME SITE $standby_site_b---"
    time_start resume_standby_b
    dr resume $standby_site_b || { echo "resume dr standby site b failed."; exit 1; }

    echo "---WAIT FOR SYNC $standby_site_b---"
    dr waitforstate $standby_site_b STANDBY_SYNCED 30 1200
    time_finish resume_standby_b
    echo "Syncing dr done"

    echo "---WAIT 30s TO STABILIZE---"
    sleep 30

    echo "---VERIFY NEW PROJECT FROM $standby_site_b---"
    BOURNE_IPADDR=$DR_SITE_B_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    echo "wait for $standby_site_b to be stable"
    dr waitforstable 30 1200
    project show ${projectid_new}

    echo "---RESUME SITE $standby_site_c---"
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    time_start resume_standby_c
    dr resume $standby_site_c || { echo "resume dr standby site c failed."; exit 1; }

    echo "---WAIT FOR SYNC $standby_site_c---"
    dr waitforstate $standby_site_c STANDBY_SYNCED 30 1200
    time_finish resume_standby_c
    echo "Syncing dr done"

    echo "---WAIT 30s TO STABILIZE---"
    sleep 30

    echo "---VERIFY NEW PROJECT FROM $standby_site_c---"
    BOURNE_IPADDR=$DR_SITE_C_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    echo "wait for $standby_site_b to be stable"
    dr waitforstable 30 1200
    project show ${projectid_new}

    echo "---PREPARING STANDBY_DEGRADED TEST---"
    syssvc $CONFIG_FILE "$DR_SITE_A_IP" set_prop system_permit_root_ssh yes
    dr waitforstable 10 600
    tune_dr_config ${DR_SITE_A_IP} degrade_standby_threshold_millis 0
    stop_vdc_services "$DR_SITE_B_IP"
    time_start degrade_standby_b
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600

    echo "---WAIT FOR $standby_site_b TO BECOME DEGRADED---"
    dr waitforstate $standby_site_b STANDBY_DEGRADED 30 1200
    time_finish degrade_standby_b

    echo "---PREPARING STANDBY REJOIN TEST---"
    start_vdc_services "$DR_SITE_B_IP"
    time_start rejoin_standby_b

    echo "---WAIT FOR RESYNC $standby_site_b---"
    dr waitforstate $standby_site_b STANDBY_SYNCED 30 1200
    time_finish rejoin_standby_b

    # set degrade_standby_threshold_millis to default value: 15 minutes
    tune_dr_config ${DR_SITE_A_IP} degrade_standby_threshold_millis 900000

    # switchover test part
    echo "---WAIT BOTH ACTIVE AND STANBY TO BE STABLE, READY FOR SWITCHOVER---"
    dr waitforstable 10 800
    BOURNE_IPADDR=$DR_SITE_C_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitforstable 10 800

    echo "---SWITCHOVER FROM TO $active_site TO $standby_site_c---"
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitfornetworkhealth $standby_site_c GOOD 30 1200
    time_start switchover_standby_c
    dr switchover $standby_site_c || { echo "switchover dr standby site c failed."; exit 1; }

    echo "---WAIT ALL 3 SITES TO BE STABLE AFTER SWITCHOVER---"
    sleep 120
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitforstable 10 800

    BOURNE_IPADDR=$DR_SITE_B_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitforstable 10 800

    BOURNE_IPADDR=$DR_SITE_C_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitforstable 10 800

    echo "---WAIT FOR SWITCHOVER FROM $active_site TO $standby_site DONE---"
    dr waitforstate $standby_site_c ACTIVE 10 1200 && echo "standby has been switched into active"
    dr waitforstate $active_site STANDBY_SYNCED 10 1200 && echo "active has been switched into standby"
    time_finish switchover_standby_c

    # NOTE: active site and standby site has been switched, site A is standby and site C is active now

    # failover test part
    echo "Stopping active site $DR_SITE_C_NAME services to prepare for failover test ..."
    stop_vdc_services "$DR_SITE_C_IP"

    echo "Sleep 1m to wait standby ZK switch to participant"
    sleep 60
    echo "Wait for stable"
    BOURNE_IPADDR=$DR_SITE_B_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitforstable --ip "${DR_SITE_B_IP}" 10 600

    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitforstable --ip "${DR_SITE_A_IP}" 10 600

    echo "wait for site to become STANDBY_PAUSED before failover"
    dr waitforstate $active_site STANDBY_PAUSED 30 1200

    # COP-20918
    echo "Sleep 1m before failover"
    sleep 60

    time_start failover_standby_a
    echo "Failover to promote site $DR_SITE_A_NAME as active"
    dr failover --ip "${DR_SITE_A_IP}" $active_site || { echo "failover to site $DR_SITE_A_NAME failed."; exit 1; }
    echo "Sleep 1m to wait failover"
    sleep 60

    echo "Wait for stable"
    dr waitforstable --ip "${DR_SITE_A_IP}" 10 600

    echo "Wait site state to be PRIAMRY after failover"
    dr waitforstate $active_site ACTIVE 30 600 --ip "${DR_SITE_A_IP}"
    time_finish failover_standby_a

    # failback test part
    echo "Starting old active site $DR_SITE_C_NAME services ..."
    start_vdc_services "$DR_SITE_C_IP"
    echo "Sleep 300 seconds to wait old site to become ACTIVE_DEGRADED ..."
    sleep 300
    BOURNE_IPADDR=$DR_SITE_C_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitforstable --ip "${DR_SITE_C_IP}" 10 600
    dr waitforstate $standby_site_c ACTIVE_DEGRADED 30 600 --ip "${DR_SITE_C_IP}"

    # resume old active site (in ACTIVE_DEGRADED state)
    echo "Waiting $DR_SITE_A_NAME and $DR_SITE_C_NAME to be stable ..."
    wait_for_site_stable "${DR_SITE_A_IP}" "${DR_SITE_C_IP}"
    echo "---RESUME OLD ACTIVE SITE ${DR_SITE_C_NAME} (ACTIVE_DEGRADED) AFTER FAILOVER---"
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    time_start failback_standby_c
    dr resume --ip "${DR_SITE_A_IP}" $standby_site_c || { echo "resume dr standby site c failed."; exit 1; }
    echo "Resuming done"
    echo "Resuming site uuid is $standby_site_c"

    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    echo "Wait standby resuming done ..."
    dr waitforstate $standby_site_c STANDBY_SYNCED 30 1200
    time_finish failback_standby_c

    # delete standby
    echo "---DELETE STANDBY $standby_site_b---"
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    time_start delete_standby_b
    dr delete $standby_site_b

    echo "---WAIT FOR REMOVED $standby_site_b---should throw exceptions---"
    dr waitforstate $standby_site_b STANDBY_REMOVING 30 600 -n|| echo "standby removed"
    echo "Removing dr done"
    time_finish delete_standby_b

    # verify deleted
    echo "---VERIFY STANDBY DELETED $standby_site_b---should throw exceptions---"
    dr list $DR_SITE_B_NAME || echo "standby removed from site list"
    dr get $standby_site_b || echo "standby uuid removed from active"

    # verify active still works
    TESTPROJECT=dr_project_$$
    echo "---CREATE PROJECT $TESTPROJECT ON ACTIVE $active_site---"
    project show $TESTPROJECT &> /dev/null && return $?
    project create $TESTPROJECT --tenant $TENANT
    test_projectid=$(project query $TESTPROJECT)
    echo "Project id of $TESTPROJECT is $test_projectid."

    # verify stanby C still syncs
    BOURNE_IPADDR=$DR_SITE_C_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    project show $test_projectid

    # Below codes are used to test failover to ACTIVE_DEGRADED site
    # power off site A, then failover to site C, then power on site A and wait it to be ACTIVE_DEGRADED, then failover to it, and wait it to be ACTIVE
    # step1: poweroff site A, wait site C to be stable and paused
    # Comment these lines out because we don't support failover to site of ACTIVE_DEGRADED state in X-wing, thus no need for sanity
<<"COMMENT"
    echo "step1: poweroff site $DR_SITE_A_NAME ..."
    stop_vdc_services "$DR_SITE_A_IP"
    echo "Sleep 1m to wait standby ZK switch to participant"
    sleep 60
    echo "Wait for stable"
    BOURNE_IPADDR=$DR_SITE_C_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitforstable --ip "${DR_SITE_C_IP}" 10 600

    echo "wait for site to become STANDBY_PAUSED before failover"
    dr waitforstate $standby_site_c STANDBY_PAUSED 30 1200

    echo "Sleep 1m before failover"
    sleep 60

    #step2: failover to site C, wait it to be stable and ACTIVE
    echo "step2: failover to site $DR_SITE_C_NAME ..."
    dr failover --ip "${DR_SITE_C_IP}" $standby_site_c || { echo "failover to site $DR_SITE_C_NAME failed."; exit 1; }
    echo "Sleep 1m to wait failover"
    sleep 60
    echo "Wait for stable"
    dr waitforstable --ip "${DR_SITE_C_IP}" 10 600
    echo "Wait site state to be PRIAMRY after failover"
    dr waitforstate $standby_site_c ACTIVE 30 600 --ip "${DR_SITE_C_IP}"

    # step3: power on site A, wait it to become ACTIVE_DEGRADED
    echo "step3: power on old active site $DR_SITE_A_NAME ..."
    start_vdc_services "$DR_SITE_A_IP"
    echo "Sleep 300 seconds to wait old site to become ACTIVE_DEGRADED ..."
    sleep 300
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitforstable --ip "${DR_SITE_A_IP}" 10 600
    dr waitforstate $active_site ACTIVE_DEGRADED 30 600 --ip "${DR_SITE_A_IP}"

    # step4: power off site C
    echo "step4: poweroff site $DR_SITE_C_NAME to prepare failover ..."
    stop_vdc_services "$DR_SITE_C_IP"
    echo "sleep 180 seconds to wait standby site recognize the broken network ..."
    sleep 180
    # step5: failover to ACTIVE_DEGRADED site and wait it to be stable and ACTIVE
    echo "step5: failover to $DR_SITE_A_NAME ..."
    dr failover --ip "${DR_SITE_A_IP}" $active_site || { echo "failover to site $DR_SITE_A_NAME failed."; exit 1; }
    echo "Sleep 1m to wait failover"
    sleep 60

    echo "Wait for stable"
    dr waitforstable --ip "${DR_SITE_A_IP}" 10 600

    echo "Wait site state to be PRIAMRY after failover"
    dr waitforstate $active_site ACTIVE 30 600 --ip "${DR_SITE_A_IP}"
COMMENT
    #disabling pipefail option to pipe exit codes
    set +o pipefail
}

vdc_setup()
{
	vdc_common_setup	
	vdc_federation_setup
}

vdc_common_setup()
{
	login_nd_configure_smtp_nd_add_licenses

    syssvc $CONFIG_FILE "$BOURNE_IP" set_prop system_proxyuser_encpassword ${SYSADMIN_PASSWORD}
    tenant_setup
    
    project_setup
    projectid=$(project query $PROJECT)
    echo "Project id of $PROJECT is $projectid."
    
    export BOURNE_API_SYNC_TIMEOUT=5600
}

ipsec_setup()
{
    echo "No ipsec test setup needed"
}

ipsec_tests() 
{
    echo "--- IPSEC Key Rotation ---"
    ipsec keyrotate || { echo "key rotation failed."; exit 1; }
    echo "Key Rotation done"
    assert_ipsec_status "good"

    echo "--- Test IPSEC disable ---"
    ipsec disable || { echo "ipsec disable failed."; exit 1; }
    echo "ipsec disabled"
    assert_ipsec_status "disabled"

    echo "--- Test IPSEC enable ---"
    ipsec enable || { echo "ipsec enable failed."; exit 1; }
    echo "ipsec enabled"
    assert_ipsec_status "good"
}

assert_ipsec_status()
{
    echo "Checking IPsec status"
    local expect_status="${1}"
    local ipsec_status=""
    local result=1

    sleep 1
    for i in {1..60}; do
        echo "trying the ${i} times to see if ipsec status change to ${expect_status}."
        ipsec_status=$(ipsec check)
        [[ "${ipsec_status}" == *"${expect_status}"* ]] && { result=0; break; }
        echo "status is: ${ipsec_status}."
        echo "sleep 5 seconds"
        sleep 5
    done

    if [[ ${result} == 1 ]]; then
        echo "ipsec status is not right, expect: ${expect_status}, actually is: ${ipsec_status}"
        exit 1
    fi

    echo "IPsec status verified, which is: ${ipsec_status}"
}


sec_start_ldap_server()
{
    echo "Starting the in memory ldap server at http://${LOCAL_LDAP_SERVER_IP}:8082."
    resp_status=0
    while (($resp_status != 200))
    do
	  exec 3>&1
	  resp_status=$(curl -sw "%{http_code}" -o >(cat >&3) -k -H "Content-Type:application/json" -X POST -d '{"listener_name":"ViPRSanityLDAP"}' http://${LOCAL_LDAP_SERVER_IP}:8082/ldap-service/start)
	  if (($resp_status != 200))
	  then
	     echo "Response received : $resp_status. Retrying. Make sure ldap simulator service is started and listening at http://${LOCAL_LDAP_SERVER_IP}:8082."
	  else
	     echo "Response received : $resp_status. In memory ldap server started successfully."
	  fi
    done
}

# after this setup, vdcs will combine a federation
vdc_federation_setup()
{
	if [ "$VDC_ENDPOINT_B" == "$VDC_DEFAULT_ENDPOINT" ] ; then
        echo -e "\nFail: Usage of vdc sanity test is like:\n ./sanity 255.254.253.vdc1 vdc 255.254.253.vdc2"
        echo -e "\nOr you can test more vdcs use:\n ./sanity 255.254.253.vdc1 vdc 255.254.253.vdc2 255.254.253.vdc3"
        exit 1
    fi

    VDC_ENDPOINT_A=$BOURNE_IPADDR
    export BOURNE_IPADDR=$VDC_ENDPOINT_B
    if [ "$BOURNE_SECURITY_DISABLED" != '1' -a "$AUTH" != 'local' ] ; then
        security login $SYSADMIN $SYSADMIN_PASSWORD
    fi

    echo "do Login, Configure SMTP, and add controller and object licenses in $BOURNE_IPADDR"
    BOURNE_IP=$BOURNE_IPADDR
    login_nd_configure_smtp_nd_add_licenses
    syssvc $CONFIG_FILE "$BOURNE_IP" set_prop system_permit_root_ssh yes
    sleep 60
    vdc waitforstablestate 10 600
    echo "login_nd_configure_smtp_nd_add_licenses in $BOURNE_IPADDR done"

    echo "Get security key from $VDC_ENDPOINT_B"
    VDC_ENDPOINT_B_SECRETKEY=`vdc get_key|tail -1`
    echo "Key: $VDC_ENDPOINT_B_SECRETKEY"

    echo "Get cert chain from $VDC_ENDPOINT_B"
    VDC_ENDPOINT_B_CERTCHAIN=`vdc get_certchain`
    echo "Certchain: $VDC_ENDPOINT_B_CERTCHAIN"

    # if ${4} exists, will test three vdcs' CRUD operations
    if [ "$VDC_ENDPOINT_C" != "$VDC_DEFAULT_ENDPOINT" ] ; then
        VDC_ENDPOINT_C_NAME=vdc_name_C_$$
        VDC_ENDPOINT_C_SECRETKEY=
        VDC_ENDPOINT_C_ID=
        VDC_ENDPOINT_C_CERTCHAIN=

        export BOURNE_IPADDR=$VDC_ENDPOINT_C
        if [ "$BOURNE_SECURITY_DISABLED" != '1' -a "$AUTH" != 'local' ] ; then
            security login $SYSADMIN $SYSADMIN_PASSWORD
        fi

        echo "do Login, Configure SMTP, and add controller and object licenses in the 3rd vdc: $BOURNE_IPADDR"
        BOURNE_IP=$BOURNE_IPADDR
        login_nd_configure_smtp_nd_add_licenses
        syssvc $CONFIG_FILE "$BOURNE_IP" set_prop system_permit_root_ssh yes
        sleep 60
        vdc waitforstablestate 10 600
        echo "login_nd_configure_smtp_nd_add_licenses in the 3rd vdc: $BOURNE_IPADDR done"

        echo "Get security key from $VDC_ENDPOINT_C"
        VDC_ENDPOINT_C_SECRETKEY=`vdc get_key|tail -1`
        echo "Key: $VDC_ENDPOINT_C_SECRETKEY"

        echo "Get cert chain from $VDC_ENDPOINT_C"
        VDC_ENDPOINT_C_CERTCHAIN=`vdc get_certchain`
        echo "Certchain: $VDC_ENDPOINT_C_CERTCHAIN"
    fi

    export BOURNE_IPADDR=$VDC_ENDPOINT_A
    export BOURNE_IP=$VDC_ENDPOINT_A
    if [ "$BOURNE_SECURITY_DISABLED" != '1' -a "$AUTH" != 'local' ] ; then
        security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
    fi
    
    echo "Adding new vdc into current vipr system"
    vdc add $VDC_ENDPOINT_B_NAME "$VDC_ENDPOINT_B" $VDC_ENDPOINT_B_SECRETKEY "$VDC_ENDPOINT_B_CERTCHAIN"
    if [ $? -ne 0 ]; then
        echo "Add vdc B failed."
        exit 1
    fi
    sleep 30
    vdc waitforstablestate 10 600
    echo "Adding vdc done"
    
    if [ "$VDC_ENDPOINT_C" != "$VDC_DEFAULT_ENDPOINT" ] ; then
        echo "Adding the 3rd vdc into current vipr system"
        vdc add $VDC_ENDPOINT_C_NAME "$VDC_ENDPOINT_C" $VDC_ENDPOINT_C_SECRETKEY "$VDC_ENDPOINT_C_CERTCHAIN"
        if [ $? -ne 0 ]; then
            echo "Add vdc C failed."
            exit 1
        fi
        sleep 30
        vdc waitforstablestate 10 600
        echo "Adding the 3rd vdc done"
    fi

	echo "Test on global resource after vdc joined"
    if [ "$BOURNE_SECURITY_DISABLED" != '1' -a "$AUTH" != 'local' ] ; then
        security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
    fi
    project show $VDC_TEST_PROJECT &> /dev/null && return $?
    project create $VDC_TEST_PROJECT --tenant $TENANT
    echo "Project $VDC_TEST_PROJECT created on first vdc."
    echo "Login next vdc to list the created vdc"
    VDC_ENDPOINT_A=$BOURNE_IPADDR
    export BOURNE_IPADDR=$VDC_ENDPOINT_B
    security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
    project search $(echo $VDC_TEST_PROJECT | head -c 2)
    if [ "$VDC_ENDPOINT_C" != "$VDC_DEFAULT_ENDPOINT" ] ; then
        export BOURNE_IPADDR=$VDC_ENDPOINT_C
        security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
        project search $(echo $VDC_TEST_PROJECT | head -c 2)
    fi
    export BOURNE_IPADDR=$VDC_ENDPOINT_A
    echo "Test on global resource done"
    
    security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
    VDC_ENDPOINT_B_ID=`vdc get_id $VDC_ENDPOINT_B_NAME|tail -1`
    echo "new vdc id: $VDC_ENDPOINT_B_ID"

    if [ "$VDC_ENDPOINT_C" != "$VDC_DEFAULT_ENDPOINT" ] ; then
        VDC_ENDPOINT_C_ID=`vdc get_id $VDC_ENDPOINT_C_NAME|tail -1`
        echo "the 3rd vdc id: $VDC_ENDPOINT_C_ID"
    fi
}

vdc_discon_reconn_test()
{
    echo "Disconnecting the vdc in current vipr system"
    if [ "$BOURNE_SECURITY_DISABLED" != '1' -a "$AUTH" != 'local' ] ; then
        security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
    fi
    echo "Disconnecting with $VDC_ENDPOINT_B"

    # Stop all the services on the vdc which we want to disconnect, make it to inaccessable.
    stop_vdc_services "$VDC_ENDPOINT_B" 
    sleep 30
    vdc disconnect $VDC_ENDPOINT_B_ID
    if [ $? -ne 0 ]; then
        echo "Disconnect vdc failed."
        exit 1
    fi
    sleep 30
    vdc waitforstablestate 10 600
    echo "Disconnecting vdc done"
    
    project show $VDC_TEST_DISCONN_RECONN_PROJECT &> /dev/null && return $?
    project create $VDC_TEST_DISCONN_RECONN_PROJECT --tenant $TENANT
    echo "Project $VDC_TEST_DISCONN_RECONN_PROJECT created on remain vdc after disconnect."
    echo "Will check above project data after reconnecting vdc."
    
    echo "Reconnecting the vdc in current vipr system"
    if [ "$BOURNE_SECURITY_DISABLED" != '1' -a "$AUTH" != 'local' ] ; then
        security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
    fi

    # Restart all the services on the vdc which we disconnected, make it back online to reconnect.
    start_vdc_services "$VDC_ENDPOINT_B"
    sleep 60
    echo "Waiting for vdc stable $VDC_ENDPOINT_B"
    export BOURNE_IPADDR=$VDC_ENDPOINT_B
    security login $SYSADMIN $SYSADMIN_PASSWORD
    vdc waitforstablestate 10 600

    export BOURNE_IPADDR=$VDC_ENDPOINT_A
    security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD 
    vdc reconnect $VDC_ENDPOINT_B_ID
    if [ $? -ne 0 ]; then
        echo "Resconnect vdc failed."
        exit 1
    fi
    sleep 30
    vdc waitforstablestate 10 600
    echo "Reconnecting vdc done"
    
    echo "Start to check the Project $VDC_TEST_DISCONN_RECONN_PROJECT data."

    VDC_ENDPOINT_A=$BOURNE_IPADDR
    export BOURNE_IPADDR=$VDC_ENDPOINT_B
    security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD

    echo "Test on disconnect and reconnect vdc done"
    export BOURNE_IPADDR=$VDC_ENDPOINT_A
}
	
vdc_tests()
{
    vdc_discon_reconn_test

    echo "Updating the new vdc in current vipr system"
    if [ "$BOURNE_SECURITY_DISABLED" != '1' -a "$AUTH" != 'local' ] ; then
        security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
    fi
    VDC_ENDPOINT_B_NAME='VDC_B_NEW_NAME'
    vdc update $VDC_ENDPOINT_B_ID $VDC_ENDPOINT_B_NAME
    if [ $? -ne 0 ]; then
        echo "Update vdc failed."
        exit 1
    fi
    sleep 30
    vdc waitforstablestate 10 600
    echo "Updating vdc done"

    if [ "$VDC_ENDPOINT_C" != "$VDC_DEFAULT_ENDPOINT" ] ; then
        echo "Updating the 3rd vdc in current vipr system"
        VDC_ENDPOINT_C_NAME='VDC_C_NEW_NAME'
        vdc update $VDC_ENDPOINT_C_ID $VDC_ENDPOINT_C_NAME
        sleep 30
        vdc waitforstablestate 10 600
        echo "Updating the 3rd vdc done"
    fi

    echo "Removing the new vdc"
    vdc del $VDC_ENDPOINT_B_ID
    if [ $? -ne 0 ]; then
        echo "Remove vdc failed."
        exit 1
    fi
    sleep 30
    vdc waitforstablestate 10 600
    echo "Removing vdc done"

    if [ "$VDC_ENDPOINT_C" != "$VDC_DEFAULT_ENDPOINT" ] ; then
        echo "Removing the 3rd vdc"
        vdc del $VDC_ENDPOINT_C_ID
        sleep 30
        vdc waitforstablestate 10 600
        echo "Removing the 3rd vdc done"
    fi

    echo "Test on resource after vdc removed"
    if [ "$BOURNE_SECURITY_DISABLED" != '1' -a "$AUTH" != 'local' ] ; then
        security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
    fi
    project show $VDC_TEST_REMOVE_PROJECT &> /dev/null && return $?
    project create $VDC_TEST_REMOVE_PROJECT --tenant $TENANT
    echo "Project $VDC_TEST_REMOVE_PROJECT created on first vdc."
    echo "Login other vdc to search the created vdc"
    VDC_ENDPOINT_A=$BOURNE_IPADDR
    export BOURNE_IPADDR=$VDC_ENDPOINT_B

    security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
    # finding nothing is right, and it will throw python exception, we must catch it
    VDC_REMOVE_TEST_RESULT=`project search $(echo $VDC_TEST_REMOVE_PROJECT | head -c 5)\
    >> /tmp/pythonexception 2>&1 || echo "ok"`
    if [ "$VDC_REMOVE_TEST_RESULT" != "ok" ] ; then
        echo -e "\nError Happens in Test on resource after vdc removed in $BOURNE_IPADDR"
        exit 1
    fi
    echo "Find nothing, the result is $VDC_REMOVE_TEST_RESULT"
    if [ "$VDC_ENDPOINT_C" != "$VDC_DEFAULT_ENDPOINT" ] ; then
        export BOURNE_IPADDR=$VDC_ENDPOINT_C
        security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
        VDC_REMOVE_TEST_RESULT=`project search $(echo $VDC_TEST_REMOVE_PROJECT | head -c 5)\
        >> /tmp/pythonexception 2>&1 || echo "ok"`
        if [ "$VDC_REMOVE_TEST_RESULT" != "ok" ] ; then
            echo -e "\nError Happens in Test on resource after vdc removed in $BOURNE_IPADDR"
            exit 1
        fi
        echo "Find nothing, the result is $VDC_REMOVE_TEST_RESULT"
    fi
    export BOURNE_IPADDR=$VDC_ENDPOINT_A
    echo "Test on resource after vdc removed done"

    echo "Delete project $VDC_TEST_PROJECT."
    if [ "$BOURNE_SECURITY_DISABLED" != '1' -a "$AUTH" != 'local' ] ; then
        security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
    fi
    project delete $VDC_TEST_PROJECT

    echo "ALL VDC TEST DONE"
}

tune_dr_config()
{
    VIP=$1
    CONFIG_KEY=$2
    CONFIG_VALUE=$3
    OPT=""
    if [[ $VIP == \[* ]];
    then
        tmp=${VIP#*[}
        VIP=${tmp%]*}
        OPT="-6"
    fi

    echo "VIP=${VIP} OPT=${OPT} KEY=${CONFIG_KEY} VALUE=${CONFIG_VALUE}"
    SSH ${VIP} "/opt/storageos/bin/zkutils tune_dr_config ${CONFIG_KEY} ${CONFIG_VALUE}" ${OPT}
}

stop_vdc_services()
{
    VIP=$1
    OPT=""
    NETWORK_PATTERN='^network_.*_ipaddr='
    if [[ $VIP == \[* ]];
    then
        tmp=${VIP#*[}
        VIP=${tmp%]*}
        OPT="-6"
        NETWORK_PATTERN='^network_.*_ipaddr6='
    fi

    echo "VIP=$VIP OPT=$OPT"
    IP_ADDRESS_LIST=`SSH $VIP "/etc/systool --getprops | grep '$NETWORK_PATTERN' | cut -d= -f 2 " $OPT`

    echo "stop services on VDC($VIP) that has following IP addresses $IP_ADDRESS_LIST"
    for ip_addr in $IP_ADDRESS_LIST
    do
        SSH $ip_addr 'service nginx stop;' $OPT
        SSH $ip_addr '/etc/storageos/storageos stop;' $OPT
    done
}

start_vdc_services()
{
    VIP=$1
    NETWORK_PATTERN='^network_.*_ipaddr='
    if [[ $VIP == \[* ]];
    then
        tmp=${VIP#*[}
        VIP=${tmp%]*}
        OPT="-6"
        NETWORK_PATTERN='^network_.*_ipaddr6='
    fi

    echo "VIP=$VIP OPT=$OPT"
    IP_ADDRESS_LIST=`SSH $VIP "/etc/systool --getprops | grep '$NETWORK_PATTERN' | cut -d= -f 2 " $OPT`
    echo "start services on VDC($VIP) that has following IP addresses $IP_ADDRESS_LIST"
    for ip_addr in $IP_ADDRESS_LIST
    do
        SSH $ip_addr '/etc/storageos/storageos start;' $OPT
        SSH $ip_addr 'service nginx start;' $OPT
    done
}

#Usage: sanity <conf path> <vip> backuprestore backup-server-uri username password domain
# E.g.: sanity conf/sanity.conf 137.69.169.21 backuprestore ftp://X.X.X.X/foo root root-password
# E.g.: sanity conf/sanity.conf 137.69.169.21 backuprestore smb://X.X.X.X/foo user1 user1-password user1-domain
backuprestore_setup()
{
    echo "Setup backup restore environment begin"
    login_nd_configure_smtp_nd_add_licenses
    vip=$BOURNE_IPADDR

    echo "Enable root ssh permit and configure backup server"
    syssvc $CONFIG_FILE "$vip" set_prop system_permit_root_ssh yes

    syssvc $CONFIG_FILE "$vip" set_prop backup_external_server_type "${BACKUP_SERVER_TYPE}"
    syssvc $CONFIG_FILE "$vip" set_prop backup_external_location_username "${BACKUP_SERVER_USERNAME}"
    syssvc $CONFIG_FILE "$vip" set_prop backup_external_location_password "${BACKUP_SERVER_PASSWORD}"
    syssvc $CONFIG_FILE "$vip" set_prop backup_external_location_domain "${BACKUP_SERVER_DOMAIN}"

    syssvc $CONFIG_FILE "$vip" set_prop backup_external_location_url "${BACKUP_SERVER_URL}"

    echo -e "Done\nurl=${BACKUP_SERVER_URL} username=${BACKUP_SERVER_USERNAME} password=${BACKUP_SERVER_PASSWORD} domain=${BACKUP_SERVER_DOMAIN}"
}

backuprestore_tests()
{
    backupname="sanity-${seed}"
    backupname2="abc-${seed}"
    filepath='/tmp/sanity.zip'
    remote_backup_file=""

    backup_tests "${backupname}" "${backupname2}" "${filepath}"
    set +E
    restore_tests "${filepath}"
    set -E
}

backup_tests()
{
    local backupname=$1
    local backupname2=$2
    local filepath=$3

    echo "Creating backup(${backupname})"
    backup create "${backupname}"

    if [ $? -ne 0 ]; then
        echo "Create backup ${backupname} failed"
        exit 1
    fi

    echo "Create backup ${backupname} successful"

    echo "Creating another backup(${backupname2})"
    backup create "${backupname2}"

    if [ $? -ne 0 ]; then
        echo "Create backup ${backupname2} failed"
        exit 1
    fi

    echo "Create backup ${backupname2} successful"

    echo "Listing local backup(${backupname})"
    local found=$(backup list "${backupname}")
    echo "found=$found"

    if [ "$found"x != "${backupname}"x ]; then
        echo "List backup failed"
        exit 1
    fi

    echo "List local backup ${backupname} successful"

    echo "Query local backup ${backupname} info"
    backup query-info "${backupname}" "true"

    echo "Uploading backup(${backupname}) to ${BACKUP_SERVER_URL}"
    backup upload "${backupname}"

    if [ $? != 0 ]; then
        echo "Upload backup ${backupname} failed"
        exit 1
    fi

    echo "Wait for upload ${backupname} to complete"

    for(( i=0; i<60; i++ )); do
       status=$(backup query-upload "${backupname}")
       echo "status=${status}"

       if [[ "${status}" == "DONE" || "${status}" == "FAILED" ]] ; then
           break;
       fi

       sleep 10s
    done

    echo "status=${status}"
    if [[ "${status}" != "DONE" ]]; then
        echo "upload backup ${backupname} failed status=${status}"
        exit 1
    fi

    echo "Listing external backup(${backupname} from ${BACKUP_SERVER_URL})"

    found=""
    for(( i=0; i<60; i++ )); do
       found=$(backup list-external "${backupname}")
       echo "found=${found}"

       if [[ "${found}" != "" ]]; then
           break;
       fi
       sleep 10s
    done

    if [[ "${found}" == "" ]]; then
       echo "Failed to find backup ${backupname} on ${BACKUP_SERVER_URL}"
       exit 1
    fi

    echo "Query remote backup ${found} info"
    backup query-info "${found}" "false"

    echo "Pull backup(${found} from ${BACKUP_SERVER_URL})"
    backup pull "${found}"

    if [[ $? -ne 0 ]]; then
        echo "pull backup failed"
        exit 1
    fi

    echo "Start to query pull status"
    local status=""

    for (( i=0; i<60; i++ )); do
       status=$(backup query-pull "${found}" "false")
       echo "status=${status}"

       if [[ "${status}" == "DOWNLOAD_SUCCESS" ]]; then
           break;
       fi

       sleep 10s
    done

    if [[ "${status}" != "DOWNLOAD_SUCCESS" ]]; then
       echo "Failed to pull backup ${found}"
       exit 1
    fi

    remote_backup_file="${found}"

    echo "Deleting backup(${backupname2})"
    backup delete "${backupname2}"

    if [[ $? -ne 0 ]]; then
        echo "Delete backup failed"
        exit 1
    fi

    echo "Delete backup ${backupname2} successful"

    echo "Listing backup(${backupname2})"

    found=`backup list "${backupname2}"`
    echo "found=$found"

    if [[ "$found"x == "${backupname2}"x ]]; then
        echo "Backup(${backupname}-tmp) should not exist"
        exit 1
    fi

    echo "Downloading backup(${backupname}) to ${filepath}"

    backup download "${backupname}" "${filepath}"

    if [[ $? -ne 0 ||  ! -f ${filepath} ]]; then
        echo "Download backup failed"
        exit 1
    fi

    echo "Download backup successful"

    echo "Deleting backup(${backupname})"

    backup delete "${backupname}"

    if [[ $? -ne 0 ]]; then
        echo "Delete backup failed"
        exit 1
    fi

    echo "Delete backup successful"
}

restore_tests()
{
    filepath="$1"
    password=$SYSADMIN_PASSWORD

    echo "Restore from pulled backup ${remote_backup_file} password=${password}"
    backup restore "${remote_backup_file}" "false" "${password}"

    echo "Wait for stable"
    sleep 60s

    for (( i=0; i<240; i++ )); do
       security login $SYSADMIN $SYSADMIN_PASSWORD
       if [[ $? -eq 0 ]]; then
            break;
       fi
       echo "login failed retry later"
       sleep 10s
    done

    echo "Uploading backup file ${filepath} to $BOURNE_IPADDR"
    SCP $BOURNE_IPADDR ${filepath} ${filepath}

    echo "Restoring backup file ${filepath} on $BOURNE_IPADDR"    
    SSH $BOURNE_IPADDR "echo -e 'yes\n$password' | /opt/storageos/bin/restore '${filepath}'"
    ret=$?
    echo "ret=$ret"
    if [[ ${ret} -ne 0 ]]; then
        echo "Please check /opt/storageos/logs/bkutils.log for details"
        exit 1
    fi
}

recovery_setup()
{
    echo "Setup minority node corrupted cluster($BOURNE_IPADDR) begin"
    vip=$BOURNE_IPADDR
    corrupted_node_id=2

    echo "Enable root ssh permit"
    syssvc $CONFIG_FILE "$vip" set_prop system_permit_root_ssh yes
    echo "Enable root ssh permit done"
    recovery wait_for_stable 10 600

    if [[ $vip == \[* ]]; then
        tmp=${vip#*[}
        vip=${tmp%]*}
        opt="-6"
        network_patten="^network_${corrupted_node_id}_ipaddr6="
    else
        opt=""
        network_patten="^network_${corrupted_node_id}_ipaddr="
    fi

    echo "network_patten=$network_patten opt=$opt"
    node_count=`SSH $vip "/etc/systool --getprops | grep '^node_count=' | cut -d= -f 2" $opt`
    if [[ $node_count -lt 3 ]]; then
        echo "Could not do node recovery on cluster that node count($node_count) less then 3"
        exit 2
    fi

    ip_addr=`SSH $vip "/etc/systool --getprops | grep '$network_patten' | cut -d= -f 2 " $opt`
    export RECOVERY_CORRUPTED_NODE_IP=$ip_addr

    echo "Simulating vipr${corrupted_node_id}($RECOVERY_CORRUPTED_NODE_IP) crash"
    recovery_node_crash $RECOVERY_CORRUPTED_NODE_IP
    echo "Make vipr${corrupted_node_id}($RECOVERY_CORRUPTED_NODE_IP) corrupted done"

    sleep 60
    echo "Setup minority node corrupted cluster($BOURNE_IPADDR) successful"
}

recovery_tests()
{
    echo "Begin to run node recovery on $BOURNE_IPADDR"
    recovery trigger

    for(( i=0; i<10; i++ )); do
       recovery_status=`recovery get_status`
       echo "Cuttent recovery status is: $recovery_status"
       if [ recovery_status != '' ]; then
           break;
       fi
       sleep 3 
    done

    recovery_status=`recovery get_status`
    if [ recovery_status != '' ]; then
        echo "Simulating vipr${corrupted_node_id}($RECOVERY_CORRUPTED_NODE_IP) redeployment"
        recovery_node_redeploy $RECOVERY_CORRUPTED_NODE_IP
        echo "Make vipr${corrupted_node_id}($RECOVERY_CORRUPTED_NODE_IP) redeployed done"
    else
        echo "Unexcepted error"
        recovery_node_startup $RECOVERY_CORRUPTED_NODE_IP
        exit 1
    fi

    while true; do
        recovery_status=`recovery get_status`
        echo "Cuttent recovery status is: $recovery_status"
        if [ recovery_status == '' ]; then
            echo "Unexcepted error"
            exit 1
        elif [[ "${recovery_status[*]}" == *"DONE"* ]]; then
            echo "Node recovery successful"
            break
        elif [[ "${recovery_status[*]}" == *"FAILED"* ]]; then
            echo "Node recovery failed"
            exit 1
        elif [[ "${recovery_status[*]}" == *"REPAIRING"* ]]; then
            db_repair_status=`recovery get_db_repair_status`
            echo "Db repair status is: $db_repair_status"
            sleep 10
        else
            sleep 5
        fi
    done
}

recovery_node_crash()
{
    ip_addr=$1
    opt=$2
    SSH $ip_addr '/etc/storageos/storageos stop; rm -rf /data/db/1 /data/geodb/1 /data/zk/*' $opt
}

recovery_node_redeploy()
{
    ip_addr=$1
    opt=$2
    SSH $ip_addr 'echo startupmode=hibernate > /data/db/startupmode; echo startupmode=hibernate > /data/geodb/startupmode; chown -R storageos:storageos /data; /etc/storageos/storageos start' $opt
}

recovery_node_startup()
{
    ip_addr=$1
    opt=$2
    SSH $ip_addr '/etc/storageos/storageos start' $opt
}

objcontrolsvc_setup()
{
    echo "Nothing to do for objcontrol setup"
}

objcontrolsvc_tests()
{
    nodeobj create "datanode-001"
    echo "Create node object datanode-001 succeed."
    nodeobj create "datanode-002"
    echo "Create node object datanode-002 succeed."
    nodeobj create "datanode-003"
    echo "Create node object datanode-003 succeed."
}

# Save the latest token file to token.txt on error for debugging and remove the current one
save_token_file() {
    if [ "$BOURNE_SECURITY_DISABLED" != "1" ]; then
       [ -f ${BOURNE_TOKEN_FILE} ] && mv ${BOURNE_TOKEN_FILE} ${BOURNE_SAVED_TOKEN_FILE}
    fi
}

_failure() {
    echo "*******************************************************" >&2
    if [ -f ${CMD_OUTPUT} -a -s ${CMD_OUTPUT} ]; then
	/usr/bin/cat ${CMD_OUTPUT}                                     >&2
    else
	echo "Command failed with $2, but no command had no output"              >&2
    fi
    echo "*******************************************************" >&2
        
    echo "***"                                                   >&2
    secho "FAILED! $0:$1: `eval echo ${cmd}`"                    >&2
    echo "***"                                                   >&2
    date
    run_undo_commands
    task check_for_pending 
    save_token_file
    exit 1
}

_success() {
    echo "***"                                                   >&2
    secho "PASSED!"                                              >&2
    echo "***"                                                   >&2
    date
    save_token_file
    finalize_undo_log
}

#counterpart for run
#executes a command that is expected to fail
fail(){
    cmd=$*
    echo === $cmd
    trap - ERR
    if [ "${HIDE_OUTPUT}" = "" -o "${HIDE_OUTPUT}" = "1" ]; then
	$cmd &> ${CMD_OUTPUT}
    else
	$cmd 2>&1
    fi

    status=$?
    if [ $status -eq 0 ] ; then
        echo '**********************************************************************'
        echo $cmd succeeded, which should not have happened
	cat ${CMD_OUTPUT}
        echo '**********************************************************************'
        trap '_failure $LINENO' ERR
        set_undo $cmd
        exit 1
    fi
    secho "$cmd failed, which is the expected ouput"
    trap '_failure $LINENO' ERR
}

trap '_failure $LINENO' ERR INT
set -E
if [ "$3" != "vdc" ] && [ "$3" != "dr" ] && [ "$3" != "backuprestore" ] && [ "$3" != "unityfile" ]; then
    common_setup	
fi

datastore_setup(){
    opname=$1
    fileCos=$2
    opsize=$3
    deviceType=$4

    echo "create datastore for device" $deviceType
    if [ "$deviceType" = "nfsexportpoints" ]; then
        run datastore create nfsexportpoints $opname $NH --size $opsize --mountpoint $opname
    fi
    if [ "$deviceType" = "filesystems" ]; then
        run datastore create filesystems $opname $NH --filecos $fileCos --size $opsize
    fi
}

output_conf(){
    file=$WS_SETUP
    echo "nh="$NH > "$file"
    echo "tenant="$TENANT >> "$file"
    echo "namespace="$NAMESPACE >> "$file"
    echo "project="$PROJECT >> "$file"
    echo "cos="${WS_SETUP_COS#,} >> "$file"
    echo "user=$WS_UID" >> "$file"
    echo "secretkey=$WS_SECRET" >> "$file"
    echo "bucket=${WS_SETUP_BUCKETS# }" >> "$file"
}

application_vpool_setup()
{
    secho "application_vpool_setup()"

    # Create CoS for VNX
    run cos create block $APP_VPOOL_VNX_BLOCK                       \
        --description 'Consistency-Group-Block-VNX-CoS' false   \
                            --protocols FC        \
                            --numpaths 1               \
                            --max_snapshots 10         \
                            --provisionType 'Thin'     \
                            --neighborhoods $APP_VARRAY1        \
                            --multiVolumeConsistency

    run cos update block $APP_VPOOL_VNX_BLOCK --storage $APPLICATION_VNX1_NATIVEGUID
    run cos allow $APP_VPOOL_VNX_BLOCK block $TENANT

    # Create CoS for VNX + VPLEX local
    run cos create block $APP_VPOOL_VNX_VPLEX_LOCAL                       \
        --description 'Consistency-Group-VPLEX-local-VNX-CoS' false   \
                            --protocols FC        \
                            --numpaths 2               \
                            --max_snapshots 10         \
                            --provisionType 'Thin'     \
                            --neighborhoods $APP_VARRAY1        \
                            --highavailability vplex_local           \
                            --multiVolumeConsistency

    run cos update block $APP_VPOOL_VNX_VPLEX_LOCAL --storage $APPLICATION_VNX1_NATIVEGUID
    run cos allow $APP_VPOOL_VNX_VPLEX_LOCAL block $TENANT

    # Create CoS for VMAX2
    run cos create block $APP_VPOOL_VMAX2_BLOCK               \
        --description 'Consistency-Group-Block-VMAX2-CoS' false   \
                            --protocols FC        \
                            --numpaths 1               \
                            --max_snapshots 10         \
                            --provisionType 'Thin'     \
                            --neighborhoods $APP_VARRAY2        \
                            --multiVolumeConsistency

    run cos update block $APP_VPOOL_VMAX2_BLOCK --storage $APPLICATION_VMAX2_NATIVEGUID
    run cos allow $APP_VPOOL_VMAX2_BLOCK block $TENANT

    # Create CoS for VMAX2 + vplex local
    run cos create block $APP_VPOOL_VMAX2_VPLEX_LOCAL \
        --description 'Consistency-Group-Block-VMAX2-VPLEX-local-CoS' false   \
                            --protocols FC        \
                            --numpaths 2               \
                            --max_snapshots 10         \
                            --provisionType 'Thin'     \
                            --neighborhoods $APP_VARRAY2        \
                            --highavailability vplex_local           \
                            --multiVolumeConsistency

    run cos update block $APP_VPOOL_VMAX2_VPLEX_LOCAL --storage $APPLICATION_VMAX2_NATIVEGUID
    run cos allow $APP_VPOOL_VMAX2_VPLEX_LOCAL block $TENANT

    # Create CoS for VMAX3
    run cos create block $APP_VPOOL_VMAX3_BLOCK               \
        --description 'Consistency-Group-Block-VMAX3-CoS' false   \
                            --protocols FC        \
                            --numpaths 1               \
                            --max_snapshots 10         \
                            --provisionType 'Thin'     \
                            --neighborhoods $APP_VARRAY3        \
                            --multiVolumeConsistency

    run cos update block $APP_VPOOL_VMAX3_BLOCK --storage $APPLICATION_VMAX3_NATIVEGUID
    run cos allow $APP_VPOOL_VMAX3_BLOCK block $TENANT

    # Create CoS for VMAX3 + vplex local
    run cos create block $APP_VPOOL_VMAX3_VPLEX_LOCAL \
        --description 'Consistency-Group-Block-VMAX3-VPLEX-local-CoS' false   \
                            --protocols FC        \
                            --numpaths 2               \
                            --max_snapshots 10         \
                            --provisionType 'Thin'     \
                            --neighborhoods $APP_VARRAY3        \
                            --highavailability vplex_local           \
                            --multiVolumeConsistency

    run cos update block $APP_VPOOL_VMAX3_VPLEX_LOCAL --storage $APPLICATION_VMAX3_NATIVEGUID
    run cos allow $APP_VPOOL_VMAX3_VPLEX_LOCAL block $TENANT

    # Create CoS for ha side of vplex distributed
    run cos create block $APP_VPOOL_VNX_HA               \
        --description 'Consistency-Group-HA-VNX-CoS' false   \
                            --protocols FC        \
                            --numpaths 1               \
                            --max_snapshots 10         \
                            --provisionType 'Thin'     \
                            --neighborhoods $APP_VARRAY1 

    run cos update block $APP_VPOOL_VNX_HA --storage $APPLICATION_VNX1_NATIVEGUID
    run cos allow $APP_VPOOL_VNX_HA block $TENANT

    # Create CoS for VMAX2/VNX + vplex distributed
    run cos create block $APP_VPOOL_VMAX2_VPLEX_DIST \
        --description 'Consistency-Group-Block-VMAX2-VPLEX-dist-CoS' false   \
                            --protocols FC        \
                            --numpaths 2               \
                            --max_snapshots 10         \
                            --provisionType 'Thin'     \
                            --neighborhoods $APP_VARRAY2        \
                            --highavailability vplex_local           \
                            --highavailability vplex_distributed \
                            --haNeighborhood $APP_VARRAY1 \
                            --haCos $APP_VPOOL_VNX_HA \
                            --multiVolumeConsistency

    run cos update block $APP_VPOOL_VMAX2_VPLEX_DIST --storage $APPLICATION_VMAX2_NATIVEGUID
    run cos allow $APP_VPOOL_VMAX2_VPLEX_DIST block $TENANT

}

application_sim_vpool_setup()
{
    secho "application_sim_vpool_setup()"

    if [ $APPLICATION_TEST_VNX -eq 1 ]; then
        # Create CoS for VNX
        run cos create block $APP_VPOOL_VNX_BLOCK                       \
            --description 'Consistency-Group-Block-VNX-CoS' false   \
                            --protocols FC        \
                            --numpaths 1               \
                            --max_snapshots 10         \
                            --provisionType 'Thin'     \
                            --neighborhoods $APP_VARRAY1        \
                            --multiVolumeConsistency

        run cos update block $APP_VPOOL_VNX_BLOCK --storage $APPLICATION_VNX1_NATIVEGUID
        run cos allow $APP_VPOOL_VNX_BLOCK block $TENANT

        # Create CoS for VNX + VPLEX local
        run cos create block $APP_VPOOL_VNX_VPLEX_LOCAL                       \
            --description 'Consistency-Group-VPLEX-local-VNX-CoS' false   \
                            --protocols FC        \
                            --numpaths 2               \
                            --max_snapshots 10         \
                            --provisionType 'Thin'     \
                            --neighborhoods $APP_VARRAY1        \
                            --highavailability vplex_local           \
                            --multiVolumeConsistency

        run cos update block $APP_VPOOL_VNX_VPLEX_LOCAL --storage $APPLICATION_VNX1_NATIVEGUID
        run cos allow $APP_VPOOL_VNX_VPLEX_LOCAL block $TENANT

        # Create CoS for ha side of vplex distributed
        run cos create block $APP_VPOOL_VNX_HA               \
            --description 'Consistency-Group-HA-VNX-CoS' false   \
                            --protocols FC        \
                            --numpaths 1               \
                            --max_snapshots 10         \
                            --provisionType 'Thin'     \
                            --neighborhoods $APP_VARRAY1

        run cos update block $APP_VPOOL_VNX_HA --storage $APPLICATION_VNX1_NATIVEGUID
        run cos allow $APP_VPOOL_VNX_HA block $TENANT
    fi

    if [ $APPLICATION_TEST_VMAX2 -eq 1 ]; then
        # Create CoS for VMAX2
        run cos create block $APP_VPOOL_VMAX2_BLOCK               \
            --description 'Consistency-Group-Block-VMAX2-CoS' false   \
                            --protocols FC        \
                            --numpaths 1               \
                            --max_snapshots 10         \
                            --provisionType 'Thin'     \
                            --neighborhoods $APP_VARRAY1        \
                            --multiVolumeConsistency

        run cos update block $APP_VPOOL_VMAX2_BLOCK --storage $APPLICATION_VMAX2_NATIVEGUID1
        run cos allow $APP_VPOOL_VMAX2_BLOCK block $TENANT

        # Create CoS for VMAX2 + vplex local
        run cos create block $APP_VPOOL_VMAX2_VPLEX_LOCAL \
            --description 'Consistency-Group-Block-VMAX2-VPLEX-local-CoS' false   \
                            --protocols FC        \
                            --numpaths 2               \
                            --max_snapshots 10         \
                            --provisionType 'Thin'     \
                            --neighborhoods $APP_VARRAY1        \
                            --highavailability vplex_local           \
                            --multiVolumeConsistency

        run cos update block $APP_VPOOL_VMAX2_VPLEX_LOCAL --storage $APPLICATION_VMAX2_NATIVEGUID1
        run cos allow $APP_VPOOL_VMAX2_VPLEX_LOCAL block $TENANT

        # Create CoS for ha side of vplex distributed
        run cos create block $APP_VPOOL_VMAX2_HA               \
            --description 'Consistency-Group-HA-VMAX2-CoS' false   \
                            --protocols FC        \
                            --numpaths 1               \
                            --max_snapshots 10         \
                            --provisionType 'Thin'     \
                            --neighborhoods $APP_VARRAY2

        run cos update block $APP_VPOOL_VMAX2_HA --storage $APPLICATION_VMAX2_NATIVEGUID2
        run cos allow $APP_VPOOL_VMAX2_HA block $TENANT

        # Create CoS for VMAX2/VNX + vplex distributed
        run cos create block $APP_VPOOL_VMAX2_VPLEX_DIST \
            --description 'Consistency-Group-Block-VMAX2-VPLEX-dist-CoS' false   \
                            --protocols FC        \
                            --numpaths 2               \
                            --max_snapshots 10         \
                            --provisionType 'Thin'     \
                            --neighborhoods $APP_VARRAY1        \
                            --highavailability vplex_distributed \
                            --haNeighborhood $APP_VARRAY2 \
                            --haCos $APP_VPOOL_VMAX2_HA \
                            --multiVolumeConsistency

        run cos update block $APP_VPOOL_VMAX2_VPLEX_DIST --storage $APPLICATION_VMAX2_NATIVEGUID1
        run cos allow $APP_VPOOL_VMAX2_VPLEX_DIST block $TENANT

        # Create CoS for VMAX2 RP target
        run cos create block $APP_VPOOL_VMAX2_BLOCK_RPTGT               \
            --description 'Consistency-Group-Block-rp-tgt-VMAX2-CoS' false   \
                            --protocols FC        \
                            --numpaths 1               \
                            --max_snapshots 10         \
                            --provisionType 'Thin'     \
                            --neighborhoods $APP_VARRAY2

        run cos update block $APP_VPOOL_VMAX2_BLOCK_RPTGT --storage $APPLICATION_VMAX2_NATIVEGUID2
        run cos allow $APP_VPOOL_VMAX2_BLOCK_RPTGT block $TENANT

        # Create CoS for VMAX2 + RP block
        run cos create block $APP_VPOOL_VMAX2_BLOCK_RPSRC               \
            --description 'Consistency-Group-Block-rp-src-VMAX2-CoS' false   \
                            --protocols FC        \
                            --numpaths 2               \
                            --max_snapshots 10         \
                            --provisionType 'Thin'     \
                            --neighborhoods $APP_VARRAY1        \
                            --multiVolumeConsistency \
                            --protectionCoS $APP_VARRAY2':'$APP_VPOOL_VMAX2_BLOCK_RPTGT':min' \
                            --rp_copy_mode ASYNCHRONOUS \
                            --rp_rpo_value 5 \
                            --rp_rpo_type MINUTES

        run cos update block $APP_VPOOL_VMAX2_BLOCK_RPSRC --storage $APPLICATION_VMAX2_NATIVEGUID1
        run cos allow $APP_VPOOL_VMAX2_BLOCK_RPSRC block $TENANT

        # Create CoS for VMAX2 + vplex local RP target
        run cos create block $APP_VPOOL_VMAX2_VPLEX_LOCAL_RPTGT \
            --description 'Consistency-Group-Block-VMAX2-VPLEX-local-rptgt-CoS' false   \
                            --protocols FC        \
                            --numpaths 2               \
                            --max_snapshots 10         \
                            --provisionType 'Thin'     \
                            --neighborhoods $APP_VARRAY3        \
                            --multiVolumeConsistency

        run cos update block $APP_VPOOL_VMAX2_VPLEX_LOCAL_RPTGT --storage $APPLICATION_VMAX2_NATIVEGUID3
        run cos allow $APP_VPOOL_VMAX2_VPLEX_LOCAL_RPTGT block $TENANT

        # Create CoS for VMAX2 + RP + vplex local
        run cos create block $APP_VPOOL_VMAX2_RP_VPLEX_LOCAL \
            --description 'Consistency-Group-Block-VMAX2-RP-VPLEX-local-CoS' false   \
                            --protocols FC        \
                            --numpaths 2               \
                            --max_snapshots 10         \
                            --provisionType 'Thin'     \
                            --neighborhoods $APP_VARRAY1        \
                            --highavailability vplex_local           \
                            --multiVolumeConsistency	\
                            --protectionCoS $APP_VARRAY3':'$APP_VPOOL_VMAX2_VPLEX_LOCAL_RPTGT':min' \
                            --rp_copy_mode ASYNCHRONOUS \
                            --rp_rpo_value 5 \
                            --rp_rpo_type MINUTES

                            #--neighborhoods $APP_VARRAY2        \
                            #--protectionCoS $APP_VARRAY1':'$APP_VPOOL_VMAX2_VPLEX_LOCAL':min' \
        #run cos update block $APP_VPOOL_VMAX2_RP_VPLEX_LOCAL --storage $APPLICATION_VMAX2_NATIVEGUID2
        run cos update block $APP_VPOOL_VMAX2_RP_VPLEX_LOCAL --storage $APPLICATION_VMAX2_NATIVEGUID1
        run cos allow $APP_VPOOL_VMAX2_RP_VPLEX_LOCAL block $TENANT

        # Create CoS for VMAX2 + metropoint
        run cos create block $APP_VPOOL_VMAX2_METROPOINT \
            --description 'Consistency-Group-Block-VMAX2-Metropoint-CoS' false   \
                            --protocols FC        \
                            --numpaths 2               \
                            --max_snapshots 10         \
                            --provisionType 'Thin'     \
                            --neighborhoods $APP_VARRAY1        \
                            --highavailability vplex_distributed \
                            --haNeighborhood $APP_VARRAY2 \
                            --haCos $APP_VPOOL_VMAX2_HA \
                            --multiVolumeConsistency	\
                            --protectionCoS $APP_VARRAY3':'$APP_VPOOL_VMAX2_VPLEX_LOCAL_RPTGT':min' \
                            --rp_copy_mode ASYNCHRONOUS \
                            --rp_rpo_value 5 \
                            --rp_rpo_type MINUTES

        run cos update block $APP_VPOOL_VMAX2_METROPOINT --storage $APPLICATION_VMAX2_NATIVEGUID1
        run cos allow $APP_VPOOL_VMAX2_METROPOINT block $TENANT

    fi

    if [ $APPLICATION_TEST_VMAX3 -eq 1 ]; then
        # Create CoS for VMAX3
        run cos create block $APP_VPOOL_VMAX3_BLOCK               \
            --description 'Consistency-Group-Block-VMAX3-CoS' false   \
                            --protocols FC        \
                            --numpaths 1               \
                            --max_snapshots 10         \
                            --provisionType 'Thin'     \
                            --neighborhoods $APP_VARRAY1        \
                            --multiVolumeConsistency

        run cos update block $APP_VPOOL_VMAX3_BLOCK --storage $APPLICATION_VMAX3_NATIVEGUID1
        run cos allow $APP_VPOOL_VMAX3_BLOCK block $TENANT

        # Create CoS for VMAX3 + vplex local
        run cos create block $APP_VPOOL_VMAX3_VPLEX_LOCAL \
            --description 'Consistency-Group-Block-VMAX3-VPLEX-local-CoS' false   \
                            --protocols FC        \
                            --numpaths 2               \
                            --max_snapshots 10         \
                            --provisionType 'Thin'     \
                            --neighborhoods $APP_VARRAY1        \
                            --highavailability vplex_local           \
                            --multiVolumeConsistency

        run cos update block $APP_VPOOL_VMAX3_VPLEX_LOCAL --storage $APPLICATION_VMAX3_NATIVEGUID1
        run cos allow $APP_VPOOL_VMAX3_VPLEX_LOCAL block $TENANT

        # Create CoS for ha side of vplex distributed
        run cos create block $APP_VPOOL_VMAX3_HA               \
            --description 'Consistency-Group-HA-VMAX2-CoS' false   \
                            --protocols FC        \
                            --numpaths 1               \
                            --max_snapshots 10         \
                            --provisionType 'Thin'     \
                            --neighborhoods $APP_VARRAY2

        run cos update block $APP_VPOOL_VMAX3_HA --storage $APPLICATION_VMAX3_NATIVEGUID2
        run cos allow $APP_VPOOL_VMAX3_HA block $TENANT

        # Create CoS for VMAX2/VNX + vplex distributed
        run cos create block $APP_VPOOL_VMAX3_VPLEX_DIST \
            --description 'Consistency-Group-Block-VMAX3-VPLEX-dist-CoS' false   \
                            --protocols FC        \
                            --numpaths 2               \
                            --max_snapshots 10         \
                            --provisionType 'Thin'     \
                            --neighborhoods $APP_VARRAY1        \
                            --highavailability vplex_distributed \
                            --haNeighborhood $APP_VARRAY2 \
                            --haCos $APP_VPOOL_VMAX3_HA \
                            --multiVolumeConsistency

        run cos update block $APP_VPOOL_VMAX3_VPLEX_DIST --storage $APPLICATION_VMAX3_NATIVEGUID1
        run cos allow $APP_VPOOL_VMAX3_VPLEX_DIST block $TENANT

    fi
}

application_setup()
{
    secho "application_setup()"

    # recoverpoint setup is used for application tests so we
    # override some of the RP options to cover application use cases

    RP_TESTS=1
    RPVPLEX_TESTS=1
    RPMP_TESTS=1
    RPXIO_TESTS=0
    RP_CDP=1
    RP_CRR=1

    if [ "$APP_QUICK_PARAM" = "quick" ]; then
        secho 'Application Simulator setup'
        APPLICATION_VNX1_NATIVEGUID=$APP_SIMULATOR_VNX_NATIVEGUID
        APPLICATION_VMAX2_NATIVEGUID1=$APP_SIMULATOR_VMAX2_NATIVEGUID1
        APPLICATION_VMAX2_NATIVEGUID2=$APP_SIMULATOR_VMAX2_NATIVEGUID2
        APPLICATION_VMAX2_NATIVEGUID3=$APP_SIMULATOR_VMAX2_NATIVEGUID3
        APPLICATION_VMAX3_NATIVEGUID1=$APP_SIMULATOR_VMAX3_NATIVEGUID1
        APPLICATION_VMAX3_NATIVEGUID2=$APP_SIMULATOR_VMAX3_NATIVEGUID2
        recoverpoint_simulator_varsetup
        recoverpoint_simulator_setup

        # applicaiton vpool setup
        application_sim_vpool_setup
    else
        secho 'Application real hardware setup'

        RECOVERPOINT_SMIS_PROVIDER_B=$APPLICATION_VMAX2_SMIS_PROVIDER
        RECOVERPOINT_SMIS_PROVIDER_B_IP=$APPLICATION_VMAX2_SMIS_PROVIDER_IP
        RECOVERPOINT_SMIS_PROVIDER_B_PORT=$SMIS_SSL_PORT
        RECOVERPOINT_STORAGE_ARRAY_B_GUID=$APPLICATION_VMAX2_STORAGE_ARRAY_GUID

        RECOVERPOINT_SMIS_PROVIDER_C=$APPLICATION_VMAX3_SMIS_PROVIDER
        RECOVERPOINT_SMIS_PROVIDER_C_IP=$APPLICATION_VMAX3_SMIS_PROVIDER_IP
        RECOVERPOINT_SMIS_PROVIDER_C_PORT=$SMIS_SSL_PORT
        RECOVERPOINT_STORAGE_ARRAY_C_GUID=$APPLICATION_VMAX3_STORAGE_ARRAY_GUID

        APPLICATION_VNX1_NATIVEGUID=$RECOVERPOINT_STORAGE_ARRAY_A_GUID
        APPLICATION_VMAX2_NATIVEGUID=$RECOVERPOINT_STORAGE_ARRAY_B_GUID
        APPLICATION_VMAX3_NATIVEGUID=$RECOVERPOINT_STORAGE_ARRAY_C_GUID
        recoverpoint_common_setup

        # applicaiton vpool setup
        application_vpool_setup
    fi
}

application_crud_test()
{
    secho "application_crud_test()"

    appname=sanityapp-crud_test-${RANDOM}

    # test and verify create volume group
    run volumegroup create ${appname} descrip COPY
    run volumegroup show ${appname}
    run volumegroup verify ${appname} description descrip
    #run volume group ${appname} roles COPY

    # test and verify list volume group
    run volumegroup list

    # test and verify volume group update
    newappname=${appname}new
    run volumegroup update ${appname} ${newappname} newdescription
    run volumegroup show ${newappname}
    run volumegroup verify ${newappname} description newdescription

    # test and verify volume group delete
    run volumegroup delete ${newappname}
    fail volumegroup show ${newappname}
}

application_createvols()
{
    secho "application_createvols()"

    # create volumes in cg1
    run blockconsistencygroup create ${PROJECT} ${VG_CGNAME1}
    a=`blockconsistencygroup show ${VG_CGNAME1} | grep '"id":' | grep BlockConsistencyGroup | awk '{print $2}'`
    b=${a:1}
    VG_CGID1=${b%'"'*}
    secho cgid1 is ${VG_CGID1}

    VG_VOLUME1=${basename}-cg1
    run volume create $VG_VOLUME1 $PROJECT $VG_VARRAY_CG $VG_VPOOL_CG $BLK_SIZE --thinVolume true --consistencyGroup ${VG_CGNAME1} --count 3
    VG_CG1VOL1="${VG_VOLUME1}-1"
    VG_CG1VOL2="${VG_VOLUME1}-2"
    VG_CG1VOL3="${VG_VOLUME1}-3"
    VG_CG1VOL_GRP="${PROJECT}/${VG_CG1VOL1},${PROJECT}/${VG_CG1VOL2},${PROJECT}/${VG_CG1VOL3}"
    VG_CG1VOLID1=`volume list ${PROJECT} | grep ${VG_CG1VOL1} | grep -v TARGET | awk '{print $7}'`
    VG_CG1VOLID2=`volume list ${PROJECT} | grep ${VG_CG1VOL2} | grep -v TARGET | awk '{print $7}'`
    VG_CG1VOLID3=`volume list ${PROJECT} | grep ${VG_CG1VOL3} | grep -v TARGET | awk '{print $7}'`
    secho cg1volid1 is ${VG_CG1VOLID1}
    secho cg1volid2 is ${VG_CG1VOLID2}
    secho cg1volid3 is ${VG_CG1VOLID3}

    # create volumes in cg2
    run blockconsistencygroup create ${PROJECT} ${VG_CGNAME2} --noarrayconsistency
    a=`blockconsistencygroup show ${VG_CGNAME2} | grep '"id":' | grep BlockConsistencyGroup | awk '{print $2}'`
    b=${a:1}
    VG_CGID2=${b%'"'*}
    secho cgid2 is ${VG_CGID2}

    VG_VOLUME2=${basename}-cg2
    run volume create $VG_VOLUME2 $PROJECT $VG_VARRAY_CG $VG_VPOOL_CG $BLK_SIZE --thinVolume true --consistencyGroup ${VG_CGNAME2} --count 3
    VG_CG2VOL1="${VG_VOLUME2}-1"
    VG_CG2VOL2="${VG_VOLUME2}-2"
    VG_CG2VOL3="${VG_VOLUME2}-3"
    VG_CG2VOL_GRP="${PROJECT}/${VG_CG2VOL1},${PROJECT}/${VG_CG2VOL2},${PROJECT}/${VG_CG2VOL3}"
    VG_CG2VOLID1=`volume list ${PROJECT} | grep ${VG_CG2VOL1} | grep -v TARGET | awk '{print $7}'`
    VG_CG2VOLID2=`volume list ${PROJECT} | grep ${VG_CG2VOL2} | grep -v TARGET | awk '{print $7}'`
    VG_CG2VOLID3=`volume list ${PROJECT} | grep ${VG_CG2VOL3} | grep -v TARGET | awk '{print $7}'`
    secho cg2volid1 is ${VG_CG2VOLID1}
    secho cg2volid2 is ${VG_CG2VOLID2}
    secho cg2volid3 is ${VG_CG2VOLID3}

    run volumegroup create ${VG_COPYVG1} copy1 COPY
}

# deletes volumes and CG used in application tests
application_deleteall()
{
    secho "application_deleteall()"
    run volume delete $VG_CG1VOL_GRP,$VG_CG2VOL_GRP --wait
    run blockconsistencygroup delete ${VG_CGNAME1}
    run blockconsistencygroup delete ${VG_CGNAME2}
    run volumegroup delete ${VG_COPYVG1}
}

# adds and removes volumes from a COPY volume group
application_add_remove_volume_tests()
{
    secho "application_add_remove_volume_tests()" 

    # add 3 volumes from same CG to application
    run volumegroup add-volumes ${VG_COPYVG1} ${VG_CG1VOLID1},${VG_CG1VOLID2},${VG_CG1VOLID3} $app_test_subgroup1a

    # show volumes in application
    run volumegroup verify-volume ${VG_COPYVG1} ${VG_CG1VOLID1}
    run volumegroup verify-volume ${VG_COPYVG1} ${VG_CG1VOLID2}
    run volumegroup verify-volume ${VG_COPYVG1} ${VG_CG1VOLID3}

    # remove
    run volumegroup remove-volumes ${VG_COPYVG1} ${VG_CG1VOLID1},${VG_CG1VOLID2},${VG_CG1VOLID3}

    fail volumegroup verify-volume ${VG_COPYVG1} ${VG_CG1VOLID1}
    fail volumegroup verify-volume ${VG_COPYVG1} ${VG_CG1VOLID2}
    fail volumegroup verify-volume ${VG_COPYVG1} ${VG_CG1VOLID3}

    # add back same 3 volumes from same CG plus 3 new volumes from a second C to application
    run volumegroup add-volumes ${VG_COPYVG1} ${VG_CG1VOLID1},${VG_CG1VOLID2},${VG_CG1VOLID3} $app_test_subgroup1a
    run volumegroup add-volumes ${VG_COPYVG1} ${VG_CG2VOLID1},${VG_CG2VOLID2},${VG_CG2VOLID3} $app_test_subgroup2a

    # show volumes in application
    run volumegroup verify-volume ${VG_COPYVG1} ${VG_CG1VOLID1}
    run volumegroup verify-volume ${VG_COPYVG1} ${VG_CG1VOLID2}
    run volumegroup verify-volume ${VG_COPYVG1} ${VG_CG1VOLID3}
    run volumegroup verify-volume ${VG_COPYVG1} ${VG_CG2VOLID1}
    run volumegroup verify-volume ${VG_COPYVG1} ${VG_CG2VOLID2}
    run volumegroup verify-volume ${VG_COPYVG1} ${VG_CG2VOLID3}

    # provision a new volume to CG1; volume should automatically be added to the applicaiton
    VG_CG1VOL4=$VG_VOLUME1-4
    run volume create $VG_CG1VOL4 $PROJECT $VG_VARRAY_CG $VG_VPOOL_CG $BLK_SIZE --thinVolume true --consistencyGroup ${VG_CGNAME1}
    VG_CG1VOLID4=`volume list ${PROJECT} | grep ${VG_CG1VOL4} | grep -v TARGET | awk '{print $7}'`
    VG_CG1VOL_GRP="${VG_CG1VOL_GRP},${PROJECT}/${VG_CG1VOL4}"
    secho cg1volid4 is ${VG_CG1VOLID4}
    # block (non0RP, non-vplex) volumes are automatically added to the application, RP and vplex volumes are not
    if [ $APP_RUNNING_BLOCK_TEST -eq 1 ]; then
        run volumegroup verify-volume ${VG_COPYVG1} ${VG_CG1VOLID4}
    else
        fail volumegroup verify-volume ${VG_COPYVG1} ${VG_CG1VOLID4}
        run volumegroup add-volumes ${VG_COPYVG1} ${VG_CG1VOLID4} $app_test_subgroup1a
        run volumegroup verify-volume ${VG_COPYVG1} ${VG_CG1VOLID4}
    fi

    run volumegroup remove-volumes ${VG_COPYVG1} ${VG_CG1VOLID1},${VG_CG1VOLID2},${VG_CG1VOLID3},${VG_CG1VOLID4}
    run volumegroup remove-volumes ${VG_COPYVG1} ${VG_CG2VOLID1},${VG_CG2VOLID2},${VG_CG2VOLID3}

}

application_copy_tests()
{
    secho "application_copy_tests()"

    run volumegroup add-volumes ${VG_COPYVG1} ${VG_CG1VOLID1},${VG_CG1VOLID2},${VG_CG1VOLID3},${VG_CG1VOLID4} $app_test_subgroup1a
    run volumegroup verify-volume ${VG_COPYVG1} ${VG_CG1VOLID1}
    run volumegroup verify-volume ${VG_COPYVG1} ${VG_CG1VOLID2}
    run volumegroup verify-volume ${VG_COPYVG1} ${VG_CG1VOLID3}
    run volumegroup verify-volume ${VG_COPYVG1} ${VG_CG1VOLID4}

    run volumegroup add-volumes ${VG_COPYVG1} ${VG_CG2VOLID1},${VG_CG2VOLID2},${VG_CG2VOLID3} $app_test_subgroup2a
    run volumegroup verify-volume ${VG_COPYVG1} ${VG_CG2VOLID1}
    run volumegroup verify-volume ${VG_COPYVG1} ${VG_CG2VOLID2}
    run volumegroup verify-volume ${VG_COPYVG1} ${VG_CG2VOLID3}

    if [ $APPLICATION_CLONE_SUPPORTED -eq 1 -a $APPLICATION_TEST_CLONES -eq 1 ]; then
        APPLICATION_RESYNC_COPY_SUPPORTED=1
        APPLICATION_DETACH_COPY_SUPPORTED=1
        APPLICATION_LINK_COPY_SUPPORTED=0
        application_full_copy_tests clone
    fi

    if [ $APPLICATION_SNAPSHOT_SUPPORTED -eq 1 -a $APPLICATION_TEST_SNAPSHOTS -eq 1 ]; then
        APPLICATION_RESYNC_COPY_SUPPORTED=$APPLICATION_RESYNC_SNAP_SUPPORTED
        APPLICATION_DETACH_COPY_SUPPORTED=0
        APPLICATION_EXPOSE_COPY_SUPPORTED=$APPLICATION_EXPOSE_SNAP_SUPPORTED
        APPLICATION_LINK_COPY_SUPPORTED=$APPLICATION_LINK_SNAP_SUPPORTED
        application_full_copy_tests $APPLICATION_SNAP_TYPE
    fi

    run volumegroup remove-volumes ${VG_COPYVG1} ${VG_CG1VOLID1},${VG_CG1VOLID2},${VG_CG1VOLID3},${VG_CG1VOLID4}
    fail volumegroup verify-volume ${VG_COPYVG1} ${VG_CG1VOLID1}
    fail volumegroup verify-volume ${VG_COPYVG1} ${VG_CG1VOLID2}
    fail volumegroup verify-volume ${VG_COPYVG1} ${VG_CG1VOLID3}
    fail volumegroup verify-volume ${VG_COPYVG1} ${VG_CG1VOLID4}

    run volumegroup remove-volumes ${VG_COPYVG1} ${VG_CG2VOLID1},${VG_CG2VOLID2},${VG_CG2VOLID3}
    fail volumegroup verify-volume ${VG_COPYVG1} ${VG_CG2VOLID1}
    fail volumegroup verify-volume ${VG_COPYVG1} ${VG_CG2VOLID2}
    fail volumegroup verify-volume ${VG_COPYVG1} ${VG_CG2VOLID3}

}

application_multi_sub_group_copy_tests()
{
    secho "application_multi_sub_group_copy_tests()"

    run volumegroup add-volumes ${VG_COPYVG1} ${VG_CG1VOLID1},${VG_CG1VOLID2} $app_test_subgroup1a
    run volumegroup verify-volume ${VG_COPYVG1} ${VG_CG1VOLID1}
    run volumegroup verify-volume ${VG_COPYVG1} ${VG_CG1VOLID2}

    run volumegroup add-volumes ${VG_COPYVG1} ${VG_CG1VOLID3},${VG_CG1VOLID4} $app_test_subgroup1b
    run volumegroup verify-volume ${VG_COPYVG1} ${VG_CG1VOLID3}
    run volumegroup verify-volume ${VG_COPYVG1} ${VG_CG1VOLID4}

    application_full_copy_tests

    # TODO add snapshot and snapshto session tests

    run volumegroup remove-volumes ${VG_COPYVG1} ${VG_CG1VOLID1},${VG_CG1VOLID2}
    fail volumegroup verify-volume ${VG_COPYVG1} ${VG_CG1VOLID1}
    fail volumegroup verify-volume ${VG_COPYVG1} ${VG_CG1VOLID2}

    run volumegroup remove-volumes ${VG_COPYVG1} ${VG_CG1VOLID3},${VG_CG1VOLID4}
    fail volumegroup verify-volume ${VG_COPYVG1} ${VG_CG1VOLID3}
    fail volumegroup verify-volume ${VG_COPYVG1} ${VG_CG1VOLID4}
}


application_export_snapshot_tests()
{
    hostseed="${RANDOM}"
    apphostname="apphost-${hostseed}"
    appexportname="host${hostseed}.sanity.com"
    volspec=$1
    copysetname=$2

    # get snapshot names
    snapshotname=`blocksnapshot list ${volspec} | grep ${copysetname} | awk '{print $1}'`

    snapspec=$volspec/$snapshotname

    PWWN1=`pwwn 6F`;
    NWWN1=`pwwn 7F`;
    run hosts create ${apphostname} $TENANT Windows ${apphostname} --port 8111 --username $RP_HOST_USER --password '${RP_HOST_PW}' --osversion 1.0
    run initiator create ${apphostname} FC $PWWN1 --node $NWWN1
    run transportzone add ${VG_VARRAY_CG}/${SIMULATOR_VSAN_11} $PWWN1

    sleep 20

    run export_group create $PROJECT ${appexportname} ${VG_VARRAY_CG} --volspec ${snapspec} --inits "${apphostname}/$PWWN1"

    # delete copy should fail because one of the snapshots is exported to a host
    fail volumegroup delete-copy $copy_type $VG_COPYVG1 $VG_FCNAME

    run export_group delete $PROJECT/${appexportname}
    run hosts delete ${apphostname}

}

application_full_copy_tests()
{
    secho "application_full_copy_tests()"

    copy_type=${1}

    VG_FCNAME="app1${copy_type}"
    VG_FCNAME2="app2${copy_type}"
    VG_FCNAME3="app3${copy_type}"

    # create the full copy for the entire application
    run volumegroup create-copy $copy_type $VG_COPYVG1 $VG_FCNAME


    # verify the full copy exists
    run volumegroup verify-copy-set $copy_type $VG_COPYVG1 $VG_FCNAME

    # resync (no way to verify)
    if [[ $APPLICATION_RESYNC_COPY_SUPPORTED -eq 1 ]]; then
       run volumegroup resync-copy $copy_type $VG_COPYVG1 $VG_FCNAME
    fi

    # expose 
    if [[ $APPLICATION_EXPOSE_COPY_SUPPORTED -eq 1 ]]; then
       run volumegroup expose-copy $copy_type $VG_COPYVG1 $VG_FCNAME
       VG_EXPOSEVOLS=`volume list ${PROJECT} | grep ${VG_FCNAME} | awk '{print $7}' | paste -s -d ,`
       volume delete $VG_EXPOSEVOLS --wait
    fi

    # run restore (no way to verify)
    if [[ $APP_RESTORE_SUPPORTED -eq 1 ]]; then
        run volumegroup restore-copy $copy_type $VG_COPYVG1 $VG_FCNAME
    fi

    # detach the copies
    if [[ $APPLICATION_DETACH_COPY_SUPPORTED -eq 1 ]]; then

        # get the full copy volumes so they can be deleted after detach
        VG_FCVOLUMES=`volumegroup list-copy-volumes $copy_type $VG_COPYVG1`

        # detach source from copy
        run volumegroup detach-copy $copy_type $VG_COPYVG1 $VG_FCNAME
        fail volumegroup verify-copy-set $copy_type $VG_COPYVG1 $VG_FCNAME

        # delete the full copy volumes
        run volume delete $VG_FCVOLUMES --wait

    else
        if [[ $APPLICATION_LINK_COPY_SUPPORTED -eq 1 ]]; then
            target_name="tgt-${VG_FCNAME}"
            run volumegroup link-copy $copy_type $VG_COPYVG1 $VG_FCNAME 1 $target_name nocopy
            application_export_snapshot_tests $PROJECT/$VG_CG1VOL1 $target_name
            if [[ $APP_RESTORE_SUPPORTED -eq 1 ]]; then
                run volumegroup restore-copy $copy_type $VG_COPYVG1 $VG_FCNAME
            fi
            run volumegroup relink-copy $copy_type $VG_COPYVG1 $VG_FCNAME ${target_name}-1
            run volumegroup unlink-copy $copy_type $VG_COPYVG1 $VG_FCNAME ${target_name}-1
        else
            application_export_snapshot_tests $PROJECT/$VG_CG1VOL1 $VG_FCNAME
        fi
        run volumegroup delete-copy $copy_type $VG_COPYVG1 $VG_FCNAME
        fail volumegroup verify-copy-set $copy_type $VG_COPYVG1 $VG_FCNAME
    fi


    if [ $APPLICATION_CREATE_INACTIVE_SUPPORTED -eq 1 ]; then
        # activate copies (exclude vplex COP-21804)
        run volumegroup create-copy-inactive $copy_type $VG_COPYVG1 $VG_FCNAME2
        run volumegroup verify-copy-set $copy_type $VG_COPYVG1 $VG_FCNAME2
        run volumegroup activate-copy $copy_type $VG_COPYVG1 $VG_FCNAME2


        if [ $APPLICATION_DETACH_COPY_SUPPORTED -eq 1 ]; then

            # get the full copy volumes so they can be deleted after detach
            VG_FCVOLUMES=`volumegroup list-copy-volumes $copy_type $VG_COPYVG1`

            # detach the copies
            run volumegroup detach-copy $copy_type $VG_COPYVG1 $VG_FCNAME2
            fail volumegroup verify-copy-set $copy_type $VG_COPYVG1 $VG_FCNAME2

            # delete the full copy volumes
            run volume delete $VG_FCVOLUMES --wait
        else
            run volumegroup delete-copy $copy_type $VG_COPYVG1 $VG_FCNAME2
            fail volumegroup verify-copy-set $copy_type $VG_COPYVG1 $VG_FCNAME2
        fi
    fi
}

application_tests()
{

    # TODO add more add/remove volume tests fo rRP and vplex that split u CGs into multiple sub groups
    # TODO add teste for all partial operations
    # TODO create clone inactive
    # TODO add RP tests
    secho "Executing application sanity tests " `date`

    unique=${RANDOM}
    basename=appsanity${unique}
    VG_CGNAME1="cg1${basename}"
    VG_CGNAME2="cg2${basename}"
    VG_COPYVG1="app1-${basename}"
    VG_COPYVG2="app2-${basename}"

    VG_VOLUME1="${VG_CGNAME1}-vol"
    VG_CG1VOL1="${VG_VOLUME1}-1"
    VG_CG1VOL2="${VG_VOLUME1}-2"
    VG_CG1VOL3="${VG_VOLUME1}-3"
    VG_CG1VOL4="${VG_VOLUME1}-4"

    VG_VOLUME2="${VG_CGNAME2}-vol"
    VG_CG2VOL1="${VG_VOLUME2}-1"
    VG_CG2VOL2="${VG_VOLUME2}-2"
    VG_CG2VOL3="${VG_VOLUME2}-3"

    APP_RUNNING_XIO_TEST=0
    APP_RESTORE_SUPPORTED=1

    if [ $APPLICATION_TEST_CRUD -eq 1 ]; then
        application_crud_test
    fi
    if [ $APPLICATION_TEST_VNX -eq 1 ]; then
        APPLICATION_CLONE_SUPPORTED=1
        APPLICATION_SNAPSHOT_SUPPORTED=0
        if [ $APPLICATION_TEST_BLOCK -eq 1 ]; then
            secho "running block vnx tests"
            VG_VPOOL_CG=$APP_VPOOL_VNX_BLOCK
            VG_VARRAY_CG=$APP_VARRAY1
            APP_RUNNING_BLOCK_TEST=1
            APPLICATION_CREATE_INACTIVE_SUPPORTED=1

            app_test_subgroup1a=""
            app_test_subgroup1b=""
            app_test_subgroup2a=""
            app_test_subgroup2b=""

            application_createvols
            application_add_remove_volume_tests
            application_copy_tests
            application_deleteall
            secho "completed running block vnx tests"
        fi
        if [ $APPLICATION_TEST_VPLEX_LOCAL -eq 1 ]; then
            secho "running block vplex+vnx tests"
            VG_VPOOL_CG=$APP_VPOOL_VNX_VPLEX_LOCAL
            VG_VARRAY_CG=$APP_VARRAY1
            APP_RUNNING_BLOCK_TEST=0
            APPLICATION_CREATE_INACTIVE_SUPPORTED=0

            app_test_subgroup1a="grpa${VG_CGNAME1}"
            app_test_subgroup1b="grpb${VG_CGNAME1}"
            app_test_subgroup2a="grpa${VG_CGNAME2}"
            app_test_subgroup2b="grpb${VG_CGNAME2}"

            application_createvols
            application_add_remove_volume_tests
            application_copy_tests
            application_deleteall
            secho "completed running block vplex+vnx tests"
        fi
    fi
    if [ $APPLICATION_TEST_VMAX2 -eq 1 ]; then
        APPLICATION_CLONE_SUPPORTED=1
        APPLICATION_SNAPSHOT_SUPPORTED=1
        APPLICATION_RESYNC_SNAP_SUPPORTED=1
        APPLICATION_LINK_SNAP_SUPPORTED=0
        APPLICATION_EXPOSE_SNAP_SUPPORTED=0
        APPLICATION_SNAP_TYPE='snapshot'
        if [ $APPLICATION_TEST_BLOCK -eq 1 ]; then
            secho "running block vmax2 tests"
            VG_VPOOL_CG=$APP_VPOOL_VMAX2_BLOCK
            VG_VARRAY_CG=$APP_VARRAY1
            APP_RUNNING_BLOCK_TEST=1
            APPLICATION_CREATE_INACTIVE_SUPPORTED=1

            app_test_subgroup1a=""
            app_test_subgroup1b=""
            app_test_subgroup2a=""
            app_test_subgroup2b=""

            application_createvols
            application_add_remove_volume_tests
            application_copy_tests
            application_deleteall
            secho "completed running block vmax2 tests"
        fi
        if [ $APPLICATION_TEST_VPLEX_LOCAL -eq 1 ]; then
            secho "running block vplex+vmax2 tests"
            VG_VPOOL_CG=$APP_VPOOL_VMAX2_VPLEX_LOCAL
            VG_VARRAY_CG=$APP_VARRAY1
            APP_RUNNING_BLOCK_TEST=0
            APPLICATION_CREATE_INACTIVE_SUPPORTED=0
            APPLICATION_EXPOSE_SNAP_SUPPORTED=1

            app_test_subgroup1a="${VG_CGNAME1}a"
            app_test_subgroup1b="${VG_CGNAME1}b"
            app_test_subgroup2a="${VG_CGNAME2}a"
            app_test_subgroup2b="${VG_CGNAME2}b"

            application_createvols 
            application_add_remove_volume_tests 
            application_copy_tests 
            application_deleteall
            secho "completed running block vplex+vmax2 tests"
        fi
        if [ $APPLICATION_TEST_VPLEX_DIST -eq 1 ]; then
            secho "running vplex+vmax2 dist tests"
            VG_VPOOL_CG=$APP_VPOOL_VMAX2_VPLEX_DIST
            VG_VARRAY_CG=$APP_VARRAY1
            APP_RUNNING_BLOCK_TEST=0
            APPLICATION_CREATE_INACTIVE_SUPPORTED=0

            app_test_subgroup1a="${VG_CGNAME1}a"
            app_test_subgroup1b="${VG_CGNAME1}b"
            app_test_subgroup2a="${VG_CGNAME2}a"
            app_test_subgroup2b="${VG_CGNAME2}b"

            application_createvols
            application_add_remove_volume_tests
            application_copy_tests
            application_deleteall
            secho "completed running vplex+vmax2 dist tests"
        fi
        if [ $APPLICATION_TEST_RP_BLOCK -eq 1 ]; then
            secho "running block rp tests"
            VG_VPOOL_CG=$APP_VPOOL_VMAX2_BLOCK_RPSRC
            VG_VARRAY_CG=$APP_VARRAY1
            APP_RUNNING_BLOCK_TEST=0
            APPLICATION_CREATE_INACTIVE_SUPPORTED=0
            # TODO restore is supported for RP but there's some simulator issue that needs to be resolved
            # issue is RP never sees the CG come back to active after adding rsets backafter retore
            APP_RESTORE_SUPPORTED=0

            app_test_subgroup1a="${VG_CGNAME1}a"
            app_test_subgroup1b="${VG_CGNAME1}b"
            app_test_subgroup2a="${VG_CGNAME2}a"
            app_test_subgroup2b="${VG_CGNAME2}b"

            application_createvols
            application_add_remove_volume_tests
            application_copy_tests
            application_deleteall
            secho "completed running block rp tests"
            APP_RESTORE_SUPPORTED=1
        fi
        if [ $APPLICATION_TEST_RP_VPLEX_LOCAL -eq 1 ]; then
            secho "running rp vplex local tests"
            VG_VPOOL_CG=$APP_VPOOL_VMAX2_RP_VPLEX_LOCAL
            VG_VARRAY_CG=$APP_VARRAY1
            APP_RUNNING_BLOCK_TEST=0
            APPLICATION_CREATE_INACTIVE_SUPPORTED=0
            # TODO restore is supported for RP but there's some simulator issue that needs to be resolved
            # issue is RP never sees the CG come back to active after adding rsets backafter retore
            APP_RESTORE_SUPPORTED=0

            app_test_subgroup1a="${VG_CGNAME1}a"
            app_test_subgroup1b="${VG_CGNAME1}b"
            app_test_subgroup2a="${VG_CGNAME2}a"
            app_test_subgroup2b="${VG_CGNAME2}b"

            application_createvols
            application_add_remove_volume_tests
            application_copy_tests
            application_deleteall
            secho "completed running rp vplex local tests"
            APP_RESTORE_SUPPORTED=1
        fi
        if [ $APPLICATION_TEST_RP_METROPOINT -eq 1 ]; then
            secho "running metropoint vplex  tests"
            VG_VPOOL_CG=$APP_VPOOL_VMAX2_METROPOINT
            VG_VARRAY_CG=$APP_VARRAY1
            APP_RUNNING_BLOCK_TEST=0
            APPLICATION_CREATE_INACTIVE_SUPPORTED=0
            # TODO restore is supported for RP but there's some simulator issue that needs to be resolved
            # issue is RP never sees the CG come back to active after adding rsets backafter retore
            APP_RESTORE_SUPPORTED=0

            app_test_subgroup1a="${VG_CGNAME1}a"
            app_test_subgroup1b="${VG_CGNAME1}b"
            app_test_subgroup2a="${VG_CGNAME2}a"
            app_test_subgroup2b="${VG_CGNAME2}b"

            application_createvols
            application_add_remove_volume_tests
            application_copy_tests
            application_deleteall
            secho "completed running metropoint tests"
            APP_RESTORE_SUPPORTED=1
        fi
    fi
    if [ $APPLICATION_TEST_VMAX3 -eq 1 ]; then
        APPLICATION_CLONE_SUPPORTED=1
        # TODO add snap session support
        APPLICATION_SNAPSHOT_SUPPORTED=1
        APPLICATION_RESYNC_SNAP_SUPPORTED=0
        APPLICATION_LINK_SNAP_SUPPORTED=1
        APPLICATION_EXPOSE_SNAP_SUPPORTED=0
        APPLICATION_CREATE_INACTIVE_SUPPORTED=0
        APPLICATION_SNAP_TYPE='snapshotsession'
        if [ $APPLICATION_TEST_BLOCK -eq 1 ]; then
            secho "running block vmax3 tests"
            VG_VPOOL_CG=$APP_VPOOL_VMAX3_BLOCK
            VG_VARRAY_CG=$APP_VARRAY1
            APP_RUNNING_BLOCK_TEST=1

            app_test_subgroup1a=""
            app_test_subgroup1b=""
            app_test_subgroup2a=""
            app_test_subgroup2b=""

            application_createvols
            application_add_remove_volume_tests
            application_copy_tests
            application_deleteall
            secho "completed running block vmax3 tests"
        fi
        if [ $APPLICATION_TEST_VPLEX_LOCAL -eq 1 ]; then
            secho "running block vplex+vmax3 local tests"
            VG_VPOOL_CG=$APP_VPOOL_VMAX3_VPLEX_LOCAL
            VG_VARRAY_CG=$APP_VARRAY1
            APP_RUNNING_BLOCK_TEST=0

            app_test_subgroup1a="${VG_CGNAME1}a"
            app_test_subgroup1b="${VG_CGNAME1}b"
            app_test_subgroup2a="${VG_CGNAME2}a"
            app_test_subgroup2b="${VG_CGNAME2}b"

            application_createvols
            application_add_remove_volume_tests
            application_copy_tests
            application_deleteall
            secho "completed running block vplex+vmax3 local tests"
        fi
        if [ $APPLICATION_TEST_VPLEX_LOCAL -eq 1 ]; then
            secho "running block vplex+vmax3 dist tests"
            VG_VPOOL_CG=$APP_VPOOL_VMAX3_VPLEX_DIST
            VG_VARRAY_CG=$APP_VARRAY1
            APP_RUNNING_BLOCK_TEST=0

            app_test_subgroup1a="${VG_CGNAME1}a"
            app_test_subgroup1b="${VG_CGNAME1}b"
            app_test_subgroup2a="${VG_CGNAME2}a"
            app_test_subgroup2b="${VG_CGNAME2}b"

            application_createvols
            application_add_remove_volume_tests
            application_copy_tests
            application_deleteall
            secho "completed running block vplex+vmax3 dist tests"
        fi
    fi

    if [ $APPLICATION_TEST_XIO -eq 1 ]; then
        APPLICATION_CLONE_SUPPORTED=0
        APPLICATION_SNAPSHOT_SUPPORTED=1
        APPLICATION_CREATE_INACTIVE_SUPPORTED=0
        APPLICATION_SNAP_TYPE='snapshot'
        if [ $APPLICATION_TEST_BLOCK -eq 1 ]; then
            secho "running block xio tests"
            VG_VPOOL_CG=$APP_VPOOL_XIO_BLOCK
            VG_VARRAY_CG=$APP_VARRAY4
            APP_RUNNING_BLOCK_TEST=1

            app_test_subgroup1a=""
            app_test_subgroup1b=""
            app_test_subgroup2a=""
            app_test_subgroup2b=""

            application_createvols
            application_add_remove_volume_tests
            application_copy_tests
            application_deleteall
            secho "completed running block xio tests"
        fi
        if [ $APPLICATION_TEST_VPLEX_LOCAL -eq 1 ]; then
            secho "running block vplex+xio tests"
            VG_VPOOL_CG=$APP_VPOOL_XIO_VPLEX_LOCAL
            VG_VARRAY_CG=$APP_VARRAY4
            APP_RUNNING_BLOCK_TEST=0

            app_test_subgroup1a="${VG_CGNAME1}a"
            app_test_subgroup1b="${VG_CGNAME1}b"
            app_test_subgroup2a="${VG_CGNAME2}a"
            app_test_subgroup2b="${VG_CGNAME2}b"

            application_createvols
            application_add_remove_volume_tests
            application_copy_tests
            application_deleteall
            secho "completed running block vplex+xio tests"
        fi
    fi

    secho "Completed application sanity tests " `date`
}

vplexsnap_setup()
{
    secho "Executing vplexsnap setup"
    vplex_setup
}

vplexsnap_tests()
{
    secho "Executing VPLEX snapshot export tests"

    hname=$(hostname)
    if [ $hname = "standalone" ]; then
        hname=$SHORTENED_HOST
    fi
    echo "hostname is $hname"

    localVolume=$hname-${RANDOM}-VPlexLocal1
    localSnapshot=$hname-${RANDOM}-VPlexLocalSnap
    host=$PROJECT.lss.emc.com
    hostLbl=$PROJECT
    PWWN1=10:00:00:E0:7E:00:00:0F
    WWNN1=20:00:00:E0:7E:00:00:0F
    PWWN2=10:00:00:90:FA:18:0E:99
    WWNN2=20:00:00:90:FA:18:0E:99

    secho "Creating source VPLEX local volume"
    run volume create $localVolume $PROJECT $NH cosvplexlocal $BLK_SIZE

    secho "Creating VPLEX snapshot for source"
    run blocksnapshot create $PROJECT/$localVolume $localSnapshot
    blocksnapshot list $PROJECT/$localVolume
    blocksnapshot show $PROJECT/$localVolume/$localSnapshot

    secho "Creating VPLEX volume from snapshot"
    blocksnapshot expose $PROJECT/$localVolume/$localSnapshot

    secho "Creating host"
    hosts create $hostLbl $TENANT Windows $host --port 8111 --username user --password 'password' --osversion 1.0

    secho "Creating initiators"
    initiator create $hostLbl FC $PWWN1 --node $WWNN1
    initiator create $hostLbl FC $PWWN2 --node $WWNN2

    secho "Export VPLEX volume built from snapshot"
    run export_group create $PROJECT $hname-1$host $NH --volspec "$PROJECT/$localSnapshot" --inits "$hostLbl/$PWWN1","$hostLbl/$PWWN2"

    secho "Unexport VPLEX volume built from snapshot"
    run export_group delete $PROJECT/$hname-1$host

    secho "Deleting VPLEX volume built on snapshot"
    run volume delete $PROJECT/$localSnapshot --wait

    secho "Deleting snapshot"
    run blocksnapshot delete $PROJECT/$localVolume/$localSnapshot

    secho "Deleting source VPLEX local volume"
    run volume delete $PROJECT/$localVolume --wait

    secho "Deleting Host"
    run hosts delete $hostLbl

    secho "Completed VPLEX snapshot export tests"
}

snapvx_vpool_setup()
{
    secho "Creating snapvx virtual pools"
    run cos create block $COS_VMAX3BLOCK_FC false \
	--description 'Virtual-Pool-for-VMAX3-block-FC' \
                      --protocols FC \
                      --numpaths 2 \
                      --max_snapshots 10 \
	               --system_type vmax \
                      --provisionType 'Thin' \
			 --neighborhoods $NH \
                         --multiVolumeConsistency

    run cos update block $COS_VMAX3BLOCK_FC --storage ${VMAX3_NATIVEGUID}
}

snapvx_setup_once()
{
    # Do this only once
    secho "Checking for the existance of the VMAX3 provider"
    smisprovider show $VMAX3_SMIS_DEV &> /dev/null && return $?

    # Create provider for VMAX3 array.
    secho "Creating VMAX3 provider"
    run smisprovider create $VMAX3_SMIS_DEV $VMAX3_SMIS_IP $VMAX3_SMIS_PORT $SMIS_USER "$SMIS_PASSWD" $VMAX3_SMIS_SSL

    # Discover the vmax3 array.
    secho "Discovering VMAX3 array"
    run storagedevice discover_all --ignore_error

    # Set up virtual pools
    secho "Setting up virtual pools"
    snapvx_vpool_setup

    # Assign the storage pools of the VMAX3 array to our virtual array.
    secho "Assiging storage pools to virtual array"
    run storagepool update $VMAX3_NATIVEGUID --nhadd $NH
 
    # Assign storage ports if necessary.     
    if [ $DISCOVER_SAN -eq 0 ]; then
        secho "Updating storage port networks"
        for porta in ${VMAX3_PORTS_A}
            do
	         run storageport update $VMAX3_NATIVEGUID FC --tzone $FCTZ_A --group ${porta}
            done
    fi

    # Update virtual pool to use mathcing pools on the system.
    secho "Assiging system storage pools to virtual pool"
    run cos update block $COS_VMAX3BLOCK_FC --storage $VMAX3_NATIVEGUID
}


snapvx_setup()
{
    secho "Executing snapvx setup"
    if [ "$SNAPVX_QUICK_PARAM" = "quick" ]; then
        echo "Test with simulator"
        VMAX3_SMIS_IP=${VMAX3_SIMULATOR_SMIS_IP}
        VMAX3_SMIS_PORT=${VMAX3_SIMULATOR_SMIS_PORT}
        VMAX3_SMIS_SSL=${VMAX3_SIMULATOR_SMIS_SSL}
        VMAX3_SN=${VMAX3_SIMULATOR_SN}
        VMAX3_NATIVEGUID=${VMAX3_SIMULATOR_NATIVE_GUID}
        VMAX3_PORTS_A=${VMAX3_SIMULATOR_PORTS_A}
    fi

    snapvx_setup_once
    cos allow $COS_VMAX3BLOCK_FC block $TENANT
}

snapvx_tests()
{
   snapvx_group_tests
   snapvx_single_tests
}

snapvx_group_tests()
{
    secho "Executing snapvx group tests"
    CG_NAME=SnapVxCG${RANDOM}
    VOLUME_NAME=SnapVxTest-CG-${RANDOM}
    SNAP_SESSION_NAME1=SnapSession-CG-${RANDOM}
    SNAP_SESSION_NAME2=SnapSession-CG-${RANDOM}
    SNAP_SESSION_NAME3=SnapSession-CG-${RANDOM}
    SNAP_SESSION_TGT_NAME1=SnapSessionTgt-CG-${RANDOM}
    SNAP_SESSION_TGT_NAME2=SnapSessionTgt-CG-${RANDOM}
    SNAP_SESSION_TGT_NAME3=SnapSessionTgt-CG-${RANDOM}

    run blockconsistencygroup create $PROJECT $CG_NAME

    # Create a VMAX3 volumes in CG to serve as the snapshot session sources.
    secho "Creating VMAX3 source volume"
    run volume create $VOLUME_NAME-1 $PROJECT $NH $COS_VMAX3BLOCK_FC $BLK_SIZE --consistencyGroup $CG_NAME
    run volume create $VOLUME_NAME-2 $PROJECT $NH $COS_VMAX3BLOCK_FC $BLK_SIZE --consistencyGroup $CG_NAME

    # Create snapvx sessions with no linked targets
    run blockconsistencygroup create_snapshot_session $CG_NAME $SNAP_SESSION_NAME1
    run blockconsistencygroup create_snapshot_session $CG_NAME $SNAP_SESSION_NAME2

    # Restore the snapvx session.
    secho "Restoring SnapVx session 1"
    run blockconsistencygroup restore_targets $CG_NAME/$SNAP_SESSION_NAME1

    # Link target to the snapvx session 1 in default nocopy mode.
    secho "Linking target to SnapVx session 1"
    run blockconsistencygroup link_targets $CG_NAME/$SNAP_SESSION_NAME1 1 $SNAP_SESSION_TGT_NAME1

    # Relink target to the snapvx session 2.
    secho "Relinking target to SnapVx session 2"
    run blockconsistencygroup relink_targets $CG_NAME/$SNAP_SESSION_NAME2 $CG_NAME/${SNAP_SESSION_TGT_NAME1}-1-1

    # Unlink target from the snapvx session and delete target volume.
    secho "unlinking target from SnapVx session 2"
    run blockconsistencygroup unlink_targets $CG_NAME/$SNAP_SESSION_NAME2 $CG_NAME/${SNAP_SESSION_TGT_NAME1}-1-1 --delete_target true

    # Delete the snapvx sessions.
    secho "Deleting SnapVx sessions 1 and 2"
    run blockconsistencygroup delete_snapshot_session $CG_NAME/$SNAP_SESSION_NAME1
    run blockconsistencygroup delete_snapshot_session $CG_NAME/$SNAP_SESSION_NAME2

    # Create a snapvx session with multiple linked targets in copy mode
    secho "Creating SnapVx session 3 with multiple linked targets"
    run blockconsistencygroup create_snapshot_session $CG_NAME $SNAP_SESSION_NAME3 --target_count 2 --target_name $SNAP_SESSION_TGT_NAME2 --target_copymode copy

    # Link another target to the snapvx session.
    secho "Linking another target to SnapVx session 3 in copy mode"
    run blockconsistencygroup link_targets $CG_NAME/$SNAP_SESSION_NAME3 1 $SNAP_SESSION_TGT_NAME3 --target_copymode copy

    # Restore the source from a linked target
    # COP-25623 [OPT 508268] workaournd. Remove the sleep once the issue fixed
    ssleep 120
    secho "Restoring source from linked target"
    run blockconsistencygroup restore_snapshot $CG_NAME $SNAP_SESSION_TGT_NAME3-1-1

    # Unlink target from the snapvx session and delete target volume.
    secho "Unlinking all targets from SnapVx session 3"
    run blockconsistencygroup unlink_targets $CG_NAME/$SNAP_SESSION_NAME3 $CG_NAME/$SNAP_SESSION_TGT_NAME2-1-1 --delete_target true
    run blockconsistencygroup unlink_targets $CG_NAME/$SNAP_SESSION_NAME3 $CG_NAME/$SNAP_SESSION_TGT_NAME2-2-1 --delete_target true
    run blockconsistencygroup unlink_targets $CG_NAME/$SNAP_SESSION_NAME3 $CG_NAME/$SNAP_SESSION_TGT_NAME3-1-1 --delete_target true

    # Delete snapvx sessions with no linked targets
    run blockconsistencygroup delete_snapshot_session $CG_NAME/$SNAP_SESSION_NAME3

    # Delete the source volumes
    volumes=`blockconsistencygroup list_volume_ids $CG_NAME | tail -n+2`
    for volume in $volumes
    do
        secho "Deleting the VMAX3 source volume $volume"
        run_noundo volume delete $volume --wait
    done

    secho "Deleting the VMAX3 CG"
    # Delete the snapshot session source consistency group.
    run blockconsistencygroup delete $CG_NAME
}

snapvx_single_tests()
{
    secho "Executing snapvx tests"
    VOLUME_NAME=SnapVxTest-${RANDOM}
    SNAP_SESSION_NAME1=SnapSession-${RANDOM}
    SNAP_SESSION_NAME2=SnapSession-${RANDOM}
    SNAP_SESSION_NAME3=SnapSession-${RANDOM}
    SNAP_SESSION_TGT_NAME1=SnapSessionTgt-${RANDOM}
    SNAP_SESSION_TGT_NAME2=SnapSessionTgt-${RANDOM}
    SNAP_SESSION_TGT_NAME3=SnapSessionTgt-${RANDOM}

    # Create a VMAX3 volume to serve as the snapshot session source.
    secho "Creating VMAX3 source volume"
    volume create $VOLUME_NAME $PROJECT $NH $COS_VMAX3BLOCK_FC $BLK_SIZE

    # Create snapvx sessions with no linked targets.
    secho "Creating SnapVx sessions 1 and 2 with no linked targets"
    snapshotsession create $PROJECT/$VOLUME_NAME $SNAP_SESSION_NAME1
    snapshotsession show $PROJECT/$VOLUME_NAME/$SNAP_SESSION_NAME1
    snapshotsession create $PROJECT/$VOLUME_NAME $SNAP_SESSION_NAME2
    snapshotsession show $PROJECT/$VOLUME_NAME/$SNAP_SESSION_NAME2

    # Restore the snapvx session.
    secho "Restoring SnapVx session 1"
    snapshotsession restore $PROJECT/$VOLUME_NAME/$SNAP_SESSION_NAME1

    # Link target to the snapvx session 1 in default nocopy mode.
    secho "Linking target to SnapVx session 1"
    snapshotsession link_targets $PROJECT/$VOLUME_NAME/$SNAP_SESSION_NAME1 1 $SNAP_SESSION_TGT_NAME1

    # Relink target to the snapvx session 2.
    secho "Relinking target to SnapVx session 2"
    snapshotsession relink_target $PROJECT/$VOLUME_NAME/$SNAP_SESSION_NAME2 $PROJECT/$VOLUME_NAME/$SNAP_SESSION_TGT_NAME1-1

    # Delete the snapvx sessions.
    secho "Deleting SnapVx session 1"
    snapshotsession delete $PROJECT/$VOLUME_NAME/$SNAP_SESSION_NAME1

    # delete the targetless session first before unlinking (OPT 498174)

    # Unlink target from the snapvx session and delete target volume.
    secho "unlinking target from SnapVx session 2"
    snapshotsession unlink_target $PROJECT/$VOLUME_NAME/$SNAP_SESSION_NAME2 $PROJECT/$VOLUME_NAME/$SNAP_SESSION_TGT_NAME1-1 --delete_target true

    # Delete the snapvx sessions.
    secho "Deleting SnapVx session 2"
    snapshotsession delete $PROJECT/$VOLUME_NAME/$SNAP_SESSION_NAME2

    # Create a snapvx session with multiple linked targets in copy mode
    secho "Creating SnapVx session 3 with multiple linked targets"
    snapshotsession create $PROJECT/$VOLUME_NAME $SNAP_SESSION_NAME3 --target_count 2 --target_name $SNAP_SESSION_TGT_NAME2 --target_copymode copy

    # Link another target to the snapvx session.
    secho "Linking another target to SnapVx session 3 in copy mode"
    snapshotsession link_targets $PROJECT/$VOLUME_NAME/$SNAP_SESSION_NAME3 1 $SNAP_SESSION_TGT_NAME3 --target_copymode copy

    # Restore the source from a linked target
    secho "Restoring source from linked target"
    blocksnapshot restore $PROJECT/$VOLUME_NAME/$SNAP_SESSION_TGT_NAME3-1

    # Unlink target from the snapvx session and delete target volume.
    secho "Unlinking all targets from SnapVx session 3"
    snapshotsession unlink_target $PROJECT/$VOLUME_NAME/$SNAP_SESSION_NAME3 $PROJECT/$VOLUME_NAME/$SNAP_SESSION_TGT_NAME2-1 --delete_target true
    snapshotsession unlink_target $PROJECT/$VOLUME_NAME/$SNAP_SESSION_NAME3 $PROJECT/$VOLUME_NAME/$SNAP_SESSION_TGT_NAME2-2 --delete_target true
    snapshotsession unlink_target $PROJECT/$VOLUME_NAME/$SNAP_SESSION_NAME3 $PROJECT/$VOLUME_NAME/$SNAP_SESSION_TGT_NAME3-1 --delete_target true

    # Delete the snapvx session.
    secho "Deleting SnapVx sessions 3"
    snapshotsession delete $PROJECT/$VOLUME_NAME/$SNAP_SESSION_NAME3

    # Delete the snapshot session source volume.
    secho "Deleting the VMAX3 source volume"
    volume delete $PROJECT/$VOLUME_NAME --wait
}

ceph_setup()
{
    secho "Ceph setup"
    storageprovider show $CEPH_PROVIDER  &> /dev/null && return $?
    run storageprovider create $CEPH_PROVIDER $CEPH_IP 22 $CEPH_USER "$CEPH_SECRET_KEY" ceph
    run storagedevice discover_all
    storagedevice list

    storage_device=`storagedevice list | awk '/ceph/ {print($2)}'`
    storage_port=`storageport list $storage_device | grep -o 'CEPH.*PORT+-'`
    run transportzone add $IP_ZONE "$storage_port"

    run cos create block $CEPH_COS true --description='Ceph-VPool' --protocols=RBD --provisionType='Thin' --max_snapshots=8 --neighborhoods="$NH" --expandable true --systemtype ceph
    run cos allow $CEPH_COS block $TENANT

    run hosts create "$CEPH_CLIENT_HOST_NAME" "$TENANT" Linux "$CEPH_CLIENT_HOST" --port 22 --username $CEPH_CLIENT_HOST_USER --password $CEPH_CLIENT_HOST_PASSWORD --discoverable true
    sleep 10
    port=`initiator list $CEPH_CLIENT_HOST_NAME | awk '/^rbd:/ {print($1)}'`
    run transportzone add $IP_ZONE "$port"

    secho "Ceph setup completed"
}

ceph_test_volume_capacity() {
    local volname=$1
    local expected_display_size=$2

    local requested_capacity_gb=`volume show $volname | awk '/"requested_capacity_gb"/ {print($2)}' | sed "s/[\"',]//g"`
    secho "Compare volume size $requested_capacity_gb with requested $display_size"
    test "$requested_capacity_gb" = "$expected_display_size"
}

ceph_test_volumes() {
    local size=$1
    local display_size=$2

    secho "${FUNCNAME} Begins: create/list/delete volumes"
    run volume create $CEPH_VOLNAME $PROJECT $NH $CEPH_COS $size
    local volname=$PROJECT/$CEPH_VOLNAME

    run volume show $volname
    ceph_test_volume_capacity $volname $display_size

    run volume delete $volname --wait
}

ceph_test_expand_volume() {
    local initial_size=$1
    local new_size=$2
    local display_new_size=$3

    secho "${FUNCNAME} Begins: Expand volume"
    run volume create $CEPH_VOLNAME $PROJECT $NH $CEPH_COS $initial_size
    local volname=$PROJECT/$CEPH_VOLNAME

    run volume expand $volname $new_size

    ceph_test_volume_capacity $volname $display_new_size

    run volume delete $volname --wait
}

ceph_test_full_copy() {
    local initial_size=$1
    local display_size=$2

    secho "${FUNCNAME} Begins: Full copy volume"
    run volume create $CEPH_VOLNAME $PROJECT $NH $CEPH_COS $initial_size
    local volname=$PROJECT/$CEPH_VOLNAME

    local fullcopy_name=FullCopy-"$CEPH_VOLNAME"
    run volume full_copy $fullcopy_name $volname
    local fcvolname=$PROJECT/$fullcopy_name

    run volume show $fcvolname
    ceph_test_volume_capacity $fcvolname $display_size

    run volume delete $volname --wait
    run volume delete $fcvolname --wait
}

ceph_test_snapshot() {
    secho "${FUNCNAME} Begins: create/list/delete snapshot"
    run volume create $CEPH_VOLNAME $PROJECT $NH $CEPH_COS $BLK_SIZE
    local volname=$PROJECT/$CEPH_VOLNAME

    local snap_name=Snap-"$VOLNAME"
    run blocksnapshot create $volname $snap_name
    local snapname=$volname/$snap_name

    run blocksnapshot show $snapname

    run blocksnapshot delete $snapname
    run volume delete $volname --wait
}

ceph_test_export() {
    secho "${FUNCNAME} Begins: Export/unexport volume"
    run volume create $CEPH_VOLNAME $PROJECT $NH $CEPH_COS $BLK_SIZE
    local volname=$PROJECT/$CEPH_VOLNAME
    local snap_name=Snap-"$CEPH_VOLNAME"
    run blocksnapshot create $volname $snap_name
    local snapname=$volname/$snap_name

    run export_group create $PROJECT $CEPH_EXPORT_GROUP_NAME $NH --type Host --volspec $volname --hosts "$CEPH_CLIENT_HOST_NAME"
    local egname=$PROJECT/$CEPH_EXPORT_GROUP_NAME
    run export_group update $egname --addVolspec $snapname

    run export_group delete $egname
    run blocksnapshot delete $snapname
    run volume delete $volname --wait
}

ceph_tests()
{
    secho "Ceph tests"
    ceph_test_volumes 1GB 1.00
    ceph_test_expand_volume 1GB 2GB 2.00
    ceph_test_full_copy 1GB 1.00
    ceph_test_snapshot
    ceph_test_export
}

unity_common_setup()
{
     login_nd_configure_smtp_nd_add_licenses

     run syssvc $CONFIG_FILE "$BOURNE_IP" set_prop system_proxyuser_encpassword ${SYSADMIN_PASSWORD}

     ROOT_TENANT=`tenant root|tail -1`

     unity_project_setup
     unity_nh_setup

     secho "Setup ACLs on neighborhood for $ROOT_TENANT"
     run neighborhood allow $NH $ROOT_TENANT

     secho "Creating transport zone"
     run transportzone create $IP_ZONE $NH --type IP

}
unity_project_setup()
{
     secho "Creating project for unity"
     project show $UNITY_PROJECT &> /dev/null && return $?
     run project create $UNITY_PROJECT --tenant $ROOT_TENANT
}
unity_nh_setup()
{
    secho "Creating neighborhood for unity"
    neighborhood show $NH  &> /dev/null && return $?
    run neighborhood create $NH

}

discover_unity()
{
    secho "Starting storage system create"
    storagedevice show $UNITY_DEV &> /dev/null && return $?
    discoveredsystem create $UNITY_DEV unity $UNITY_IP $UNITY_PORT $UNITY_USER $UNITY_PW --serialno=$UNITY_SN
    storagedevice list
}
#Separated out unityfile_setup and tests from unity block tests since unityfile tests require different setup.
#Nas server domain validation does not allow viprsanity.com domain which is added by default for all other tests.
#Unity file side tests will run with local authentication
unityfile_setup()
{
     unity_common_setup

     discover_unity 
     unityfile_cos_setup

     storagepool update $UNITY_NATIVEGUID --type file
     storageport update $UNITY_NATIVEGUID IP --tzone nh/iptz
     run cos update file $COS_UNITY --storage $UNITY_NATIVEGUID
     run transportzone add $NH/$IP_ZONE $UNITY_IP_ENDPOINT1
     run storagedevice discover_all
}
unityfile_tests()
{
    FS_SIZEMB=$FS_UNITY_SIZE;
    FS_SIZE=$FS_UNITY_SIZE;
    FS_EXPAND_SIZE=$FS_UNITY_EXPAND_SIZE;
    PROJECT=$UNITY_PROJECT;
    file_tests $COS_UNITY default

}

unityfile_cos_setup()
{
     secho "Setting up Unity File Virtual Pools"

     run cos create file $COS_UNITY                               \
         --description 'Virtual-Pool-for-unity-file' true \
                         --protocols NFS --max_snapshots 10    \
                         --provisionType 'Thin' \
                         --neighborhoods $NH
    

    run cos create file $COS_UNITY_CIFS                               \
         --description 'Virtual-Pool-for-unity-file' true \
                         --protocols CIFS --max_snapshots 10    \
                         --provisionType 'Thin' \
                         --neighborhoods $NH

    ROOT_TENANT=`tenant root|tail -1`
    run cos allow $COS_UNITY file $ROOT_TENANT
    run cos allow $COS_UNITY_CIFS file $ROOT_TENANT

}

unityblock_setup()
{
    secho "Starting unity storage system create"
    run discoveredsystem create $UNITY_DEV unity $UNITY_IP $UNITY_PORT $UNITY_USER $UNITY_PW --serialno=$UNITY_SN
    run storagedevice list

    secho "Setup ACLs on neighborhood for $TENANT"
    run neighborhood allow $NH $TENANT

    run transportzone assign ${SRDF_VMAXA_VSAN} $NH
    run transportzone assign ${XTREMIO_4X_VSAN} $NH
    run storagepool update $UNITY_NATIVEGUID --type block --volume_type THIN_AND_THICK
    run transportzone add ${XTREMIO_4X_VSAN} $UNITY_INIT_PWWN1
    run transportzone add ${XTREMIO_4X_VSAN} $UNITY_INIT_PWWN2
    unityblock_cos_setup
    run cos update block $COS_UNITYBLOCK_CG --storage $UNITY_NATIVEGUID
    if [ "${UNITY_OPTION}" = "all" -o "${UNITY_OPTION}" = "vplex" ]; then
        vplexunity_setup
    fi
    if [ "${UNITY_OPTION}" = "all" -o "${UNITY_OPTION}" = "rp" ]; then
        rpunity_setup
    fi
   
}


# Unity tests command: "sanity sanity.conf viprIP unityblock" test all, including block, VPlex+Unity and RP+Unity
# "sanity sanity.conf viprIP unityblock vplex" only test VPlex+Unity
# "sanity sanity.conf viprIP unityblock rp" only test RP+Unity
# "sanity sanity.conf viprIP unityblock block" only test Unity
unityblock_tests()
{
    secho "unity block tests"

    if [ "${UNITY_OPTION}" = "all" -o "${UNITY_OPTION}" = "block" ]; then
        unityblock_base_tests
    fi

    if [ "${UNITY_OPTION}" = "all" -o "${UNITY_OPTION}" = "vplex" ]; then
        vplexunity_tests
    fi
    
    if [ "${UNITY_OPTION}" = "all" -o "${UNITY_OPTION}" = "rp" ]; then
        rpunity_tests
    fi

    if [ "${UNITY_OPTION}" = "hlu" ]; then
        secho "Creating tools.yml"
        setup_yaml unity

        unityblock_export_consistent_hlu_tests
    fi
}

unityblock_base_tests()
{
    secho "unity block tests"
    
    vol1=unity1-${RANDOM};
    vol2=unity-cg-${RANDOM};
    host=$PROJECT.lss.emc.com
    hostLbl=$PROJECT
    consistency_group=cg-${RANDOM}
    snap1_label=snap1-${RANDOM}
    snap2_label=snap2-${RANDOM}
    eg=eg-${RANDOM}
	    
    run volume create $vol1 $PROJECT $NH $COS_UNITYBLOCK_CG $BLK_SIZE
    sleep 60
    run volume expand $PROJECT/$vol1 $BLK_SIZE_EXPAND
	
    run hosts create $hostLbl $TENANT Windows $host --port 8111 --username user --password 'password' --osversion 1.0
    run initiator create $hostLbl FC $UNITY_INIT_PWWN1 --node $UNITY_INIT_NODE
    run initiator create $hostLbl FC $UNITY_INIT_PWWN2 --node $UNITY_INIT_NODE
	
    run export_group create $PROJECT $eg $NH --type Host --volspec $PROJECT/$vol1 --hosts $hostLbl
		
    run blocksnapshot create $PROJECT/$vol1 $snap1_label
    run blocksnapshot list $PROJECT/$vol1
    run blocksnapshot delete $PROJECT/$vol1/${snap1_label}
    run export_group delete $PROJECT/$eg
    run volume delete $PROJECT/$vol1 --wait
    run hosts delete $hostLbl

    echo "unity consistency group tests"
	    
    run blockconsistencygroup create $PROJECT $consistency_group
    run volume create $vol2 $PROJECT $NH $COS_UNITYBLOCK_CG 1280000000 --consistencyGroup $consistency_group
    sleep 60
    run blocksnapshot create $PROJECT/$vol2 $snap2_label
    sleep 30
    run blocksnapshot delete $PROJECT/$vol2/${snap2_label}
    sleep 30
    run volume delete $PROJECT/$vol2 --wait
    run blockconsistencygroup delete $consistency_group
    echo "**** Done unity block"
}

unity_verify_export() {
    # Parameters: Storage View Name, Number of Initiators, Number of Luns, HLU
    # While checking if the Initiator group does not exist, then parameter $2 should be "gone"
    host_name=$1
    if [ "$host_name" = "$HOST1" ]; then
        masking_view_name="$H1ID"
	elif [ "$host_name" = "$HOST2" ]; then
        masking_view_name="$H2ID"
    fi
    run ${BASEDIR}/\export-tests/vnxehelper.sh verify_export "${masking_view_name}" $2 $3 $4
    if [ $? -ne "0" ]; then
       echo There was a failure
       VERIFY_EXPORT_FAIL_COUNT=`expr $VERIFY_EXPORT_FAIL_COUNT + 1`
    fi
    VERIFY_EXPORT_COUNT=`expr $VERIFY_EXPORT_COUNT + 1`
}

unityblock_export_consistent_hlu_tests()
{
    echo "unity block export consistent HLU tests START"
    unityblock_export_consistent_hlu_test1
    unityblock_export_consistent_hlu_test2

    unityblock_export_consistent_hlu_snap_test1
    unityblock_export_consistent_hlu_snap_test2

    echo There were $VERIFY_EXPORT_COUNT export verifications
    echo There were $VERIFY_EXPORT_FAIL_COUNT export verification failures
    echo "unity block export consistent HLU tests END"
}

#Consistent Cluster HLU
# CLUSTER with 2 hosts
# Step-1: Export a volume to the cluster. Result: All hosts in the cluster sees the volume with same HLU (least unused number)
# Step-2: Export a volume to HOST1 (exclusive). Result: HLU assigned should be unused among cluster view
# Step-3: Export a volume to the cluster. Result: All hosts in the cluster sees the volume with same HLU (least unused number)
# Step-4: Export a volume to HOST2 (exclusive). Result: HLU assigned should be unused among cluster view
# Step-5: Delete the private volume from HOST1 exported in step-2
# Step-6: Remove HOST2 from cluster
# Step-7: Remove one shared volume from cluster exported in Step-1
# Step-8: Add HOST2 to CLUSTER. Result: All shared volumes of cluster to be exported to this new host with the HLU for those volumes same as that of cluster's view
# Step-9: Export new 2 volumes to the cluster. Result: All hosts in the cluster sees the volume with same HLU.
unityblock_export_consistent_hlu_test1()
{
    HOST1=$hostbase${tenant}11
    HOST2=$hostbase${tenant}12
    expname=unityEG-${RANDOM}
    volname=unityVol-${RANDOM}
    cluster=sanityCluster1

    k=`wwnIdx 1 1`
    nwwn1=`nwwn A$k`
    pwwn1=`pwwn A$k`
    H1ID=${nwwn1}:${pwwn1}

    k=`wwnIdx 1 2`
    nwwn2=`nwwn A$k`
    pwwn2=`pwwn A$k`
    H2ID=${nwwn2}:${pwwn2}

    echo "Unity Consistent HLU Test 1"

    run volume create ${volname} $PROJECT $NH $COS_UNITYBLOCK_CG $BLK_SIZE --count 4

    run export_group create $PROJECT ${expname}1 $NH --type Cluster --volspec $PROJECT/${volname}-1 --cluster ${tenant}/${cluster}
    unity_verify_export ${HOST1} 2 1 1
    unity_verify_export ${HOST2} 2 1 1

    run export_group create $PROJECT ${expname}2 $NH --type Host --volspec $PROJECT/${volname}-2 --hosts "${HOST1}"
    unity_verify_export ${HOST1} 2 2 0,1
    unity_verify_export ${HOST2} 2 1 1

    run export_group update $PROJECT/${expname}1 --addVols $PROJECT/${volname}-3
    unity_verify_export ${HOST1} 2 3 0,1,2
    unity_verify_export ${HOST2} 2 2 1,2

    run export_group create $PROJECT ${expname}3 $NH --type Host --volspec $PROJECT/${volname}-4 --hosts "${HOST2}"
    unity_verify_export ${HOST1} 2 3 0,1,2
    unity_verify_export ${HOST2} 2 3 0,1,2

    run export_group delete $PROJECT/${expname}2
    unity_verify_export ${HOST1} 2 2 1,2
    unity_verify_export ${HOST2} 2 3 0,1,2

    run export_group update $PROJECT/${expname}1 --remHosts "${HOST2}"
    unity_verify_export ${HOST1} 2 2 1,2
    unity_verify_export ${HOST2} 2 1 0

    run export_group update $PROJECT/${expname}1 --remVols $PROJECT/${volname}-1
    unity_verify_export ${HOST1} 2 1 2
    unity_verify_export ${HOST2} 2 1 0

    run export_group update $PROJECT/${expname}1 --addHosts "${HOST2}"
    unity_verify_export ${HOST1} 2 1 2
    unity_verify_export ${HOST2} 2 2 0,2

    run export_group update $PROJECT/${expname}1 --addVols "${PROJECT}/${volname}-1,${PROJECT}/${volname}-2"
    unity_verify_export ${HOST1} 2 3 1,2,3
    unity_verify_export ${HOST2} 2 4 0,1,2,3

    run export_group delete $PROJECT/${expname}3
    unity_verify_export ${HOST1} 2 3 1,2,3
    unity_verify_export ${HOST2} 2 3 1,2,3

    run export_group delete $PROJECT/${expname}1
    unity_verify_export ${HOST1} gone
    unity_verify_export ${HOST2} gone

    run volume delete $PROJECT/${volname}-1 --wait
    run volume delete $PROJECT/${volname}-2 --wait
    run volume delete $PROJECT/${volname}-3 --wait
    run volume delete $PROJECT/${volname}-4 --wait
}

#Consistent Cluster HLU
# CLUSTER with 2 hosts
# Step-1: Export a volume to HOST1 (exclusive). Result: HLU assigned should be unused among cluster view
# Step-2: Export a volume to the cluster. Result: All hosts in the cluster sees the volume with same HLU (least unused number)
# Step-3: Export a volume to the cluster. Result: All hosts in the cluster sees the volume with same HLU (least unused number)
# Step-4: Export a volume to HOST2 (exclusive). Result: HLU assigned should be unused among cluster view
# Step-5: Delete the private volume from HOST1 exported in step-2
# Step-6: Remove HOST2 from cluster
# Step-7: Remove one shared volume from cluster exported in Step-1
# Step-8: Add HOST2 to CLUSTER. Result: All shared volumes of cluster to be exported to this new host with the HLU for those volumes same as that of cluster's view
# Step-9: Export a new volume to the cluster. Result: All hosts in the cluster sees the volume with same HLU.
# Step-10: Export a new volume to HOST2 (exclusive). Result: HLU assigned should be unused among cluster view
unityblock_export_consistent_hlu_test2()
{
    HOST1=$hostbase${tenant}21
    HOST2=$hostbase${tenant}22
    expname=unityEG-${RANDOM}
    volname=unityVol-${RANDOM}
    cluster=sanityCluster2

    k=`wwnIdx 2 1`
    nwwn1=`nwwn A$k`
    pwwn1=`pwwn A$k`
    H1ID=${nwwn1}:${pwwn1}

    k=`wwnIdx 2 2`
    nwwn2=`nwwn A$k`
    pwwn2=`pwwn A$k`
    H2ID=${nwwn2}:${pwwn2}

    echo "Unity Consistent HLU Test 2"

    run volume create ${volname} $PROJECT $NH $COS_UNITYBLOCK_CG $BLK_SIZE --count 4

    run export_group create $PROJECT ${expname}1 $NH --type Host --volspec $PROJECT/${volname}-1 --hosts "${HOST1}"
    unity_verify_export ${HOST1} 2 1 0

    run export_group create $PROJECT ${expname}2 $NH --type Cluster --volspec $PROJECT/${volname}-2 --cluster ${tenant}/${cluster}
    unity_verify_export ${HOST1} 2 2 0,1
    unity_verify_export ${HOST2} 2 1 1

    run export_group update $PROJECT/${expname}2 --addVols $PROJECT/${volname}-3
    unity_verify_export ${HOST1} 2 3 0,1,2
    unity_verify_export ${HOST2} 2 2 1,2

    run export_group create $PROJECT ${expname}3 $NH --type Host --volspec $PROJECT/${volname}-4 --hosts "${HOST2}"
    unity_verify_export ${HOST1} 2 3 0,1,2
    unity_verify_export ${HOST2} 2 3 0,1,2

    run export_group delete $PROJECT/${expname}1
    unity_verify_export ${HOST1} 2 2 1,2
    unity_verify_export ${HOST2} 2 3 0,1,2

    run export_group update $PROJECT/${expname}2 --remHosts "${HOST2}"
    unity_verify_export ${HOST1} 2 2 1,2
    unity_verify_export ${HOST2} 2 1 0

    run export_group update $PROJECT/${expname}2 --remVols $PROJECT/${volname}-2
    unity_verify_export ${HOST1} 2 1 2

    run export_group update $PROJECT/${expname}2 --addHosts "${HOST2}"
    unity_verify_export ${HOST1} 2 1 2
    unity_verify_export ${HOST2} 2 2 0,2

    run export_group update $PROJECT/${expname}2 --addVols $PROJECT/${volname}-2
    unity_verify_export ${HOST1} 2 2 1,2
    unity_verify_export ${HOST2} 2 3 0,1,2

    run export_group update $PROJECT/${expname}3 --addVols $PROJECT/${volname}-1
    unity_verify_export ${HOST1} 2 2 1,2
    unity_verify_export ${HOST2} 2 4 0,1,2,3

    run export_group delete $PROJECT/${expname}3
    unity_verify_export ${HOST1} 2 2 1,2
    unity_verify_export ${HOST2} 2 2 1,2

    run export_group delete $PROJECT/${expname}2
    unity_verify_export ${HOST1} gone
    unity_verify_export ${HOST2} gone

    run volume delete $PROJECT/${volname}-1 --wait
    run volume delete $PROJECT/${volname}-2 --wait
    run volume delete $PROJECT/${volname}-3 --wait
    run volume delete $PROJECT/${volname}-4 --wait
}

#Consistent Cluster HLU
# CLUSTER with 2 hosts
# Step-1: Export a volume to the cluster. Result: All hosts in the cluster sees the volume with same HLU (least unused number)
# Step-2: Export a volume to HOST1 (exclusive). Result: HLU assigned should be unused among cluster view
# Step-3: Export a snapshot to the cluster. Result: All hosts in the cluster sees the snapshot with same HLU (least unused number)
# Step-4: Export a volume to HOST2 (exclusive). Result: HLU assigned should be unused among cluster view
# Step-5: Delete the private volume from HOST1 exported in step-2
# Step-6: Remove HOST2 from cluster
# Step-7: Remove one shared volume from cluster exported in Step-1
# Step-8: Add HOST2 to CLUSTER. Result: All shared volumes of cluster to be exported to this new host with the HLU for those volumes same as that of cluster's view
# Step-9: Export 2 new snapshots to the cluster. Result: All hosts in the cluster sees the snapshot with same HLU.
unityblock_export_consistent_hlu_snap_test1()
{
    HOST1=$hostbase${tenant}11
    HOST2=$hostbase${tenant}12
    expname=unityEG-${RANDOM}
    volname=unityVol-${RANDOM}
    snapname=snap-${RANDOM}
    cluster=sanityCluster1

    k=`wwnIdx 1 1`
    nwwn1=`nwwn A$k`
    pwwn1=`pwwn A$k`
    H1ID=${nwwn1}:${pwwn1}

    k=`wwnIdx 1 2`
    nwwn2=`nwwn A$k`
    pwwn2=`pwwn A$k`
    H2ID=${nwwn2}:${pwwn2}

    echo "Unity Consistent HLU Snap Test 1"

    run volume create ${volname} $PROJECT $NH $COS_UNITYBLOCK_CG $BLK_SIZE --count 3
    run blocksnapshot create $PROJECT/${volname}-1 ${snapname}-1
    run blocksnapshot create $PROJECT/${volname}-2 ${snapname}-2
    run blocksnapshot create $PROJECT/${volname}-3 ${snapname}-3

    run export_group create $PROJECT ${expname}1 $NH --type Cluster --volspec $PROJECT/${volname}-1 --cluster ${tenant}/${cluster}
    unity_verify_export ${HOST1} 2 1 1
    unity_verify_export ${HOST2} 2 1 1

    run export_group create $PROJECT ${expname}2 $NH --type Host --volspec $PROJECT/${volname}-2 --hosts "${HOST1}"
    unity_verify_export ${HOST1} 2 2 0,1
    unity_verify_export ${HOST2} 2 1 1

    run export_group update $PROJECT/${expname}1 --addVolspec $PROJECT/${volname}-1/${snapname}-1
    sleep 30 # exports involving snap and volumes requires sleep interval before verification
    unity_verify_export ${HOST1} 2 3 0,1,2
    unity_verify_export ${HOST2} 2 2 1,2

    run export_group create $PROJECT ${expname}3 $NH --type Host --volspec $PROJECT/${volname}-3 --hosts "${HOST2}"
    sleep 30
    unity_verify_export ${HOST1} 2 3 0,1,2
    unity_verify_export ${HOST2} 2 3 0,1,2

    run export_group delete $PROJECT/${expname}2
    unity_verify_export ${HOST1} 2 2 1,2
    unity_verify_export ${HOST2} 2 3 0,1,2

    run export_group update $PROJECT/${expname}1 --remHosts "${HOST2}"
    sleep 30
    unity_verify_export ${HOST1} 2 2 1,2
    unity_verify_export ${HOST2} 2 1 0

    run export_group update $PROJECT/${expname}1 --remVols $PROJECT/${volname}-1
    unity_verify_export ${HOST1} 2 1 2

    run export_group update $PROJECT/${expname}1 --addHosts "${HOST2}"
    unity_verify_export ${HOST1} 2 1 2
    unity_verify_export ${HOST2} 2 2 0,2

    run export_group update $PROJECT/${expname}1 --addVolspec "$PROJECT/${volname}-2/${snapname}-2,$PROJECT/${volname}-3/${snapname}-3"
    sleep 30
    unity_verify_export ${HOST1} 2 3 1,2,3
    unity_verify_export ${HOST2} 2 4 0,1,2,3

    run export_group delete $PROJECT/${expname}3
    unity_verify_export ${HOST1} 2 3 1,2,3
    unity_verify_export ${HOST2} 2 3 1,2,3

    run export_group delete $PROJECT/${expname}1
    unity_verify_export ${HOST1} gone
    unity_verify_export ${HOST2} gone

    run blocksnapshot delete $PROJECT/${volname}-1/${snapname}-1
    run blocksnapshot delete $PROJECT/${volname}-2/${snapname}-2
    run blocksnapshot delete $PROJECT/${volname}-3/${snapname}-3
    run volume delete $PROJECT/${volname}-1 --wait
    run volume delete $PROJECT/${volname}-2 --wait
    run volume delete $PROJECT/${volname}-3 --wait
}

#Consistent Cluster HLU
# CLUSTER with 2 hosts
# Step-1: Export a volume to HOST1 (exclusive). Result: HLU assigned should be unused among cluster view
# Step-2: Export a volume to the cluster. Result: All hosts in the cluster sees the volume with same HLU (least unused number)
# Step-3: Export a snapshot to the cluster. Result: All hosts in the cluster sees the snapshot with same HLU (least unused number)
# Step-4: Export a volume to HOST2 (exclusive). Result: HLU assigned should be unused among cluster view
# Step-5: Delete the private volume from HOST1 exported in step-2
# Step-6: Remove HOST2 from cluster
# Step-7: Remove one shared volume from cluster exported in Step-1
# Step-8: Add HOST2 to CLUSTER. Result: All shared volumes of cluster to be exported to this new host with the HLU for those volumes same as that of cluster's view
# Step-9: Export a new snapshot to the cluster. Result: All hosts in the cluster sees the snapshot with same HLU.
# Step-10: Export a new snapshot to HOST2 (exclusive). Result: HLU assigned should be unused among cluster view
unityblock_export_consistent_hlu_snap_test2()
{
    HOST1=$hostbase${tenant}21
    HOST2=$hostbase${tenant}22
    expname=unityEG-${RANDOM}
    volname=unityVol-${RANDOM}
    snapname=snap-${RANDOM}
    cluster=sanityCluster2

    k=`wwnIdx 2 1`
    nwwn1=`nwwn A$k`
    pwwn1=`pwwn A$k`
    H1ID=${nwwn1}:${pwwn1}

    k=`wwnIdx 2 2`
    nwwn2=`nwwn A$k`
    pwwn2=`pwwn A$k`
    H2ID=${nwwn2}:${pwwn2}

    echo "Unity Consistent HLU Snap Test 2"

    run volume create ${volname} $PROJECT $NH $COS_UNITYBLOCK_CG $BLK_SIZE --count 3
    run blocksnapshot create $PROJECT/${volname}-1 ${snapname}-1
    run blocksnapshot create $PROJECT/${volname}-2 ${snapname}-2
    run blocksnapshot create $PROJECT/${volname}-3 ${snapname}-3

    run export_group create $PROJECT ${expname}1 $NH --type Host --volspec $PROJECT/${volname}-1 --hosts "${HOST1}"
    unity_verify_export ${HOST1} 2 1 0

    run export_group create $PROJECT ${expname}2 $NH --type Cluster --volspec $PROJECT/${volname}-2 --cluster ${tenant}/${cluster}
    unity_verify_export ${HOST1} 2 2 0,1
    unity_verify_export ${HOST2} 2 1 1

    run export_group update $PROJECT/${expname}2 --addVolspec $PROJECT/${volname}-1/${snapname}-1
    sleep 30
    unity_verify_export ${HOST1} 2 3 0,1,2
    unity_verify_export ${HOST2} 2 2 1,2

    run export_group create $PROJECT ${expname}3 $NH --type Host --volspec $PROJECT/${volname}-3 --hosts "${HOST2}"
    sleep 30
    unity_verify_export ${HOST1} 2 3 0,1,2
    unity_verify_export ${HOST2} 2 3 0,1,2

    run export_group delete $PROJECT/${expname}1
    unity_verify_export ${HOST1} 2 2 1,2
    unity_verify_export ${HOST2} 2 3 0,1,2

    run export_group update $PROJECT/${expname}2 --remHosts "${HOST2}"
    sleep 30
    unity_verify_export ${HOST1} 2 2 1,2
    unity_verify_export ${HOST2} 2 1 0

    run export_group update $PROJECT/${expname}2 --remVols $PROJECT/${volname}-2
    unity_verify_export ${HOST1} 2 1 2

    run export_group update $PROJECT/${expname}2 --addHosts "${HOST2}"
    unity_verify_export ${HOST1} 2 1 2
    unity_verify_export ${HOST2} 2 2 0,2

    run export_group update $PROJECT/${expname}2 --addVolspec $PROJECT/${volname}-2/${snapname}-2
    sleep 30
    unity_verify_export ${HOST1} 2 2 1,2
    unity_verify_export ${HOST2} 2 3 0,1,2

    run export_group update $PROJECT/${expname}3 --addVolspec $PROJECT/${volname}-3/${snapname}-3
    sleep 30
    unity_verify_export ${HOST1} 2 2 1,2
    unity_verify_export ${HOST2} 2 4 0,1,2,3

    run export_group delete $PROJECT/${expname}3
    unity_verify_export ${HOST1} 2 2 1,2
    unity_verify_export ${HOST2} 2 2 1,2

    run export_group delete $PROJECT/${expname}2
    unity_verify_export ${HOST1} gone
    unity_verify_export ${HOST2} gone

    run blocksnapshot delete $PROJECT/${volname}-1/${snapname}-1
    run blocksnapshot delete $PROJECT/${volname}-2/${snapname}-2
    run blocksnapshot delete $PROJECT/${volname}-3/${snapname}-3
    run volume delete $PROJECT/${volname}-1 --wait
    run volume delete $PROJECT/${volname}-2 --wait
    run volume delete $PROJECT/${volname}-3 --wait
}

unityblock_cos_setup()
{
    secho "setting up Unity block virtual pool"
    
    run cos create block $COS_UNITYBLOCK_CG                            \
	--description 'Virtual-Pool-for-unity-block-cg' true         \
                         --protocols FC                   \
                         --numpaths 2 \
                         --max_snapshots 10 \
                         --system_type unity \
                         --provisionType 'Thin' \
                         --expandable true \
                         --neighborhoods $NH \
                         --multiVolumeConsistency 
}


vplexunity_setup()
{
    run syssvc $CONFIG_FILE $BOURNE_IPADDR set_prop controller_vplex_director_min_port_count 1
    
    # Discover the VPLEX storage systems 
    echo "Discovering VPLEX"
    #discoveredsystem create $UNITY_DEV unity $UNITY_IP $UNITY_PORT $UNITY_USER $UNITY_PW --serialno=$UNITY_SN
    run storageprovider create $VPLEX_DEV_NAME $VPLEX_IP 443 $VPLEX_USER "$VPLEX_PASSWD" vplex
    
    run storagedevice discover_all
    storagedevice list
    storageport list $UNITY_NATIVEGUID --v
    storageport list $VPLEX_GUID --v
    sleep 90
    storageport list $VPLEX_GUID --v
    
    # Setup the varrays. $NH contains VPLEX cluster-1
    run storageport update $VPLEX_GUID FC --group director-1-1-A --addvarrays $NH
    run storageport update $VPLEX_GUID FC --group director-1-1-B --addvarrays $NH
    run storageport update $VPLEX_GUID FC --group director-2-1-A --addvarrays $NH2
    run storageport update $VPLEX_GUID FC --group director-2-1-B --addvarrays $NH2
    storageport list $VPLEX_GUID --v

    vplexunity_cos_setup
    storageport list $VPLEX_GUID --v
}

vplexunity_cos_setup()
{
    run cos create block cosvplexlocal true                   \
                     --description 'Local-CoS-for-VPlex'      \
                         --protocols FC                   \
                         --numpaths 2 \
                         --provisionType 'Thin' \
                     --highavailability vplex_local           \
                         --neighborhoods $NH \
                     --max_snapshots 1                        \
                     --max_mirrors 1                          \
                     --expandable false 

    run cos allow cosvplexlocal block $TENANT

    run cos update block cosvplexlocal --storage $UNITY_NATIVEGUID
    
}

vplexunity_tests()
{
    echo "**** Executing VPLEX tests"
    
    hname=$(hostname)
    if [ $hname = "standalone" ]; then
        hname=$SHORTENED_HOST
    fi
    echo "hostname is $hname"
    	
    localVolume1=$hname-${RANDOM}-VPlexLocal1
    localVolume2=$hname-${RANDOM}-VPlexLocal2

    sourceSideSuffix=-0
    localSnapshot=$hname-${RANDOM}-VPlexLocalSnap
    
    echo "**** Creating VPLEX local volumes"
    run volume create $localVolume1 $PROJECT $NH cosvplexlocal $BLK_SIZE
    run volume create $localVolume2 $PROJECT $NH cosvplexlocal $BLK_SIZE --count=2
    	
    echo "**** Volumes created so far"
    run volume list $PROJECT
    	
    echo "**** Creating VPLEX volume snapshots"
    run blocksnapshot create $PROJECT/$localVolume1 $localSnapshot
    blocksnapshot list $PROJECT/$localVolume1
    blocksnapshot show $PROJECT/$localVolume1/$localSnapshot
   
    
    echo "**** Deleting VPLEX volume snapshots"
    run blocksnapshot delete $PROJECT/$localVolume1/$localSnapshot
    
    echo "**** Deleting VPLEX volumes"
    run volume delete $PROJECT/$localVolume1 --wait
    run volume delete $PROJECT/$localVolume2-1 --wait
    run volume delete $PROJECT/$localVolume2-2 --wait
    
    echo "**** Completed VPLEX Unity Tests"
    
}

rpunity_setup()
{
    rp_protection_system_setup
    #run storageport update $UNITY_NATIVEGUID FC --addvarrays $NH
    
    run cos create block rpunity_cdp true \
            --description 'RP-Unity-Source-CDP'  \
            --protocols FC \
            --numpaths 2 \
            --provisionType 'Thin' \
            --neighborhoods $NH \
            --multiVolumeConsistency \
            --protectionCoS $NH:$COS_UNITYBLOCK_CG:min \
            --max_snapshots 10 \
            --expandable true \
            --rp_copy_mode ASYNCHRONOUS \
            --rp_rpo_value 5 \
            --rp_rpo_type MINUTES
    run cos update block rpunity_cdp --storage $UNITY_NATIVEGUID
    
}

rpunity_tests()
{
    secho "*** unity RP tests"
    run blockconsistencygroup create $PROJECT $RP_UNITY_CONSISTENCY_GROUP
    run volume create $RP_UNITY_VOLUME $PROJECT $NH rpunity_cdp $RP_VOLUME_SIZE --consistencyGroup $RP_UNITY_CONSISTENCY_GROUP --count=1
    # Failover Test
    #recoverpoint_failover_test ${RP_UNITY_VOLUME} $RP_UNITY_VOLUME-target-${NH}
    
    # CG Failover/Swap Test
    #recoverpoint_cg_failover_test ${RP_UNITY_CONSISTENCY_GROUP} ${RP_UNITY_VOLUME} ${NH} $RP_UNITY_VOLUME-target-${NH} ${NH}

    # Bookmark Export Tests
    rp_tgt_varrays=("${NH}")
    recoverpoint_export_bookmark_tests ${RP_UNITY_VOLUME} ${NH} ${rp_tgt_varrays} rpunity_cdp "unity"
   
    run volume delete $PROJECT/${RP_UNITY_VOLUME} --wait

    secho 'CG cleanup...'
    run blockconsistencygroup delete $RP_UNITY_CONSISTENCY_GROUP
    secho 'RP Unity tests done'

}

custvolname_setup()
{
    echo "Executing custom volume name setup"
    vplex_setup
}

custvolname_tests()
{
    echo "Executing custom volume name tests"
    custvolname_vplex_tests
}

custvolname_vplex_tests()
{
    echo "Executing VPLEX custom volume name tests"

    # Create the custom volume name custom configurations for VPLEX.
    echo "Enabling custom volume naming"
    customconfig create VolumeNamingEnabled true systemType vplex --register true

    echo "Setting custom volume name configuration"
    customconfig create VolumeNamingProvisionNoExport "{volume_label}-{project_name}" systemType vplex --register true

    echo "Setting export custom volume name configuration"
    customconfig create VolumeNamingProvisionWithExport "{volume_label}-{project_name}-{export_name}" systemType vplex --register true

    # Test the use case where a VPLEX volume is provisioned and no compute
    # resource is specified in the request.
    localVolume1=VPlexLocal1-${RANDOM}
    echo "Creating VPLEX local volume $localVolume1"
    volume create $localVolume1 $PROJECT $NH cosvplexlocal $BLK_SIZE

    # We should be able to find the volume by its custom name.
    echo "Show VPLEX local volume with custom name"
    customName1=$localVolume1-$PROJECT
    volume show $PROJECT/$customName1

    # Clean up the volume.
    echo "Deleting VPLEX local volume"
    volume delete $PROJECT/$customName1 --wait

    # Create a host and get the URI.
    host=$PROJECT.lss.emc.com
    hostLbl=$PROJECT
    echo "Creating host with name $hostLbl"
    hosts create $hostLbl $TENANT Windows $host --port 8111 --username user --password 'password' --osversion 1.0
    echo "Getting host URI"
    hosturi=`hosts list ${TENANT} | grep ${hostLbl} | awk '{print $4}'`
    echo "Host URI is $hosturi"

    # Test the case where a VPLEX volume is prvisioned and a compute
    # resource is specified in the request.
    localVolume2=VPlexLocal2-${RANDOM}
    echo "Creating VPLEX local volume $localVolume2 for host $hostLbl"
    volume create $localVolume2 $PROJECT $NH cosvplexlocal $BLK_SIZE --computeResource $hosturi

    # Again we should be able to find the volume by its custom name.
    echo "Show VPLEX local volume with custom name"
    customName2=$localVolume2-$PROJECT-$hostLbl
    volume show $PROJECT/$customName2

    # Clean up volume
    echo "Deleting VPLEX local volume"
    volume delete $PROJECT/$customName2 --wait

    # Clean up host
    echo "Deleting host"
    hosts delete $hostLbl

    # Test the case where a VPLEX backend snapshot is exposed as a VPLEX volume.
    localVolume3=VPlexLocal3-${RANDOM}
    localVolume3Snap=$localVolume3-Snap
    echo "Creating VPLEX local volume $localVolume3"
    volume create $localVolume3 $PROJECT $NH cosvplexlocal $BLK_SIZE
    customName3=$localVolume3-$PROJECT
    echo "Creating VPLEX snapshot for $customName3"
    blocksnapshot create $PROJECT/$customName3 $localVolume3Snap
    echo "Exposing snapshot as VPLEX volume"
    blocksnapshot expose $PROJECT/$customName3/$localVolume3Snap

    # We should be able to find the the VPLEX volume built on top
    # of the snapshot by its custom name.
    exposedVolumeCustomName=$localVolume3Snap-$PROJECT
    volume show $PROJECT/$exposedVolumeCustomName

    # Clean up volumes and snapshot.
    echo "Deleting VPLEX volume built on snapshot"
    volume delete $PROJECT/$exposedVolumeCustomName --wait
    echo "Deleting snapshot"
    blocksnapshot delete $PROJECT/$customName3/$localVolume3Snap
    echo "Deleting VPLEX local volume"
    volume delete $PROJECT/$customName3 --wait

    # Test case where a deatched mirror is promoted to a VPLEX volume.
    localVolume4=Vol4-${RANDOM}
    localVolume4Mirror=$localVolume4-Mir
    echo "Creating VPLEX local volume $localVolume4"
    volume create $localVolume4 $PROJECT $NH cosvplexlocal $BLK_SIZE
    customName4=$localVolume4-$PROJECT
    echo "Attaching mirror $localVolume4Mirror to volume $localVolume4"
    blockmirror attach $PROJECT/$customName4 $localVolume4Mirror 1
    sleep 30
    echo "Detaching mirror and promoting to VPLEX volume"
    blockmirror detach $PROJECT/${customName4}

    # We should be able to find the the promoted VPLEX volume by its custom name.
    promotedMirrorCustomName=$customName4-$localVolume4Mirror-0-$PROJECT
    volume show $PROJECT/$promotedMirrorCustomName

    # Clean up volumes.
    echo "Deleting VPLEX local volume"
    volume delete $PROJECT/$customName4 --wait
    echo "Deleting VPLEX local volume"
    volume delete $PROJECT/$promotedMirrorCustomName --wait

    # Clean up custom configs.
    echo "Disable custom configurations"
    customconfig delete systemType.vplex.VolumeNamingEnabled systemType vplex False
    customconfig delete systemType.vplex.VolumeNamingProvisionNoExport systemType vplex False
    customconfig delete systemType.vplex.VolumeNamingProvisionWithExport systemType vplex False

    echo "Completed custom volume name tests"
}

arrayaffinity_vmaxblock_setup_once()
{
    # do this only once
    smisprovider show $VMAX_SMIS_DEV &> /dev/null && return $?

    run smisprovider create $VMAX_SMIS_DEV $VMAX_SMIS_IP $VMAX_SMIS_PORT $SMIS_USER "$SMIS_PASSWD" $VMAX_SMIS_SSL
    run storagedevice discover_all --ignore_error

    sleep 15

    secho "VMAX storage pool update"
    run storagepool update $VMAX_NATIVEGUID --type block --volume_type THIN_ONLY
    run storagepool update $VMAX_NATIVEGUID --nhadd $NH --type block
}

arrayaffinity_vnxblock_setup_once()
{
    # do this only once
    smisprovider show $VNX_SMIS_DEV &> /dev/null && return $?

    run smisprovider create $VNX_SMIS_DEV $VNX_SMIS_IP $VNX_SMIS_PORT $SMIS_USER "$SMIS_PASSWD" $VNX_SMIS_SSL
    run storagedevice discover_all --ignore_error

    sleep 15

    secho "VNX Block storage pool update"
    run storagepool update $VNXB_NATIVEGUID --type block --volume_type THIN_ONLY
    run storagepool update $VNXB_NATIVEGUID --nhadd $NH --type block
}

arrayaffinity_cos_setup()
{
    secho "setting up array affinity block virtual pool"

    run cos create block $COS_ARRAYAFFINITY \
        --description 'Virtual-Pool-for-arrayaffinity' true \
                         --protocols FC \
                         --numpaths 2 \
                         --max_snapshots 10 \
                         --provisionType 'Thin' \
                         --expandable true \
                         --neighborhoods $NH \
                         --multiVolumeConsistency \
                         --placement_policy default_policy

    run cos update block $COS_ARRAYAFFINITY --storage $VMAX_NATIVEGUID
    run cos update block $COS_ARRAYAFFINITY --storage $VNXB_NATIVEGUID

    run cos create block $COS_ARRAYAFFINITY_VMAX \
	    --description 'Virtual-Pool-for-arrayaffinity-vmax' true \
                         --protocols FC \
                         --numpaths 2 \
                         --max_snapshots 10 \
                         --system_type vmax \
                         --provisionType 'Thin' \
                         --expandable true \
                         --neighborhoods $NH \
                         --multiVolumeConsistency \
                         --placement_policy array_affinity

    run cos update block $COS_ARRAYAFFINITY_VMAX --storage $VMAX_NATIVEGUID

    run cos create block $COS_ARRAYAFFINITY_VNX \
	    --description 'Virtual-Pool-for-arrayaffinity-vnx' true \
                         --protocols FC \
                         --numpaths 2 \
                         --max_snapshots 10 \
                         --system_type vnxblock \
                         --provisionType 'Thin' \
                         --expandable true \
                         --neighborhoods $NH \
                         --multiVolumeConsistency \
                         --placement_policy array_affinity

    run cos update block $COS_ARRAYAFFINITY_VNX --storage $VNXB_NATIVEGUID
}

arrayaffinity_host_setup()
{
    portwwn=10:a0:b0:c0:d0:e0:f0:01
    nodewwn=20:a0:b0:c0:d0:e0:f0:01
    fctz="${NH}/FABRIC_losam082-fabric"

    echo "Creating host with name $HOST_ARRAYAFFINITY"
    run hosts create ${HOST_ARRAYAFFINITY} $TENANT Windows ${HOST_ARRAYAFFINITY_FQDN} --port 8111 --username user --password 'password' --osversion 1.0
    run initiator create ${HOST_ARRAYAFFINITY} FC $portwwn --node $nodewwn
	echo "Adding WWNs to Network"
	run transportzone add $fctz $portwwn

    echo "Getting host URI"
    HOST_ARRAYAFFINITY_URI=`hosts list ${TENANT} | grep ${HOST_ARRAYAFFINITY_FQDN} | awk '{print $4}'`
    echo "Host URI is ${HOST_ARRAYAFFINITY_URI}"
}

arrayaffinity_setup()
{
    secho "Run array affinity setup"
    arrayaffinity_vmaxblock_setup_once
    arrayaffinity_vnxblock_setup_once

    arrayaffinity_cos_setup
    arrayaffinity_host_setup
    secho "Completed array affinity setup"
}

arrayaffinity_cos_tests()
{
    echo "Executing array affinity update cos tests"

    run cos update block $COS_ARRAYAFFINITY_VMAX --placement_policy default_policy
    run cos update block $COS_ARRAYAFFINITY_VNX --placement_policy default_policy

    echo "**** Done array affinity update cos tests"
}

# Test array affinity placement
#
# Precondition
#    1. VPool with default policy (with both VMAX3 and VNX arrays)
#       VPool with only VMAX3, and VPool with only VNX (used to create pre-existing volume for host)
#    2. Host to be used for provision
#
# Steps
#    1. Create vol1 from the VPool VMAX3-VNX (with default policy)
#    2. Get system type of vol1
#    3. If system type is vmax3, provison vol2 from VNX VPool for the host
#       Otherwise, provision vol2 from VMAX3 VPool for the host
#    4. Export vol2 to the host
#    5. Delete vol2 in ViPR
#    6. Run ARRAY_AFFINITY namespace discovery on vol2 array
#    7. Verify the host's preferredPools
#    8. Change the VPool VMAX3-VNX to use array affinity policy
#    9. Provision vol3 to the host from the VPool VMAX3-VNX
#    10. Verify vol3 is placed to preferred array
#
# Cleanup
#    1. Delete vol1 and vol3
#    2. Run umanaged volume discovery on vol2 array
#    3. Ingest vol2
#    4. Delete export group
#    5. Delete vol2
#    6. Delete VPools
#    7. Delete host
#
arrayaffinity_placement_tests()
{
    echo "Executing array affinity placement tests"
    vol1=arrayaffinity-${RANDOM}

    echo "Creating volume with name $vol1 with default policy"
    run volume create $vol1 $PROJECT $NH $COS_ARRAYAFFINITY $BLK_SIZE

    echo "Finding volume's system type"
    system_type=`volume show $PROJECT/$vol1 | grep system_type | awk -F\" '{print $4}'`

    preferred_cos=$COS_ARRAYAFFINITY_VMAX
    preferred_system=$VMAX_NATIVEGUID
    preferred_system_type=vmax3
    if [ "$system_type" = "vmax3" ] ; then
        preferred_cos=$COS_ARRAYAFFINITY_VNX
        preferred_system=$VNXB_NATIVEGUID
        preferred_system_type=vnxblock
    fi

    vol2=arrayaffinity-${RANDOM}
    echo "Creating volume $vol2 using virtual pool $preferred_cos"
    run volume create $vol2 $PROJECT $NH $preferred_cos $BLK_SIZE

    eg=arrayaffinity-${RANDOM}
    run export_group create $PROJECT $eg $NH --type Host --volspec $PROJECT/$vol2 --hosts $HOST_ARRAYAFFINITY

    echo "Delete volume $vol2 from ViPR"
    run_noundo volume delete ${PROJECT}/${vol2} --vipronly

    run storagedevice discover_namespace $preferred_system ARRAY_AFFINITY
	# verify host's preferredPools is populated
	echo "Verifying host's preferredPools"
	preferred_pools=`hosts show ${HOST_ARRAYAFFINITY} | grep preferred_pools`
	if [[ -z $preferred_pools ]] ; then
	    echo "Fail: no preferred pool discovered"
	    exit 1
	fi

    run cos update block $COS_ARRAYAFFINITY --placement_policy array_affinity

    vol3=arrayaffinity-${RANDOM}
    echo "Creating volume $vol3 for host ${HOST_ARRAYAFFINITY}"
    run volume create $vol3 $PROJECT $NH $COS_ARRAYAFFINITY $BLK_SIZE --computeResource ${HOST_ARRAYAFFINITY_URI}

    # verify the system type of the volume is same as the preferred one
    echo "Verifying array affinity placement"
    system_type_vol3=`volume show $PROJECT/$vol3 | grep system_type | awk -F\" '{print $4}'`
    if [ "$preferred_system_type" != "$system_type_vol3" ] ; then
        echo -e "Fail: system type \"$system_type_vol3\" isn't equal to preferred array's system type \"$preferred_system_type\""
        exit 1
    fi

    echo "Clean up"
    echo "Deleting volume $vol1 and $vol3"
    run volume delete ${PROJECT}/${vol1} --wait
    run volume delete ${PROJECT}/${vol3} --wait

    # ingest vol2 so that it can be deleted
    run storagedevice discover_namespace $preferred_system 'UNMANAGED_VOLUMES'
    run unmanagedvolume ingest_export ${NH} ${preferred_cos} $PROJECT --volspec "${vol2}" --host ${HOST_ARRAYAFFINITY}
    run export_group list $PROJECT

    echo "Deleting volume $vol2"
    run export_group delete ${PROJECT}/${eg}
    run volume delete ${PROJECT}/${vol2} --wait

    echo "**** Done array affinity placement tests"
}

arrayaffinity_cleanup()
{
    echo "Deleting VPools"

    # delete cos so that test can be executed again
    run cos delete $COS_ARRAYAFFINITY block
    run cos delete $COS_ARRAYAFFINITY_VMAX block
    run cos delete $COS_ARRAYAFFINITY_VNX block

	echo "Deleting host ${HOST_ARRAYAFFINITY}"
	run hosts delete ${HOST_ARRAYAFFINITY}
}

# Array affinity tests command: "sanity sanity.conf viprIP arrayaffinity"
arrayaffinity_tests()
{
    secho "Executing array affinity tests"

    arrayaffinity_cos_tests
    arrayaffinity_placement_tests
    arrayaffinity_cleanup

    secho "Completed array affinity tests"
}

COS_PORTGROUP=cos_portgroup
HOST_PORTGROUP=hostpg
portgroup_setup_once()
{
    # do this only once
    smisprovider show $VMAX_SMIS_DEV &> /dev/null && return $?

    run smisprovider create $VMAX_SMIS_DEV $VMAX_SMIS_IP $VMAX_SMIS_PORT $SMIS_USER "$SMIS_PASSWD" $VMAX_SMIS_SSL
    run storagedevice discover_all --ignore_error

    sleep 15

    secho "VMAX storage pool update"
    run storagepool update $VMAX_NATIVEGUID --type block --volume_type THIN_ONLY
    run storagepool update $VMAX_NATIVEGUID --nhadd $NH --type block
    
    echo "Enabling use port group"
    customconfig create VMAXUsePortGroupEnabled true systemType vmax --register true
}

portgroup_cos_setup()
{
    secho "setting up port group block virtual pool"

    run cos create block $COS_PORTGROUP \
        --description 'Virtual-Pool-for-portgroup' true \
                         --protocols FC \
                         --numpaths 2 \
                         --max_snapshots 10 \
                         --system_type vmax \
                         --provisionType 'Thin' \
                         --expandable true \
                         --neighborhoods $NH \
                         --multiVolumeConsistency 

    run cos update block $COS_PORTGROUP --storage $VMAX_NATIVEGUID

}

portgroup_host_setup()
{
    portwwn1=10:a1:b1:c1:d1:e1:f1:01
    nodewwn1=20:a1:b1:c1:d1:e1:f1:01
    portwwn2=10:a2:b2:c2:d2:e2:f2:02
    portwwn3=10:a3:b3:c3:d3:e3:f3:03
    portwwn4=10:a4:b4:c4:d4:e4:f4:04
	portwwn5=50:00:09:73:68:03:14:4B
    fctz=${NH}/FABRIC_losam082-fabric
    host1=hostpg
    echo "Creating host with name $HOST_PORTGROUP"
    run hosts create ${HOST_PORTGROUP} $TENANT Other ${HOST_PORTGROUP} --port 8111 --username user --password 'password' --osversion 1.0
    run initiator create ${HOST_PORTGROUP} FC $portwwn1 --node $nodewwn1
    run initiator create ${HOST_PORTGROUP} FC $portwwn2 --node $nodewwn1
    run initiator create ${HOST_PORTGROUP} FC $portwwn3 --node $nodewwn1
    run initiator create ${HOST_PORTGROUP} FC $portwwn4 --node $nodewwn1
    echo "Adding WWNs to Network"
    run transportzone add $fctz $portwwn1
    run transportzone add $fctz $portwwn2
    run transportzone add $fctz $portwwn3
    run transportzone add $fctz $portwwn4
	run transportzone add $fctz $portwwn5

    echo "Getting host URI"
    HOST_PORTGROUP_URI=`hosts list ${TENANT} | grep ${HOST_PORTGROUP} | awk '{print $4}'`
    echo "Host URI is ${HOST_PORTGROUP_URI}"
}

portgroup_setup()
{
    secho "Run port group tests setup"
    portgroup_setup_once
    portgroup_cos_setup
    portgroup_host_setup
    secho "Completed port group setup"
}

portgroup_operations_tests()
{
    echo "Executing port group tests"
    portgroup=portgroup-${RANDOM}
	portgroup2=portgroup-${RANDOM}
    storageportgroup create $portgroup --storage=$VMAX_NATIVEGUID --storageports 'FA-1D:5' 'FA-1D:6'
	storageportgroup create $portgroup2 --storage=$VMAX_NATIVEGUID --storageports 'FA-2D:5' 'FA-2D:6'
    storageportgroup show $VMAX_NATIVEGUID/$portgroup
    storageportgroup deregister ${VMAX_NATIVEGUID}/${portgroup}
    storageportgroup register ${VMAX_NATIVEGUID}/${portgroup}
	storageportgroup register ${VMAX_NATIVEGUID}/${portgroup2}
    
    vol1=volpg1-${RANDOM}
    vol2=volpg2-${RANDOM}

    echo "Creating volume with name $vol1 $vol2"
    run volume create $vol1 $PROJECT $NH $COS_PORTGROUP $BLK_SIZE --portgroup ${VMAX_NATIVEGUID}/${portgroup}
    run volume create $vol2 $PROJECT $NH $COS_PORTGROUP $BLK_SIZE --portgroup ${VMAX_NATIVEGUID}/${portgroup}

    eg=egpg-${RANDOM}
    run export_group create $PROJECT $eg $NH --type Host --volspec $PROJECT/$vol1 --hosts $HOST_PORTGROUP --portgroup ${VMAX_NATIVEGUID}/${portgroup}
    run export_group update ${PROJECT}/$eg --addVolspec "${PROJECT}/${vol2}"
	run export_group changeportgroup ${PROJECT}/$eg ${VMAX_NATIVEGUID}/${portgroup2}
    
    echo "Deleting volume $vol1"
    run export_group delete ${PROJECT}/${eg}
    run volume delete ${PROJECT}/${vol1} --wait
    run volume delete ${PROJECT}/${vol2} --wait
    storageportgroup delete ${VMAX_NATIVEGUID}/${portgroup}
	storageportgroup delete ${VMAX_NATIVEGUID}/${portgroup2}

    echo "**** Done port group operations tests"
}

portgroup_cleanup()
{
    echo "Deleting VPools"

    # delete cos so that test can be executed again
    run cos delete $COS_PORTGROUP block
    echo "Deleting host ${HOST_PORTGROUP}"
    run hosts delete ${HOST_PORTGROUP}
}

# Port group tests command: "sanity sanity.conf viprIP portgroup"
portgroup_tests()
{
    secho "Executing portgroup tests"

    portgroup_operations_tests
    portgroup_cleanup

    secho "Completed portgroup tests"
}

# set DO_TESTS_ONLY to 1 if setup does not need to be re-run
DO_TESTS_ONLY=${DO_TESTS_ONLY:-0}
if [ $DO_TESTS_ONLY -eq 0 ]; then
    ${SS}_setup
fi
if [ $DO_SETUP_ONLY -eq 0 ]; then
    ${SS}_tests
fi

if [ $WS_SETUP_MODE -eq 1 ] ; then
    output_conf
fi

# check for pending before completing succesful sanity run
task check_for_pending --fail

_success
